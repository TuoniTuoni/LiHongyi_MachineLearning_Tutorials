{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# È°πÁõÆ8-ÁªàË∫´Â≠¶‰π†\n",
    "\n",
    "## ÂèãÊÉÖÊèêÁ§∫\n",
    "ÂêåÂ≠¶‰ª¨ÂèØ‰ª•ÂâçÂæÄËØæÁ®ã‰Ωú‰∏öÂå∫ÂÖàË°åÂä®ÊâãÂ∞ùËØï ÔºÅÔºÅÔºÅ\n",
    "\n",
    "## È°πÁõÆÊèèËø∞\n",
    "\n",
    "* ‰ΩøÁî®EWC - Elastic Weight ConsolidationÂíåMAS - Memory Aware Synapse‰∏§Áßçregularization based lifelong learning ÁöÑÊñπÊ≥ïÂú®PaddleÁöÑ‰∏§‰∏™‰∏çÂêåÁöÑÊï∞ÊçÆÈõÜMNISTÂíåCifar10‰∏äËøõË°åÂ≠¶‰π†ËÆ≠ÁªÉ\n",
    "* Ê†πÊçÆÁªôÂá∫ÁöÑ‰∏§Áßçlifelong learning regularizationÁÆóÊ≥ïËá™Â∑±ÂÅö‰∏Ä‰∏™SCPÁÆóÊ≥ï\n",
    "\n",
    "## Êï∞ÊçÆÈõÜ‰ªãÁªç\n",
    "\n",
    "Êú¨Ê¨°‰ΩøÁî®ÁöÑÊï∞ÊçÆÈõÜ‰∏∫MNISTÂíåCifar10Êï∞ÊçÆÈõÜÔºåÂÖ±Êúâ10Á±ª\n",
    "\n",
    "MNISTÊï∞ÊçÆÈõÜÔºö\n",
    "ÂõæÂÉèÈÉΩÊòØ28x28Â§ßÂ∞èÁöÑÁÅ∞Â∫¶ÂõæÂÉèÔºåÊØè‰∏™ÂÉèÁ¥†ÁöÑÊòØ‰∏Ä‰∏™ÂÖ´‰ΩçÂ≠óËäÇÔºà0~255Ôºâ\n",
    "ËØÜÂà´Êï∞Â≠ó0ÔΩû9Ôºà0Ôºå1Ôºå2Ôºå3Ôºå4Ôºå5Ôºå6Ôºå7Ôºå8Ôºå9Ôºâ\n",
    "Training set: 60000\n",
    "Testing set: 10000Âº†\n",
    "\n",
    "Cifar10Êï∞ÊçÆÈõÜÔºö\n",
    "ÂõæÂÉèÈÉΩÊòØ32x32Â§ßÂ∞èÁöÑÂΩ©Ëâ≤ÂõæÂÉèÔºåÊØè‰∏™ÂÉèÁ¥†üà∂Áî±‰∏Ä‰∏™3xÂÖ´‰ΩçÂ≠óËäÇÔºà0~255ÔºâÁªÑÊàê\n",
    "ËØÜÂà´'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "Training set: 50000Âº†\n",
    "Testing set: 10000Âº†\n",
    "\n",
    "**Êï∞ÊçÆÊ†ºÂºè**\n",
    "\n",
    "Paddle‰∏≠Ëá™Â∏¶Êï∞ÊçÆÈõÜÔºåÂú®‰ª£Á†Å‰∏≠‰∏ãËΩΩÊï∞ÊçÆÂåÖ\n",
    "\n",
    "## È°πÁõÆË¶ÅÊ±Ç\n",
    "\n",
    "* ËØ∑‰ª•‰∏≠ÊñáËØ¥Êòé‰∏Ä‰∏ã lifelong learning ÁöÑ‰∏≠ÂøÉÊ¶ÇÂøµÊòØ‰ªÄ‰πàÔºü\n",
    "* ÂàóÂá∫EWC,MASÁöÑ‰ΩúÊ≥ïÊòØ‰ªÄ‰πàÔºüÊ†πÊçÆ‰Ω†ÁöÑÁêÜËß£ÔºåËØ¥Êòé‰∏Ä‰∏ãÂ§ßÊ¶ÇÁöÑÊµÅÁ®ãËØ•ÊÄé‰πàÂÅö\n",
    "* EWCÂíåMASÊñπÊ≥ï‰∏äÊâÄÈúÄË¶ÅÁöÑËµÑÊñôÊúÄÂ§ßÁöÑÂ∑ÆÂºÇÊòØ‰ªÄ‰πà\n",
    "* ÁßÄÂá∫part1Âèäpart2ÊúÄÂêéÁªìÊûúÁöÑÂõæÔºåÂπ∂ÂàÜÊûê‰∏Ä‰∏ãÁªìÊûúÔºå‰ª•Âèä‰Ω†Ë∑ëÁöÑÂÆûÈ™å‰∏≠Êúâ‰ªÄ‰πàÂèëÁé∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Êï∞ÊçÆÂáÜÂ§á\n",
    "        \n",
    "Êó†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ÁéØÂ¢ÉÈÖçÁΩÆ/ÂÆâË£Ö\n",
    "\n",
    "Êó†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ÁªàË∫´Êú∫Âô®Â≠¶‰π†\n",
    "\n",
    "### ÊñπÊ≥ï\n",
    "Âú®2019Âπ¥Â∫ïÔºåÊúâ‰∫∫ÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§ßÊ±áÊï¥Â∞Ülifelong learning ÁöÑÊñπÊ≥ïÔºå‰ªé2016- 2019 Âπ¥Âàù ÁöÑÊ®°ÂûãÂÅö‰∫ÜÂΩíÁ±ªÔºåÂ§ßËá¥‰∏äÂèØ‰ª•ÂàÜÊàê‰∏âÁßçÂ§ßÊñπÊ≥ï\n",
    "* Replay-based methods\n",
    "* Regularization-based methods\n",
    "* Parameter isolation methods\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/dc2465afd2ee4e51a35d93b8129dba025afa68f7fbe44993a11bc73891f45214)\n",
    "\n",
    "\n",
    "Âú®ËøôÊ¨°ÁöÑ‰Ωú‰∏ö‰πã‰∏≠ÔºåÊàë‰ª¨Ë¶ÅËµ∞Ëøá‰∏ÄÊ¨°regularization-based methods ÈáåÈù¢ÁöÑ prior-focusedÁöÑ‰∏§ÁßçÊñπÊ≥ï ÂàÜÂà´ÊòØ EWC Âíå MAS Ëøô‰∏§ÁßçÊñπÊ≥ï\n",
    "\n",
    "ÂõæÁâáÂá∫Â§Ñ [Continual Learning in Neural\n",
    "Networks](https://arxiv.org/pdf/1910.02718.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ËøõÂÖ•ÂÆòÁΩëÈÄÇÂêàËá™Â∑±ÁöÑpaddlepaddleÁâàÊú¨ÔºåÂπ∂ËøêË°åÁõ∏Â∫îÁöÑÂÆâË£ÖÂëΩ‰ª§ÂÆâË£Öpaddle paddle 2.0 rcÁâàÊú¨\n",
    "# !python -m pip install paddlepaddle-gpu==2.0.0rc0.post90 -f https://paddlepaddle.org.cn/whl/stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ÂØºÂÖ•Â∫ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as optim\n",
    "import paddle.nn.functional as F\n",
    "import paddle.fluid.data as data\n",
    "import paddle.fluid.core as core\n",
    "import paddle.fluid.dataloader.sampler as sampler\n",
    "from paddle import vision\n",
    "from paddle.io import DataLoader\n",
    "from paddle.vision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "# Â¶ÇÊûúÊîØÊåÅgpuÂàô‰ΩøÁî®gpuËÆ≠ÁªÉÂê¶Âàô‰ΩøÁî®cpuËÆ≠ÁªÉ\n",
    "support_gpu = paddle.is_compiled_with_cuda()\n",
    "place = paddle.CPUPlace()\n",
    "if support_gpu:\n",
    "    place = paddle.CUDAPlace(0)\n",
    "paddle.disable_static(place)\n",
    "print(paddle.__version__)\n",
    "print(\"place:{} \".format(place))\n",
    "\n",
    "device = paddle.set_device(\"gpu\" if core.is_compiled_with_cuda() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ê®°Âûã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " >Âõ†‰∏∫Êú¨Ê¨°‰Ωú‰∏öÂº∫Ë∞ÉÁöÑÊòØlifelong learning ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÂπ∂ÈùûÂè†Ê®°ÂûãÔºåÊâÄ‰ª•‰ªäÂ§©Êàë‰ª¨ÊâÄ‰∏æÁöÑ‰æãÂ≠êÔºåÈÉΩ‰ºö‰ΩøÁî®Âêå‰∏Ä‰∏™Ê®°ÂûãÊù•ÂÅöËÆ≠ÁªÉÂè™ÊòØÂ∫îÁî®‰∏ä‰∏çÂêålifelong learningÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºå Âú®ËøôÊ¨°ÁöÑ‰Ωú‰∏öÁöÑ‰æãÂ≠êÂÜÖ Êàë‰ª¨‰ΩøÁî®ÁöÑÊòØ‰∏Ä‰∏™ÂÖ≠Â±ÇÁöÑfully-connected layerÁöÑÊ®°ÂûãÂä†‰∏äreluÁöÑactivation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Âü∫ÂáÜÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.fc1 = nn.Linear(3*32*32, 1024)\n",
    "    self.fc2 = nn.Linear(1024, 512)\n",
    "    self.fc3 = nn.Linear(512, 256)\n",
    "    self.fc4 = nn.Linear(256, 128)\n",
    "    self.fc5 = nn.Linear(128, 128)\n",
    "    self.fc6 = nn.Linear(128, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.reshape((-1, 3*32*32))\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc4(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc5(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc6(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "‰ª•‰∏ãÊàë‰ª¨Â∞á‰æùÂ∫è‰ªãÁ¥πËøô‰∏§ÁßçÊñπÊ≥ï EWC Ë∑ü MAS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Elastic Weight Consolidation\n",
    "\n",
    "#### Ê¶ÇÂøµ\n",
    "ËÄÅÂ∏àÂú®ÂΩ±Áâá‰∏≠Â∑≤ÁªèÊääÊ†∏ÂøÉÊ¶ÇÂøµ‰ªãÁªçÁªôÂ§ßÂÆ∂ÔºåÈÇ£Âú®ËøôËæπÊàëÊÉ≥Â§ßÂÆ∂ÈÉΩÈùûÂ∏∏‰∫ÜËß£‰∫ÜËøô‰∏™ÊñπÊ≥ïÁöÑÊ¶ÇÂøµÔºåÊàë‰ª¨Â∞±Áõ¥Êé•ËøõÂÖ•‰∏ªÈ¢ò\n",
    "\n",
    "‰ªäÂ§©Êàë‰ª¨ÁöÑ‰ªªÂä° ÊòØÂú®Â≠¶‰π†ËøûÁª≠ÁöÑ‰∏§‰∏™ task task A Ë∑ü task B:\n",
    "\n",
    "Âú® EWC ‰ΩúÊ≥ï‰∏ã ‰ªñÁöÑ loss function ‰ºöË¢´ÂÆö‰πâÂ¶Ç‰∏ã\n",
    " $$\\mathcal{L}_B = \\mathcal{L}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_i (\\theta_{i} - \\theta_{A,i}^{*})^2  $$\n",
    "\n",
    "ÂÖàËß£ÈáäËøô‰∏™ loss function Ë£°ÁöÑÂèòÊï∞Ôºå$\\mathcal{L}_B$ ÊòØÊåá task B ÁöÑ loss, ‰ºöÁ≠â‰∫é Ê≠£Â∏∏ÁöÑloss function $\\mathcal{L}(\\theta)$ (Â¶ÇÊûú ÊòØ classification ÁöÑÈóÆÈ¢ò,Â∞±ÊòØ cross entropy ÁöÑ loss function) Âä†‰∏ä‰∏Ä‰∏™Ê≠£ÂàôÈ°π (regularization term) \n",
    "\n",
    "Ëøô‰∏™Ê≠£ÂàôÈ°πÁöÑÁî±‰∏§‰∏™ÈÉ®‰ªΩÁªÑÊàêÔºåÁ¨¨‰∏Ä‰∏™ÊòØ $F_i$ ‰πüÊòØËøô‰∏™ÊñπÊ≥ïÁöÑÊ†∏ÂøÉ, Á¨¨‰∫å‰∏™ÈÉ®‰ªΩÊòØ $(\\theta_{i} - \\theta_{A,i}^{*})^2$  ,  $\\theta_{A,i}^{*}$ ‰ª£Ë°®ÁöÑÊòØ ËÆ≠ÁªÉÂÆåtask A Â≠ò‰∏ãÊù•Ê®°ÂûãÁ¨¨ i ‰∏™ÂèÇÊï∞ÁöÑÂÄº, $\\theta_i$ ‰ª£Ë°®ÁöÑÊòØÁõÆÂâçÊ®°ÂûãÁ¨¨i‰∏™ÂèÇÊï∞ÁöÑÂÄºÔºåÊ≥®ÊÑè‰∏ÄÁÇπÊòØÊ®°ÂûãÁöÑÊû∂ÊûÑÂú®ËøôÁßç regularization based ÁöÑÊñπÊ≥ï‰∏äÔºåÈÉΩÊòØÂõ∫ÂÆö„ÑâÔºåÁõÆÂâçÊ®°ÂûãË∑ü task A Â≠ò‰∏ãÊù•ÁöÑÊ®°Âûã Êû∂ÊûÑÈÉΩ‰∏ÄÊ†∑Âè™ÊòØÂÄº‰∏ç‰∏ÄÊ†∑„ÄÇÂ∫ï‰∏ãÊàëÂ∞ÜËØ¥ÊòéËøô‰∏™ $F_i$ ÊòØÊÄé‰πàÂÆûÂÅöÂá∫Êù•\n",
    "\n",
    "Âú®ËÄÅÂ∏àÁöÑÂΩ±Áâá‰∏≠ÔºåËÄÅÂ∏àÊòØ‰ª•Âè™Êúâ‰∏§‰∏™ÂèÇÊï∞ÁöÑÊ®°Âûã‰∏æ‰æãÂ≠êÔºåÈÇ£ÂÅáËÆæÊàë‰ªäÂ§©Ê®°ÂûãÂ∞±ÊòØ‰∏Ä‰∏™ neural network(ÂèÇÊï∞‰∏çÂè™‰∏§‰∏™) ËØ•ÊÄé‰πàÂäûÂë¢Ôºü   \n",
    "\n",
    "$F_i$ ÂØπÂ∫îÂà∞ËÄÅÂ∏àÁöÑÂΩ±ÁâáÂèôËø∞ÊòØÊåáÁ¨¨i‰∏™ÂèÇÊï∞ÁöÑÂÆàÂç´ÔºåÂÅáËÆæËøô‰∏™ÂèÇÊï∞ÂØπ task A ÂæàÈáçË¶ÅÔºåÈÇ£Ëøô‰∏™ $F_i$ ÁöÑÂÄºÂ∞±‰ºöÂæàÂ§ßÔºåËøô‰∏™ÂèÇÊï∞Â∞ΩÈáè‰∏çËÉΩË¢´Êõ¥Âä®...\n",
    "\n",
    "ÂÆûÈôÖ‰∏äËøô‰∏™ÂèÇÊï∞ÁöÑÁÆóÊ≥ï Âç≥ÊòØ Â¶Ç‰∏ãÁöÑÂºèÂ≠ê\n",
    "\n",
    "$$ F = [ \\nabla \\log(p(y_n | x_n, \\theta_{A}^{*}) \\nabla \\log(p(y_n | x_n, \\theta_{A}^{*})^T ] $$ \n",
    "\n",
    "$F$ ‰πã‰∏≠ Âè™‰ª•ÂØπËßíÁ∫øÁöÑÂÄºÂéªËøë‰ººÂêÑ‰∏™ÂèÇÊï∞ÁöÑ $F_i$ ÂÄº\n",
    "\n",
    "$p(y_n | x_n, \\theta_{A}^{*})$ ÊåáÁöÑÂ∞±ÊòØÊ®°ÂûãÂú®ÁªôÂÆö‰πãÂâç task ÁöÑ data $x_n$ ‰ª•Âèä ÁªôÂÆö ËÆ≠ÁªÉÂÆå task A Â≠ò‰∏ãÊù•ÁöÑÊ®°ÂûãÂèÇÊï∞ $\\theta_A^*$ ÂæóÂà∞ $y_n$($x_n$ ÂØπÂ∫îÁöÑ label ) ÁöÑ posterior probability.\n",
    "ÈÇ£ÁªüÊï¥‰∏Ä‰∏ã‰ΩúÊ≥ïÂ∞±ÊòØ ÂÜçÂØπËøô‰∏™ $p(y_n | x_n, \\theta_{A}^{*})$ Âèñ log ÂÜçÂèñ gradient Âπ∂‰∏îÂπ≥Êñπ ( parameter.grad )^2.\n",
    "\n",
    "ÊØè‰∏Ä‰∏™ÂèÇÊï∞ÊàëÈÉΩÂèØ‰ª•‰ΩøÁî® paddle ÁöÑ backward ‰πãÂêéÂÜçÂèñ gradient ÁöÑÊÄßË¥®ÁÆóÂá∫ÂêÑËá™ÁöÑ $F_i$.\n",
    "\n",
    "ÊúâÂÖ≥Ëøô‰∏™ $F$ ÂÖ∂ÂÆûÂçöÂ§ßÁ≤æÊ∑±ÔºåÊòØÊù•Ëá™‰∫é fisher information matrix. Â∫ï‰∏ãÊàëÊîæ‰∏äÊúâÂÖ≥Ëøô‰∏™lifelong learning Âú® fisher information matrix ‰∏äÊòØÊÄé‰πàÁÆÄÂçïÁöÑËøë‰ººÂà∞Ëøô‰∏ÄÈ°πÔºåÁÆÄÂçïÁöÑÊé®ÂØºÊù•Ëá™ [Continual Learning in Neural\n",
    "Networks](https://arxiv.org/pdf/1910.02718.pdf) Á¨¨2.4.1 Â∞èËäÇ ‰∏é 2.4 ËäÇ\n",
    "\n",
    "For You Information: [Elastic Weight Consolidation](https://arxiv.org/pdf/1612.00796.pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EWC(object):\n",
    "  \"\"\"\n",
    "    @article{kirkpatrick2017overcoming,\n",
    "        title={Overcoming catastrophic forgetting in neural networks},\n",
    "        author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},\n",
    "        journal={Proceedings of the national academy of sciences},\n",
    "        year={2017},\n",
    "        url={https://arxiv.org/abs/1612.00796}\n",
    "    }\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, model: nn.Layer, dataloaders: list, device):\n",
    "\n",
    "      self.model = model\n",
    "      self.dataloaders = dataloaders\n",
    "      self.device = device\n",
    "\n",
    "      self.params = {n: p for n, p in self.model.named_parameters() if not p.stop_gradient}  # ÊäìÂá∫Ê®°ÂûãÁöÑÊâÄÊúâÂèÇÊï∞\n",
    "      self._means = {}  # ÂàùÂßãÂåñ Âπ≥ÂùáÂèÇÊï∞\n",
    "      self._precision_matrices = self._calculate_importance()  # ‰∫ßÁîü EWC ÁöÑ Fisher (F) Áü©Èòµ\n",
    "\n",
    "      for n, p in self.params.items():\n",
    "          self._means[n] = p.clone().detach()  # ÁÆóÂá∫ÊØè‰∏™ÂèÇÊï∞ÁöÑÂπ≥Âùá ÔºàÁî®‰πãÂâç‰ªªÂä°ÁöÑËµÑÊñôÂéªÁÆóÂπ≥ÂùáÔºâ\n",
    "\n",
    "  def _calculate_importance(self):\n",
    "      print('Computing EWC')\n",
    "\n",
    "      precision_matrices = {}\n",
    "      for n, p in self.params.items():  # ÂàùÂßãÂåñ Fisher (F) ÁöÑÁü©ÈòµÔºàÈÉΩË°•Èõ∂Ôºâ\n",
    "          t_val = p.clone().detach()\n",
    "          print(type(t_val))\n",
    "          t_val.set_value(np.zeros(shape=t_val.numpy().shape, dtype=np.float32))\n",
    "          print(t_val)\n",
    "          precision_matrices[n] = t_val.numpy()\n",
    "\n",
    "      self.model.eval()\n",
    "      dataloader_num = len(self.dataloaders)\n",
    "      number_data = sum([len(loader) for loader in self.dataloaders])\n",
    "      for dataloader in self.dataloaders:\n",
    "          for data in dataloader:\n",
    "              self.model.clear_gradients()\n",
    "              input = data[0]\n",
    "              output = self.model(input)\n",
    "              label = np.argmax(output.numpy(), axis=-1)\n",
    "              label = paddle.to_tensor(label)\n",
    "\n",
    "              ############################################################################\n",
    "              #####                      ‰∫ßÁîü EWC ÁöÑ Fisher(F) Áü©Èòµ                    #####\n",
    "              ############################################################################\n",
    "              loss = F.nll_loss(F.log_softmax(output, axis=1), label)\n",
    "              loss.backward()\n",
    "              for n, p in self.model.named_parameters():\n",
    "                  precision_matrices[n] += np.power(p.grad, 2) / number_data\n",
    "\n",
    "      precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "      return precision_matrices\n",
    "\n",
    "  def penalty(self, model: nn.Layer):\n",
    "      loss = 0\n",
    "      for n, p in model.named_parameters():\n",
    "          _loss = self._precision_matrices[n] * (p - self._means[n]).numpy() ** 2\n",
    "          loss += _loss.sum()\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Memory Aware Synapses\n",
    "Ê¶ÇÂøµ:\n",
    "ËÄÅÂ∏àÁöÑÂΩ±Áâá‰∏≠ÔºåÂ∞ÜÂÆÉÂΩíÁ±ªÂà∞Âíå EWC ‰∏ÄÊ†∑ÁöÑÊñπÊ≥ïÔºåÂè™ÊòØÁÆóËøô‰∏™ important weight ÁöÑÊñπÂºè‰∏çÂ§™‰∏ÄÊ†∑.Â∫ï‰∏ãÊàëÂ∞ÜËØ¥ÊòéËøô‰∏™ÊñπÊ≥ïËØ•ÊÄé‰πàÂÆûÂÅö\n",
    "\n",
    "MAS:\n",
    "Âú® MAS ÂÜÖÔºåÂ≠¶‰π†‰∏Ä‰∏™ËøûÁª≠ÁöÑ tasks, task A, Âíå task B, ‰ªñÁöÑ loss function ÂÆö‰πâÂ¶Ç‰∏ã:\n",
    "\n",
    "$$\\mathcal{L}_B = \\mathcal{L}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} \\Omega_i (\\theta_{i} - \\theta_{A,i}^{*})^2$$\n",
    "\n",
    "Âíå ewc‰∏çÂêåÁöÑÊòØ ÂºèÂ≠ê‰∏≠ÁöÑ $F_i$ Ë¢´Âèñ‰ª£Êàê $\\Omega_i$ , $\\Omega_i$ Êù•Ëá™‰∫é‰ª•‰∏ãÁöÑÂºèÂ≠êÔºö\n",
    "\n",
    "$$\\Omega_i = || \\frac{\\partial \\ell_2^2(M(x_k; \\theta))}{\\partial \\theta_i} || $$ \n",
    "\n",
    "$x_k$ ÊòØ Êù•Ëá™‰∫é ÂâçÈù¢ task ÁöÑ sample data„ÄÇ ÂºèÂ≠ê‰∏äÁöÑ‰ΩúÊ≥ïÂ∞±ÊòØÂØπÊúÄÂêéÊ®°ÂûãÁöÑ output vector (ÊúÄÂêé‰∏ÄÂ±Ç)ÂÅö l2 norm ÂêéÂèñÂπ≥Êñπ ÂÜçÂØπÂêÑËá™ÁöÑweightÂæÆÂàÜ(Âèñgradient) Âπ∂‰∏îÂèñ ËØ• gradient ÁöÑÁªùÂØπÂÄºÔºåÂú®ËØ•paper ‰∏≠ÂÖ∂ÂÆû‰πüÂèØ‰ª•ÂØπÂêÑ‰∏™Â±ÇÁöÑ output vector ÂÅö l2 norm ( local ÁâàÊú¨)ÔºåËøôËæπÂè™ÂÆûÂÅö global ÁöÑÁâàÊú¨„ÄÇ\n",
    "\n",
    "\n",
    "For Your Information: \n",
    "[Memory Aware Synapses](https://arxiv.org/pdf/1711.09601.pdf)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MAS(object):\n",
    "    \"\"\"\n",
    "    @article{aljundi2017memory,\n",
    "      title={Memory Aware Synapses: Learning what (not) to forget},\n",
    "      author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},\n",
    "      booktitle={ECCV},\n",
    "      year={2018},\n",
    "      url={https://eccv2018.org/openaccess/content_ECCV_2018/papers/Rahaf_Aljundi_Memory_Aware_Synapses_ECCV_2018_paper.pdf}\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Layer, dataloaders: list, device):\n",
    "        self.model = model\n",
    "        self.dataloaders = dataloaders\n",
    "        self.params = {n: p for n, p in self.model.named_parameters() if not p.stop_gradient}  # ÊäìÂá∫Ê®°ÂûãÁöÑÊâÄÊúâÂèÇÊï∞\n",
    "        self._means = {}  # ÂàùÂßãÂåñ Âπ≥ÂùáÂèÇÊï∞\n",
    "        self.device = device\n",
    "        self._precision_matrices = self.calculate_importance()  # ‰∫ßÁîü MAS ÁöÑ Omega(Œ©) Áü©Èòµ\n",
    "\n",
    "        for n, p in self.params.items():\n",
    "            self._means[n] = p.clone().detach()\n",
    "\n",
    "    def calculate_importance(self):\n",
    "        print('Computing MAS')\n",
    "\n",
    "        precision_matrices = {}\n",
    "        for n, p in self.params.items():\n",
    "            # ÂàùÂßãÂåñ Omega(Œ©) Áü©ÈòµÔºàÈÉΩË°•0Ôºâ\n",
    "            t_val = p.clone().detach()\n",
    "            t_val.set_value(np.zeros(shape=t_val.numpy().shape, dtype=np.float32))\n",
    "            precision_matrices[n] = t_val.numpy()\n",
    "\n",
    "        self.model.eval()\n",
    "        dataloader_num = len(self.dataloaders)\n",
    "        num_data = sum([len(loader) for loader in self.dataloaders])\n",
    "        for dataloader in self.dataloaders:\n",
    "            for data in dataloader:\n",
    "                self.model.clear_gradients()\n",
    "                # output = self.model(data[0].to(self.device))\n",
    "                output = self.model(data[0])\n",
    "\n",
    "                #######################################################################################\n",
    "                #####  ‰∫ßÁîü MAS ÁöÑ Omega(Œ©) Áü©Èòµ ( ÂØπ output ÂêëÈáè ÁÆó‰ªñÁöÑ l2 norm ÁöÑÂπ≥Êñπ) ÂÜçÂèñ gradient  ####\n",
    "                #######################################################################################\n",
    "                output = paddle.pow(output, 2)\n",
    "                #output.pow_(2)\n",
    "                loss = paddle.sum(output, axis=1)\n",
    "                loss = loss.mean()\n",
    "                loss.backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    precision_matrices[n] += abs(p.grad) / num_data\n",
    "\n",
    "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "        return precision_matrices\n",
    "\n",
    "    def penalty(self, model: nn.Layer):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            _loss = self._precision_matrices[n] * (p - self._means[n]).numpy() ** 2\n",
    "            loss += _loss.sum()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ËµÑÊñô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ËµÑÊñôÈ¢ÑÂ§ÑÁêÜ\n",
    "- ËΩ¨Êç¢ MNIST  ($1*28*28$) Âà∞ ($3*32*32$)\n",
    "- ËΩ¨Êç¢ USPS   ($1*16*16$) Âà∞ ($3*32*32$)\n",
    "- Ê≠£ËßÑÂåñ ÂõæÁâá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "from paddle.vision import transforms\n",
    "import paddle.nn.functional as F\n",
    "from paddle.vision.transforms import functional\n",
    "\n",
    "\n",
    "class ResizeImg(object):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_size_0 = img.shape[0]\n",
    "        img_size_1 = img.shape[1]\n",
    "        if (self.size - img_size_0) % 2 == 1:\n",
    "            img_size_0 += 1\n",
    "        if (self.size - img_size_1) % 2 == 1:\n",
    "            img_size_1 += 1\n",
    "        img = functional.resize(img, (img_size_0, img_size_1))\n",
    "        idx_tuple = np.where(img == 0)\n",
    "        if idx_tuple:\n",
    "            for i in range(len(idx_tuple[0])):\n",
    "                img[idx_tuple[0][i]][idx_tuple[1][i]][idx_tuple[2][i]] = 1\n",
    "        print(\"shape of img:{} \".format(img.shape))\n",
    "        return img\n",
    "\n",
    "\n",
    "class Convert2RGB(object):\n",
    "\n",
    "    def __init__(self, num_channel):\n",
    "        self.num_channel = num_channel\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # If the channel of img is not equal to desired size,\n",
    "        # then expand the channel of img to desired size.\n",
    "        img_channel = img.shape[0]\n",
    "        img = paddle.concat([img] * (self.num_channel - img_channel + 1), 0)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Pad(object):\n",
    "\n",
    "    def __init__(self, size, fill=0, padding_mode='constant'):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # If the H and W of img is not equal to desired size,\n",
    "        # then pad the channel of img to desired size.'\n",
    "        img = img.reshape((img.shape[1], img.shape[0], img.shape[2]))\n",
    "        img_size_h = img.shape[1]\n",
    "        img_size_w = img.shape[2]\n",
    "        left_pad = (self.size - img_size_w) // 2\n",
    "        right_pad = self.size - left_pad - img_size_w\n",
    "        up_pad = (self.size - img_size_h) // 2\n",
    "        low_pad = self.size - up_pad - img_size_h\n",
    "        img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "        pad_rs = F.pad(img, [left_pad, right_pad, up_pad, low_pad], value=1, mode='constant', data_format=\"NCHW\")\n",
    "        pad_rs = pad_rs.reshape((pad_rs.shape[-3], pad_rs.shape[-2], pad_rs.shape[-1]))\n",
    "        return pad_rs\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    transform = transforms.Compose([  # ResizeImg(32),\n",
    "        transforms.ToTensor(),\n",
    "        Pad(32),\n",
    "        Convert2RGB(3),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ÂáÜÂ§á ËµÑÊñôÈõÜ\n",
    "- MNIST   : ‰∏ÄÂº†ÂõæÁâáËµÑÊñôÂ§ßÂ∞è:  $28*28*1$, ÁÅ∞Èò∂ , 10 ‰∏™ÁßçÁ±ª\n",
    "- SVHN    : ‰∏ÄÂº†ÂõæÁâáËµÑÊñôÂ§ßÂ∞è:  $32*32*3$, RGB , 10 ‰∏™ÁßçÁ±ª\n",
    "- USPS    : ‰∏ÄÂº†ÂõæÁâáËµÑÊñôÂ§ßÂ∞è:  $16*16*1$, ÁÅ∞Èò∂ , 10 ‰∏™ÁßçÁ±ª\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from paddle.dataset import common\n",
    "from paddle.vision import datasets\n",
    "\n",
    "\n",
    "class Data():\n",
    "\n",
    "    def __init__(self):\n",
    "        transform = get_transform()\n",
    "\n",
    "        print('download training data and load training data of mnist dataset ! ')\n",
    "        self.MNIST_train_dataset = datasets.MNIST(mode='train',\n",
    "                                                  transform=transform)\n",
    "        # self.MNIST_test_dataset = datasets.MNIST(mode='test',\n",
    "        #                                          transform=transform)\n",
    "        print('load mnist dataset finished ! ')\n",
    "\n",
    "        # Â¶ÇÊûúÈúÄË¶Å‰∏â‰∏™Êï∞ÊçÆÈõÜÔºåÂèØ‰ª•‰ΩøÁî®FlowersÊï∞ÊçÆÈõÜÔºå‰ΩÜÊòØÈúÄË¶Å‰øÆÊîπÁΩëÁªú‰∏≠ÁöÑÁ±ªÂà´Êï∞ÁõÆ\n",
    "        #print('download training data and load training data of flowers dataset ! ')\n",
    "        #self.Flowers_train_dataset = datasets.Flowers(mode='train',\n",
    "        #                                             transform=transform)\n",
    "        # self.Flowers_test_dataset = datasets.MNIST(mode='test',\n",
    "        #                                            transform=transform)\n",
    "        #print('load flowers dataset finished ! ')\n",
    "\n",
    "        print('download training data and load training data of cifar10 dataset ! ')\n",
    "        self.Cifar10_train_dataset = datasets.Cifar10(mode='train',\n",
    "                                                      transform=transform)\n",
    "        # self.Cifar10_test_dataset = datasets.Cifar10(mode='test',\n",
    "        #                                              transform=transform)\n",
    "        print('load cifar10 dataset finished ! ')\n",
    "\n",
    "        self.path = common.DATA_HOME\n",
    "\n",
    "    def get_datasets(self):\n",
    "        #a = [(self.MNIST_train_dataset, \"MNIST\"), (self.Cifar10_train_dataset, \"Cifar10\"),\n",
    "        #    (self.Cifar10_train_dataset, \"Cifar10\")]\n",
    "        a = [(self.MNIST_train_dataset, \"MNIST\"), (self.Cifar10_train_dataset, \"Cifar10\")]\n",
    "        return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Âª∫Á´ã Dataloader\n",
    "- *.train_loader: ÊãøÂèñËÆ≠ÁªÉÈõÜÂπ∂ËÆ≠ÁªÉ \\\\\n",
    "- *.val_loader: ÊãøÂèñÈ™åËØÅÈõÜÂπ∂È™åÊµãÁªìÊûú \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "import paddle.fluid.dataloader.sampler as sampler\n",
    "\n",
    "\n",
    "class Dataloader():\n",
    "\n",
    "    def __init__(self, dataset, batch_size, split_ratio=0.1):\n",
    "        self.dataset = dataset[0]\n",
    "        self.name = dataset[1]\n",
    "        train_sampler, val_sampler = self.split_dataset(split_ratio)\n",
    "\n",
    "        self.train_dataset_size = len(train_sampler)\n",
    "        self.val_dataset_size = len(val_sampler)\n",
    "\n",
    "        bs_train = BatchSampler(sampler=train_sampler,\n",
    "                                shuffle=False,\n",
    "                                batch_size=batch_size,\n",
    "                                drop_last=True)\n",
    "        bs_val = BatchSampler(sampler=val_sampler,\n",
    "                              shuffle=False,\n",
    "                              batch_size=batch_size,\n",
    "                              drop_last=True)\n",
    "        self.train_loader = DataLoader(self.dataset, batch_sampler=bs_train)\n",
    "        self.val_loader = DataLoader(self.dataset, batch_sampler=bs_val)\n",
    "        # print(\"number of labels: {}\".format(len(set(self.dataset.labels))))\n",
    "        self.train_iter = self.infinite_iter()\n",
    "\n",
    "    def split_dataset(self, split_ratio):\n",
    "        data_size = len(self.dataset)\n",
    "        split = int(data_size * split_ratio)\n",
    "        indices = list(range(data_size))\n",
    "        np.random.shuffle(indices)\n",
    "        train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "        train_sampler = sampler.RandomSampler(train_idx)\n",
    "        val_sampler = sampler.RandomSampler(valid_idx)\n",
    "        return train_sampler, val_sampler\n",
    "\n",
    "    def infinite_iter(self):\n",
    "        it = iter(self.train_loader)\n",
    "        while True:\n",
    "            try:\n",
    "                ret = next(it)\n",
    "                yield ret\n",
    "            except StopIteration:\n",
    "                it = iter(self.train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Â∞èÂ∑•ÂÖ∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ÂÇ®Â≠òÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, store_model_path):\n",
    "    # save model and optimizer\n",
    "    paddle.save(model.state_dict(), f'{store_model_path}.pdparams')\n",
    "    paddle.save(optimizer.state_dict(), f'{store_model_path}.pdopt')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##ËΩΩÂÖ•Ê®°Âûã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ËΩΩÂÖ•Ê®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, load_model_path):\n",
    "    # load model and optimizer\n",
    "    print(f'Load model from {load_model_path}')\n",
    "    model_state_dict = paddle.load(f'{load_model_path}.pdparams')\n",
    "    opt_state_dict = paddle.load(f'{load_model_path}.pdopt')\n",
    "    model.set_state_dict(model_state_dict)\n",
    "    optimizer.set_state_dict(opt_state_dict)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Âª∫Á´ãÊ®°Âûã & ‰ºòÂåñÂô®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(data_path, batch_size, learning_rate):\n",
    "  # create model\n",
    "    model = Model()\n",
    "    optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=learning_rate)\n",
    "    data = Data()\n",
    "    datasets = data.get_datasets()\n",
    "    tasks = []\n",
    "    for dataset in datasets:\n",
    "        tasks.append(Dataloader(dataset, batch_size))\n",
    "\n",
    "    return model, optimizer, tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Ê≠£Â∏∏ËÆ≠ÁªÉ ( baseline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normal_train(model, optimizer, task, total_epochs, summary_epochs):\n",
    "    model.train()\n",
    "    model.clear_gradients()\n",
    "    ceriation = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    loss = 0.0\n",
    "    for epoch in range(summary_epochs):\n",
    "        imgs, labels = next(task.train_iter)\n",
    "        outputs = model(imgs)\n",
    "        ce_loss = ceriation(outputs, labels)\n",
    "\n",
    "        optimizer.clear_gradients()\n",
    "        ce_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += ce_loss.numpy()[0]\n",
    "        print(\"loss:{} of epoch:{} \".format(ce_loss.numpy()[0], epoch))\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            loss = loss / 50\n",
    "            print(\"\\r\", \"train task {} [{}] loss: {:.3f}      \\n\".format(task.name, (total_epochs + epoch + 1), loss),\n",
    "                  end=\" \")\n",
    "            losses.append(loss)\n",
    "            loss = 0.0\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EWC ËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ewc_train(model, optimizer, task, total_epochs, summary_epochs, ewc, lambda_ewc):\n",
    "    model.train()\n",
    "    model.clear_gradients()\n",
    "    ceriation = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    loss = 0.0\n",
    "    for epoch in range(summary_epochs):\n",
    "        imgs, labels = next(task.train_iter)\n",
    "        outputs = model(imgs)\n",
    "        ce_loss = ceriation(outputs, labels)\n",
    "        total_loss = ce_loss\n",
    "        ewc_loss = ewc.penalty(model)\n",
    "        total_loss += lambda_ewc * ewc_loss\n",
    "\n",
    "        optimizer.clear_gradients()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += total_loss.numpy()[0]\n",
    "        print(\"loss:{} of epoch:{} \".format(ce_loss.numpy()[0], epoch))\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            loss = loss / 50\n",
    "            print(\"\\r\", \"train task {} [{}] loss: {:.3f}      \\n\".format(task.name, (total_epochs + epoch + 1), loss),\n",
    "                  end=\" \")\n",
    "            losses.append(loss)\n",
    "            loss = 0.0\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MAS ËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mas_train(model, optimizer, task, total_epochs, summary_epochs, mas_tasks, lambda_mas, alpha=0.8):\n",
    "    model.train()\n",
    "    model.clear_gradients()\n",
    "    ceriation = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    loss = 0.0\n",
    "    for epoch in range(summary_epochs):\n",
    "        imgs, labels = next(task.train_iter)\n",
    "        # imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        ce_loss = ceriation(outputs, labels)\n",
    "        total_loss = ce_loss\n",
    "        mas_tasks.reverse()\n",
    "        if len(mas_tasks) > 1:\n",
    "            preprevious = 1 - alpha\n",
    "            scalars = [alpha, preprevious]\n",
    "            for mas, scalar in zip(mas_tasks[:2], scalars):\n",
    "                mas_loss = mas.penalty(model)\n",
    "                total_loss += lambda_mas * mas_loss * scalar\n",
    "        elif len(mas_tasks) == 1:\n",
    "            mas_loss = mas_tasks[0].penalty(model)\n",
    "            total_loss += lambda_mas * mas_loss\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        optimizer.clear_gradients()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += total_loss.numpy()[0]\n",
    "        print(\"loss:{} of epoch:{} \".format(ce_loss.numpy()[0], epoch))\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            loss = loss / 50\n",
    "            print(\"\\r\", \"train task {} [{}] loss: {:.3f}      \\n\".format(task.name, (total_epochs + epoch + 1), loss),\n",
    "                  end=\" \")\n",
    "            losses.append(loss)\n",
    "            loss = 0.0\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## È™åËØÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def val(model, task):\n",
    "    model.eval()\n",
    "    correct_cnt = 0\n",
    "    for imgs, labels in task.val_loader:\n",
    "        outputs = model(imgs)\n",
    "        pred_label = np.argmax(outputs.numpy(), axis=-1)\n",
    "\n",
    "        correct_cnt += (pred_label == labels.numpy()).sum()\n",
    "\n",
    "    return correct_cnt / task.val_dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ‰∏ªËÆ≠ÁªÉÁ®ãÂ∫è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_process(model, optimizer, tasks, config):\n",
    "    task_loss, acc = {}, {}\n",
    "    for task_id, task in enumerate(tasks):\n",
    "        print('\\n')\n",
    "        total_epochs = 0\n",
    "        task_loss[task.name] = []\n",
    "        acc[task.name] = []\n",
    "        if config.mode == 'basic' or task_id == 0:\n",
    "            while (total_epochs < config.num_epochs):\n",
    "                model, optimizer, losses = normal_train(model, optimizer, task, total_epochs,\n",
    "                                                                    config.summary_epochs)\n",
    "                task_loss[task.name] += losses\n",
    "\n",
    "                for subtask in range(task_id + 1):\n",
    "                    acc[tasks[subtask].name].append(val(model, tasks[subtask]))\n",
    "\n",
    "                total_epochs += config.summary_epochs\n",
    "                if total_epochs % config.store_epochs == 0 or total_epochs >= config.num_epochs:\n",
    "                    save_model(model, optimizer, config.store_model_path)\n",
    "\n",
    "        if config.mode == 'ewc' and task_id > 0:\n",
    "            old_dataloaders = []\n",
    "            for old_task in range(task_id):\n",
    "                old_dataloaders += [tasks[old_task].val_loader]\n",
    "            ewc = EWC(model, old_dataloaders, device)\n",
    "            while (total_epochs < config.num_epochs):\n",
    "                model, optimizer, losses = ewc_train(model, optimizer, task, total_epochs,\n",
    "                                                                 config.summary_epochs, ewc,\n",
    "                                                                 config.lifelong_coeff)\n",
    "                task_loss[task.name] += losses\n",
    "\n",
    "                for subtask in range(task_id + 1):\n",
    "                    acc[tasks[subtask].name].append(val(model, tasks[subtask]))\n",
    "\n",
    "                total_epochs += config.summary_epochs\n",
    "                if total_epochs % config.store_epochs == 0 or total_epochs >= config.num_epochs:\n",
    "                    save_model(model, optimizer, config.store_model_path)\n",
    "\n",
    "        if config.mode == 'mas' and task_id > 0:\n",
    "            old_dataloaders = []\n",
    "            mas_tasks = []\n",
    "            for old_task in range(task_id):\n",
    "                old_dataloaders += [tasks[old_task].val_loader]\n",
    "                mas = MAS(model, old_dataloaders, device)\n",
    "                mas_tasks += [mas]\n",
    "            while (total_epochs < config.num_epochs):\n",
    "                model, optimizer, losses = mas_train(model, optimizer, task, total_epochs,\n",
    "                                                                 config.summary_epochs,\n",
    "                                                                 mas_tasks, config.lifelong_coeff)\n",
    "                task_loss[task.name] += losses\n",
    "\n",
    "                for subtask in range(task_id + 1):\n",
    "                    acc[tasks[subtask].name].append(val(model, tasks[subtask]))\n",
    "\n",
    "                total_epochs += config.summary_epochs\n",
    "                if total_epochs % config.store_epochs == 0 or total_epochs >= config.num_epochs:\n",
    "                    save_model(model, optimizer, config.store_model_path)\n",
    "\n",
    "        if config.mode == 'scp' and task_id > 0:\n",
    "            pass\n",
    "            ########################################\n",
    "            ##       TODO Âå∫Âùó Ôºà PART 2 Ôºâ         ##\n",
    "            ########################################\n",
    "            ##    PART 2  implementation ÁöÑÈÉ®‰ªΩ    ##\n",
    "            ##   ‰Ω†‰πüÂèØ‰ª•ÂÜôÂà´ÁöÑ regularization ÊñπÊ≥ï  ##\n",
    "            ##    Âä©ÊïôËøôÈáåÊúâÊèê‰æõÁöÑÊòØ  scp    ÁöÑ ‰ΩúÊ≥ï   ##\n",
    "            ##     Slicer Cramer Preservation     ##\n",
    "            ########################################\n",
    "            ########################################\n",
    "            ##       TODO Âå∫Âùó Ôºà PART 2 Ôºâ         ##\n",
    "            ########################################\n",
    "    return task_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ËÆæÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class configurations(object):\n",
    "  def __init__(self):\n",
    "    self.batch_size = 256\n",
    "    self.num_epochs = 10000\n",
    "    self.store_epochs = 250\n",
    "    self.summary_epochs = 250\n",
    "    self.learning_rate = 0.0005\n",
    "    self.load_model = False\n",
    "    self.store_model_path = \"./model\"\n",
    "    self.load_model_path = \"./model\"\n",
    "    self.data_path = \"./data\"\n",
    "    self.mode = None\n",
    "    self.lifelong_coeff = 0.5\n",
    "\n",
    "###### ‰Ω†‰πüÂèØ‰ª•Ëá™Â∑±ËÆæÂÆöÂèÇÊï∞   ########\n",
    "###### ‰ΩÜ‰∏äÈù¢ÁöÑÂèÇÊï∞ ÊòØËøôÊ¨°‰Ωú‰∏öÁöÑÈ¢ÑËÆæÁõ¥ #########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#‰∏ªÁ®ãÂºèÂå∫Âùó\n",
    "- Áªô EWC, MAS Ë∂ÖÂèÇÊï∞ $\\lambda$ \n",
    "- ËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ewc training\n",
      "download training data and load training data of mnist dataset ! \n",
      "load mnist dataset finished ! \n",
      "download training data and load training data of cifar10 dataset ! \n",
      "load cifar10 dataset finished ! \n",
      "Finish build model\n",
      "\n",
      "\n",
      "loss:46.33511734008789 of epoch:0 \n",
      "loss:33.42926025390625 of epoch:1 \n",
      "loss:32.75161361694336 of epoch:2 \n",
      "loss:27.345964431762695 of epoch:3 \n",
      "loss:18.874662399291992 of epoch:4 \n",
      "loss:12.846699714660645 of epoch:0 \n",
      "loss:12.845219612121582 of epoch:1 \n",
      "loss:10.548949241638184 of epoch:2 \n",
      "loss:8.525252342224121 of epoch:3 \n",
      "loss:6.8116655349731445 of epoch:4 \n",
      "\n",
      "\n",
      "Computing EWC\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[3072, 1024], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[1024], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., ..., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[1024, 512], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[512], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[512, 256], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[256], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[256, 128], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[128], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[128, 128], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[128], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[128, 10], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[10], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "loss:2.3116037845611572 of epoch:0 \n",
      "loss:2.304475784301758 of epoch:1 \n",
      "loss:2.297457456588745 of epoch:2 \n",
      "loss:2.2906739711761475 of epoch:3 \n",
      "loss:2.2876973152160645 of epoch:4 \n",
      "loss:2.2779436111450195 of epoch:0 \n",
      "loss:2.2705955505371094 of epoch:1 \n",
      "loss:2.249182939529419 of epoch:2 \n",
      "loss:2.2467591762542725 of epoch:3 \n",
      "loss:2.233130931854248 of epoch:4 \n",
      "mas training\n",
      "download training data and load training data of mnist dataset ! \n",
      "load mnist dataset finished ! \n",
      "download training data and load training data of cifar10 dataset ! \n",
      "load cifar10 dataset finished ! \n",
      "Finish build model\n",
      "\n",
      "\n",
      "loss:43.74868392944336 of epoch:0 \n",
      "loss:46.93212890625 of epoch:1 \n",
      "loss:46.7099723815918 of epoch:2 \n",
      "loss:39.844512939453125 of epoch:3 \n",
      "loss:27.4708194732666 of epoch:4 \n",
      "loss:19.834367752075195 of epoch:0 \n",
      "loss:18.584468841552734 of epoch:1 \n",
      "loss:14.387933731079102 of epoch:2 \n",
      "loss:10.485311508178711 of epoch:3 \n",
      "loss:7.7073073387146 of epoch:4 \n",
      "\n",
      "\n",
      "Computing MAS\n",
      "loss:2.318295478820801 of epoch:0 \n",
      "loss:2.3155226707458496 of epoch:1 \n",
      "loss:2.3129923343658447 of epoch:2 \n",
      "loss:2.3052303791046143 of epoch:3 \n",
      "loss:2.2961201667785645 of epoch:4 \n",
      "loss:2.2931642532348633 of epoch:0 \n",
      "loss:2.273019552230835 of epoch:1 \n",
      "loss:2.2643775939941406 of epoch:2 \n",
      "loss:2.270339250564575 of epoch:3 \n",
      "loss:2.24211049079895 of epoch:4 \n",
      "basic training\n",
      "download training data and load training data of mnist dataset ! \n",
      "load mnist dataset finished ! \n",
      "download training data and load training data of cifar10 dataset ! \n",
      "load cifar10 dataset finished ! \n",
      "Finish build model\n",
      "\n",
      "\n",
      "loss:50.06311798095703 of epoch:0 \n",
      "loss:39.723419189453125 of epoch:1 \n",
      "loss:35.58266067504883 of epoch:2 \n",
      "loss:24.19887351989746 of epoch:3 \n",
      "loss:23.171886444091797 of epoch:4 \n",
      "loss:10.090304374694824 of epoch:0 \n",
      "loss:11.420814514160156 of epoch:1 \n",
      "loss:8.748123168945312 of epoch:2 \n",
      "loss:9.404326438903809 of epoch:3 \n",
      "loss:5.019579887390137 of epoch:4 \n",
      "\n",
      "\n",
      "loss:2.309945821762085 of epoch:0 \n",
      "loss:2.304807662963867 of epoch:1 \n",
      "loss:2.311410665512085 of epoch:2 \n",
      "loss:2.29559588432312 of epoch:3 \n",
      "loss:2.293915033340454 of epoch:4 \n",
      "loss:2.2782130241394043 of epoch:0 \n",
      "loss:2.280367612838745 of epoch:1 \n",
      "loss:2.26499342918396 of epoch:2 \n",
      "loss:2.264601230621338 of epoch:3 \n",
      "loss:2.2559287548065186 of epoch:4 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the order is mnist -> cifar10\n",
    "==============================================\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    #mode_list = ['mas', 'ewc', 'basic']\n",
    "    mode_list = ['ewc', 'mas', 'basic']\n",
    "\n",
    "    ## hint: Ë∞®ÊÖéÁöÑÂéªÈÄâÊã© lambda Ë∂ÖÂèÇÊï∞ / ewc: 80~400, mas: 0.1 - 10\n",
    "    ############################################################################\n",
    "    #####                           TODO Âå∫Âùó Ôºà PART 1 Ôºâ                  #####\n",
    "    ############################################################################\n",
    "    coeff_list = [0, 0, 0]  ## ‰Ω†ÈúÄË¶ÅÂú®Ëøô ÂæÆË∞É lambda ÂèÇÊï∞, mas, ewc, baseline=0 ##\n",
    "    ############################################################################\n",
    "    #####                           TODO Âå∫Âùó Ôºà PART 1 Ôºâ                  #####\n",
    "    ############################################################################\n",
    "\n",
    "    config = configurations()\n",
    "    count = 0\n",
    "    for mode in mode_list:\n",
    "        config.mode = mode\n",
    "        config.lifelong_coeff = coeff_list[count]\n",
    "        print(\"{} training\".format(config.mode))\n",
    "        model, optimizer, tasks = build_model(config.data_path, config.batch_size, config.learning_rate)\n",
    "        print(\"Finish build model\")\n",
    "        if config.load_model:\n",
    "            model, optimizer = load_model(model, optimizer, config.load_model_path)\n",
    "        task_loss, acc = train_process(model, optimizer, tasks, config)\n",
    "        with open(f'./{config.mode}_acc.txt', 'w') as f:\n",
    "            json.dump(acc, f)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ÁîªÂá∫ Result ÂõæÁâá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_result(mode_list, task1, task2):\n",
    "    # draw the lines\n",
    "    count = 0\n",
    "    for reg_name in mode_list:\n",
    "        label = reg_name\n",
    "        with open(f'./{reg_name}_acc.txt', 'r') as f:\n",
    "            acc = json.load(f)\n",
    "        if count == 0:\n",
    "            color = 'red'\n",
    "        elif count == 1:\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'purple'\n",
    "        ax1 = plt.subplot(2, 1, 1)\n",
    "        plt.plot(range(len(acc[task1])), acc[task1], color, label=label)\n",
    "        ax1.set_ylabel(task1)\n",
    "        ax2 = plt.subplot(2, 1, 2, sharex=ax1, sharey=ax1)\n",
    "        plt.plot(range(len(acc[task2])), acc[task2], color, label=label)\n",
    "        ax2.set_ylabel(task2)\n",
    "        count += 1\n",
    "    plt.ylim((0.02, 1.02))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "mode_list = ['ewc', 'mas', 'basic']\n",
    "plot_result(mode_list, 'MNIST', 'Cifar10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Âú®‰ªäÂπ¥ ICLR 2020 ÁöÑ paperÔºåÊúâ‰ª•Ëøô‰∏§ÁßçÊñπÊ≥ïÂÅö baselineÔºåÂπ∂ÂØπËøô‰∏§ÁßçÊñπÊ≥ïÂêÑËá™ÂÅö‰∫Ü‰∏Ä‰∏™ geometry viewÔºå‰πüÊèêÂá∫Êñ∞ÁöÑÊñπÊ≥ïÔºåÊúâÂÖ¥Ë∂£ÁöÑ‰∫∫ÂèØ‰ª•ÂèÇËÄÉ\n",
    "\n",
    "paper link Â¶Ç‰∏ã [SLICED CRAMER¬¥ SYNAPTIC CONSOLIDATION FOR\n",
    "PRESERVING DEEPLY LEARNED REPRESENTATIONS](https://openreview.net/pdf?id=BJge3TNKwH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ËøõÈò∂ \n",
    "ËØ∑ÂÆûÂÅöÂÖ∂‰ªñÁöÑ regularization ÁöÑÊñπÊ≥ïÔºåÂä©ÊïôÊúâÊèê‰æõÁöÑÊòØ SCP ÁöÑ‰ΩúÊ≥ïÔºå\n",
    "\n",
    "‰Ω†‰πüÂèØ‰ª•ËÄÉËôëÂÆûÂÅöÂá∫ SI, Rimennian Walk, IMM, ÊàñÊòØ‰∏äÈù¢ÁöÑÊñπÊ≥ï, \n",
    "\n",
    "‰Ω†ÂèØ‰ª•ÂèÇËÄÉÂä©Êïô‰∏äÊñπÁöÑÂÜôÊ≥ïÔºåÂÜôÂá∫Èõ∑ÂêåÁöÑ class Ë∑ü training Êù• trainÔºå\n",
    "\n",
    "ËÆ∞ÂæóÁîªÂá∫‰∏é‰∏äÊñπÈõ∑ÂêåÁöÑ evaluation ÂõæË°® (show result) example ÈúÄË¶ÅÊØîÂØπÁöÑËØù ÂèØ‰ª•ÂèÇËÄÉÂä©ÊïôÁªôÁöÑ slide„ÄÇ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_spherical(npoints, ndim=3):\n",
    "    vec = np.random.randn(ndim, npoints)\n",
    "    vec /= np.linalg.norm(vec, axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SCP(object):\n",
    "    \"\"\"\n",
    "    OPEN REVIEW VERSION:\n",
    "    https://openreview.net/forum?id=BJge3TNKwH\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Layer, dataloaders: list, L: int, device):\n",
    "        self.model = model \n",
    "        self.dataloaders = dataloaders\n",
    "        self.params = {n: p for n, p in self.model.named_parameters() if not p.stop_gradient}\n",
    "        self._means = {}\n",
    "        self.L= L\n",
    "        self.device = device\n",
    "        self._precision_matrices = self.calculate_importance()\n",
    "    \n",
    "        for n, p in self.params.items():\n",
    "            self._means[n] = p.clone().detach()\n",
    "    \n",
    "    def calculate_importance(self):\n",
    "        print('Computing SCP')\n",
    "\n",
    "        precision_matrices = {}\n",
    "        for n, p in self.params.items():  # ÂàùÂßãÂåñ Fisher (F) ÁöÑÁü©ÈòµÔºàÈÉΩË°•Èõ∂Ôºâ\n",
    "            t_val = p.clone().detach()\n",
    "            t_val.set_value(np.zeros(shape=t_val.numpy().shape, dtype=np.float32))\n",
    "            precision_matrices[n] = t_val.numpy()\n",
    "\n",
    "        self.model.eval()\n",
    "        dataloader_num = len(self.dataloaders)\n",
    "        num_data = sum([len(loader) for loader in self.dataloaders])\n",
    "        for dataloader in self.dataloaders:\n",
    "            for data in dataloader:\n",
    "                self.model.clear_gradients()\n",
    "                output = self.model(data[0])\n",
    "\n",
    "                ####################################################################################\n",
    "                #####                            TODO Âå∫Âùó Ôºà PART 2 Ôºâ                           #####\n",
    "                ####################################################################################\n",
    "                ##### ‰∫ßÁîü SCP ÁöÑ Gamma(Œì) Áü©ÈòµÔºà Â¶ÇÂêå MAS ÁöÑ Omega(Œ©) Áü©Èòµ, EWC ÁöÑ Fisher(F) Áü©Èòµ Ôºâ#####\n",
    "                ####################################################################################\n",
    "                #####        1.ÂØπÊâÄÊúâËµÑÊñôÁöÑ Output vector Âèñ Âπ≥Âùá ÂæóÂà∞ Âπ≥Âùá vector œÜ(:,Œ∏_A* )       #####\n",
    "                ####################################################################################\n",
    "\n",
    "                ####################################################################################\n",
    "                #####   2. ÈöèÊú∫ ‰ªé Âçï‰ΩçÁêÉÂ£≥ ÂèñÊ†∑ L ‰∏™ vector Œæ #Ôºà Hint: sample_spherical() Ôºâ      #####\n",
    "                ####################################################################################\n",
    "\n",
    "                ####################################################################################\n",
    "                #####   3.    ÊØè‰∏Ä‰∏™ vector Œæ Âíå vector œÜ( :,Œ∏_A* )ÂÜÖÁßØÂæóÂà∞ scalar œÅ               #####\n",
    "                #####           ÂØπ scalar œÅ Âèñ backward Ôºå ÊØè‰∏™ÂèÇÊï∞ÂæóÂà∞ÂêÑËá™ÁöÑ gradient ‚àáœÅ           #####\n",
    "                #####       ÊØè‰∏™ÂèÇÊï∞ÁöÑ gradient ‚àáœÅ ÂèñÂπ≥Êñπ Âèñ L Âπ≥Âùá ÂæóÂà∞ ÂêÑ‰∏™ÂèÇÊï∞ÁöÑ Œì scalar          #####  \n",
    "                #####              ÊâÄÊúâÂèÇÊï∞ÁöÑ  Œì scalar ÁªÑÂêàËÄåÊàêÂÖ∂ÂÆûÂ∞±ÊòØ Œì Áü©Èòµ                      #####\n",
    "                ####(hint:ËÆ∞ÂæóÊØèÊ¨°backward‰πãÂêéË¶Åclear_gradients ÂéªÊ∏Ögradient, ‰∏çÁÑ∂ gradient‰ºöÁ¥ØÂä†)   ######   \n",
    "                ####################################################################################\n",
    "      \n",
    "                ####################################################################################      \n",
    "                #####                            TODO Âå∫Âùó Ôºà PART 2 Ôºâ                          #####\n",
    "                ####################################################################################\n",
    "\n",
    "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "        return precision_matrices\n",
    "\n",
    "    def penalty(self, model: nn.Layer):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            _loss = self._precision_matrices[n] * (p - self._means[n]).numpy() ** 2\n",
    "            loss += _loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scp_train(model, optimizer, task, total_epochs, summary_epochs, scp_tasks, lambda_scp,alpha=0.65):\n",
    "  losses = []\n",
    "  loss = 0.0\n",
    "  ###############################\n",
    "  #####  TODO Âå∫Âùó ÔºàPART 2Ôºâ #####\n",
    "  ###############################\n",
    "  ##  ÂèÇËÄÉ MAS. EWC train ÁöÑÂÜôÊ≥ï ##                 \n",
    "  ###############################\n",
    "  #####  TODO Âå∫Âùó ÔºàPART 2Ôºâ #####\n",
    "  ###############################\n",
    "  return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\": \n",
    "#   pass \n",
    "###############################\n",
    "#####  TODO Âå∫Âùó ÔºàPART 2Ôºâ #####\n",
    "###############################\n",
    "##     ÂèÇËÄÉ main Âå∫Âùó‰∏ÄÊ†∑       ##                 \n",
    "##     ÁöÑ code ÁµêÂêàÊñ∞ÊñπÊ≥ï       ##\n",
    "###############################\n",
    "#####  TODO Âå∫Âùó ÔºàPART 2Ôºâ #####\n",
    "###############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
