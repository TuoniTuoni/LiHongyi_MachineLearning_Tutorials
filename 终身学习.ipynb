{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目8-终身学习\n",
    "\n",
    "## 友情提示\n",
    "同学们可以前往课程作业区先行动手尝试 ！！！\n",
    "\n",
    "## 项目描述\n",
    "\n",
    "* 使用EWC - Elastic Weight Consolidation和MAS - Memory Aware Synapse两种regularization based lifelong learning 的方法在Paddle的两个不同的数据集MNIST和Cifar10上进行学习训练\n",
    "* 根据给出的两种lifelong learning regularization算法自己做一个SCP算法\n",
    "\n",
    "## 数据集介绍\n",
    "\n",
    "本次使用的数据集为MNIST和Cifar10数据集，共有10类\n",
    "\n",
    "MNIST数据集：\n",
    "图像都是28x28大小的灰度图像，每个像素的是一个八位字节（0~255）\n",
    "识别数字0～9（0，1，2，3，4，5，6，7，8，9）\n",
    "Training set: 60000\n",
    "Testing set: 10000张\n",
    "\n",
    "Cifar10数据集：\n",
    "图像都是32x32大小的彩色图像，每个像素🈶由一个3x八位字节（0~255）组成\n",
    "识别'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "Training set: 50000张\n",
    "Testing set: 10000张\n",
    "\n",
    "**数据格式**\n",
    "\n",
    "Paddle中自带数据集，在代码中下载数据包\n",
    "\n",
    "## 项目要求\n",
    "\n",
    "* 请以中文说明一下 lifelong learning 的中心概念是什么？\n",
    "* 列出EWC,MAS的作法是什么？根据你的理解，说明一下大概的流程该怎么做\n",
    "* EWC和MAS方法上所需要的资料最大的差异是什么\n",
    "* 秀出part1及part2最后结果的图，并分析一下结果，以及你跑的实验中有什么发现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据准备\n",
    "        \n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境配置/安装\n",
    "\n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 终身机器学习\n",
    "\n",
    "### 方法\n",
    "在2019年底，有人提出了一个大汇整将lifelong learning 的方法，从2016- 2019 年初 的模型做了归类，大致上可以分成三种大方法\n",
    "* Replay-based methods\n",
    "* Regularization-based methods\n",
    "* Parameter isolation methods\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/dc2465afd2ee4e51a35d93b8129dba025afa68f7fbe44993a11bc73891f45214)\n",
    "\n",
    "\n",
    "在这次的作业之中，我们要走过一次regularization-based methods 里面的 prior-focused的两种方法 分别是 EWC 和 MAS 这两种方法\n",
    "\n",
    "图片出处 [Continual Learning in Neural\n",
    "Networks](https://arxiv.org/pdf/1910.02718.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# 进入官网适合自己的paddlepaddle版本，并运行相应的安装命令安装paddle paddle 2.0 rc版本\n",
    "# !python -m pip install paddlepaddle-gpu==2.0.0rc0.post90 -f https://paddlepaddle.org.cn/whl/stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as optim\n",
    "import paddle.nn.functional as F\n",
    "import paddle.fluid.data as data\n",
    "import paddle.fluid.core as core\n",
    "import paddle.fluid.dataloader.sampler as sampler\n",
    "from paddle import vision\n",
    "from paddle.io import DataLoader\n",
    "from paddle.vision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "# 如果支持gpu则使用gpu训练否则使用cpu训练\n",
    "support_gpu = paddle.is_compiled_with_cuda()\n",
    "place = paddle.CPUPlace()\n",
    "if support_gpu:\n",
    "    place = paddle.CUDAPlace(0)\n",
    "paddle.disable_static(place)\n",
    "print(paddle.__version__)\n",
    "print(\"place:{} \".format(place))\n",
    "\n",
    "device = paddle.set_device(\"gpu\" if core.is_compiled_with_cuda() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " >因为本次作业强调的是lifelong learning 的训练方法，并非叠模型，所以今天我们所举的例子，都会使用同一个模型来做训练只是应用上不同lifelong learning的训练方法， 在这次的作业的例子内 我们使用的是一个六层的fully-connected layer的模型加上relu的activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 基准模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.fc1 = nn.Linear(3*32*32, 1024)\n",
    "    self.fc2 = nn.Linear(1024, 512)\n",
    "    self.fc3 = nn.Linear(512, 256)\n",
    "    self.fc4 = nn.Linear(256, 128)\n",
    "    self.fc5 = nn.Linear(128, 128)\n",
    "    self.fc6 = nn.Linear(128, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.reshape((-1, 3*32*32))\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc4(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc5(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc6(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以下我们將依序介紹这两种方法 EWC 跟 MAS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Elastic Weight Consolidation\n",
    "\n",
    "#### 概念\n",
    "老师在影片中已经把核心概念介绍给大家，那在这边我想大家都非常了解了这个方法的概念，我们就直接进入主题\n",
    "\n",
    "今天我们的任务 是在学习连续的两个 task task A 跟 task B:\n",
    "\n",
    "在 EWC 作法下 他的 loss function 会被定义如下\n",
    " $$\\mathcal{L}_B = \\mathcal{L}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_i (\\theta_{i} - \\theta_{A,i}^{*})^2  $$\n",
    "\n",
    "先解释这个 loss function 裡的变数，$\\mathcal{L}_B$ 是指 task B 的 loss, 会等于 正常的loss function $\\mathcal{L}(\\theta)$ (如果 是 classification 的问题,就是 cross entropy 的 loss function) 加上一个正则项 (regularization term) \n",
    "\n",
    "这个正则项的由两个部份组成，第一个是 $F_i$ 也是这个方法的核心, 第二个部份是 $(\\theta_{i} - \\theta_{A,i}^{*})^2$  ,  $\\theta_{A,i}^{*}$ 代表的是 训练完task A 存下来模型第 i 个参数的值, $\\theta_i$ 代表的是目前模型第i个参数的值，注意一点是模型的架构在这种 regularization based 的方法上，都是固定ㄉ，目前模型跟 task A 存下来的模型 架构都一样只是值不一样。底下我将说明这个 $F_i$ 是怎么实做出来\n",
    "\n",
    "在老师的影片中，老师是以只有两个参数的模型举例子，那假设我今天模型就是一个 neural network(参数不只两个) 该怎么办呢？   \n",
    "\n",
    "$F_i$ 对应到老师的影片叙述是指第i个参数的守卫，假设这个参数对 task A 很重要，那这个 $F_i$ 的值就会很大，这个参数尽量不能被更动...\n",
    "\n",
    "实际上这个参数的算法 即是 如下的式子\n",
    "\n",
    "$$ F = [ \\nabla \\log(p(y_n | x_n, \\theta_{A}^{*}) \\nabla \\log(p(y_n | x_n, \\theta_{A}^{*})^T ] $$ \n",
    "\n",
    "$F$ 之中 只以对角线的值去近似各个参数的 $F_i$ 值\n",
    "\n",
    "$p(y_n | x_n, \\theta_{A}^{*})$ 指的就是模型在给定之前 task 的 data $x_n$ 以及 给定 训练完 task A 存下来的模型参数 $\\theta_A^*$ 得到 $y_n$($x_n$ 对应的 label ) 的 posterior probability.\n",
    "那统整一下作法就是 再对这个 $p(y_n | x_n, \\theta_{A}^{*})$ 取 log 再取 gradient 并且平方 ( parameter.grad )^2.\n",
    "\n",
    "每一个参数我都可以使用 paddle 的 backward 之后再取 gradient 的性质算出各自的 $F_i$.\n",
    "\n",
    "有关这个 $F$ 其实博大精深，是来自于 fisher information matrix. 底下我放上有关这个lifelong learning 在 fisher information matrix 上是怎么简单的近似到这一项，简单的推导来自 [Continual Learning in Neural\n",
    "Networks](https://arxiv.org/pdf/1910.02718.pdf) 第2.4.1 小节 与 2.4 节\n",
    "\n",
    "For You Information: [Elastic Weight Consolidation](https://arxiv.org/pdf/1612.00796.pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EWC(object):\n",
    "  \"\"\"\n",
    "    @article{kirkpatrick2017overcoming,\n",
    "        title={Overcoming catastrophic forgetting in neural networks},\n",
    "        author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},\n",
    "        journal={Proceedings of the national academy of sciences},\n",
    "        year={2017},\n",
    "        url={https://arxiv.org/abs/1612.00796}\n",
    "    }\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, model: nn.Layer, dataloaders: list, device):\n",
    "\n",
    "      self.model = model\n",
    "      self.dataloaders = dataloaders\n",
    "      self.device = device\n",
    "\n",
    "      self.params = {n: p for n, p in self.model.named_parameters() if not p.stop_gradient}  # 抓出模型的所有参数\n",
    "      self._means = {}  # 初始化 平均参数\n",
    "      self._precision_matrices = self._calculate_importance()  # 产生 EWC 的 Fisher (F) 矩阵\n",
    "\n",
    "      for n, p in self.params.items():\n",
    "          self._means[n] = p.clone().detach()  # 算出每个参数的平均 （用之前任务的资料去算平均）\n",
    "\n",
    "  def _calculate_importance(self):\n",
    "      print('Computing EWC')\n",
    "\n",
    "      precision_matrices = {}\n",
    "      for n, p in self.params.items():  # 初始化 Fisher (F) 的矩阵（都补零）\n",
    "          t_val = p.clone().detach()\n",
    "          print(type(t_val))\n",
    "          t_val.set_value(np.zeros(shape=t_val.numpy().shape, dtype=np.float32))\n",
    "          print(t_val)\n",
    "          precision_matrices[n] = t_val.numpy()\n",
    "\n",
    "      self.model.eval()\n",
    "      dataloader_num = len(self.dataloaders)\n",
    "      number_data = sum([len(loader) for loader in self.dataloaders])\n",
    "      for dataloader in self.dataloaders:\n",
    "          for data in dataloader:\n",
    "              self.model.clear_gradients()\n",
    "              input = data[0]\n",
    "              output = self.model(input)\n",
    "              label = np.argmax(output.numpy(), axis=-1)\n",
    "              label = paddle.to_tensor(label)\n",
    "\n",
    "              ############################################################################\n",
    "              #####                      产生 EWC 的 Fisher(F) 矩阵                    #####\n",
    "              ############################################################################\n",
    "              loss = F.nll_loss(F.log_softmax(output, axis=1), label)\n",
    "              loss.backward()\n",
    "              for n, p in self.model.named_parameters():\n",
    "                  precision_matrices[n] += np.power(p.grad, 2) / number_data\n",
    "\n",
    "      precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "      return precision_matrices\n",
    "\n",
    "  def penalty(self, model: nn.Layer):\n",
    "      loss = 0\n",
    "      for n, p in model.named_parameters():\n",
    "          _loss = self._precision_matrices[n] * (p - self._means[n]).numpy() ** 2\n",
    "          loss += _loss.sum()\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Memory Aware Synapses\n",
    "概念:\n",
    "老师的影片中，将它归类到和 EWC 一样的方法，只是算这个 important weight 的方式不太一样.底下我将说明这个方法该怎么实做\n",
    "\n",
    "MAS:\n",
    "在 MAS 内，学习一个连续的 tasks, task A, 和 task B, 他的 loss function 定义如下:\n",
    "\n",
    "$$\\mathcal{L}_B = \\mathcal{L}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} \\Omega_i (\\theta_{i} - \\theta_{A,i}^{*})^2$$\n",
    "\n",
    "和 ewc不同的是 式子中的 $F_i$ 被取代成 $\\Omega_i$ , $\\Omega_i$ 来自于以下的式子：\n",
    "\n",
    "$$\\Omega_i = || \\frac{\\partial \\ell_2^2(M(x_k; \\theta))}{\\partial \\theta_i} || $$ \n",
    "\n",
    "$x_k$ 是 来自于 前面 task 的 sample data。 式子上的作法就是对最后模型的 output vector (最后一层)做 l2 norm 后取平方 再对各自的weight微分(取gradient) 并且取 该 gradient 的绝对值，在该paper 中其实也可以对各个层的 output vector 做 l2 norm ( local 版本)，这边只实做 global 的版本。\n",
    "\n",
    "\n",
    "For Your Information: \n",
    "[Memory Aware Synapses](https://arxiv.org/pdf/1711.09601.pdf)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MAS(object):\n",
    "    \"\"\"\n",
    "    @article{aljundi2017memory,\n",
    "      title={Memory Aware Synapses: Learning what (not) to forget},\n",
    "      author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},\n",
    "      booktitle={ECCV},\n",
    "      year={2018},\n",
    "      url={https://eccv2018.org/openaccess/content_ECCV_2018/papers/Rahaf_Aljundi_Memory_Aware_Synapses_ECCV_2018_paper.pdf}\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Layer, dataloaders: list, device):\n",
    "        self.model = model\n",
    "        self.dataloaders = dataloaders\n",
    "        self.params = {n: p for n, p in self.model.named_parameters() if not p.stop_gradient}  # 抓出模型的所有参数\n",
    "        self._means = {}  # 初始化 平均参数\n",
    "        self.device = device\n",
    "        self._precision_matrices = self.calculate_importance()  # 产生 MAS 的 Omega(Ω) 矩阵\n",
    "\n",
    "        for n, p in self.params.items():\n",
    "            self._means[n] = p.clone().detach()\n",
    "\n",
    "    def calculate_importance(self):\n",
    "        print('Computing MAS')\n",
    "\n",
    "        precision_matrices = {}\n",
    "        for n, p in self.params.items():\n",
    "            # 初始化 Omega(Ω) 矩阵（都补0）\n",
    "            t_val = p.clone().detach()\n",
    "            t_val.set_value(np.zeros(shape=t_val.numpy().shape, dtype=np.float32))\n",
    "            precision_matrices[n] = t_val.numpy()\n",
    "\n",
    "        self.model.eval()\n",
    "        dataloader_num = len(self.dataloaders)\n",
    "        num_data = sum([len(loader) for loader in self.dataloaders])\n",
    "        for dataloader in self.dataloaders:\n",
    "            for data in dataloader:\n",
    "                self.model.clear_gradients()\n",
    "                # output = self.model(data[0].to(self.device))\n",
    "                output = self.model(data[0])\n",
    "\n",
    "                #######################################################################################\n",
    "                #####  产生 MAS 的 Omega(Ω) 矩阵 ( 对 output 向量 算他的 l2 norm 的平方) 再取 gradient  ####\n",
    "                #######################################################################################\n",
    "                output = paddle.pow(output, 2)\n",
    "                #output.pow_(2)\n",
    "                loss = paddle.sum(output, axis=1)\n",
    "                loss = loss.mean()\n",
    "                loss.backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    precision_matrices[n] += abs(p.grad) / num_data\n",
    "\n",
    "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "        return precision_matrices\n",
    "\n",
    "    def penalty(self, model: nn.Layer):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            _loss = self._precision_matrices[n] * (p - self._means[n]).numpy() ** 2\n",
    "            loss += _loss.sum()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 资料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 资料预处理\n",
    "- 转换 MNIST  ($1*28*28$) 到 ($3*32*32$)\n",
    "- 转换 USPS   ($1*16*16$) 到 ($3*32*32$)\n",
    "- 正规化 图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "from paddle.vision import transforms\n",
    "import paddle.nn.functional as F\n",
    "from paddle.vision.transforms import functional\n",
    "\n",
    "\n",
    "class ResizeImg(object):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_size_0 = img.shape[0]\n",
    "        img_size_1 = img.shape[1]\n",
    "        if (self.size - img_size_0) % 2 == 1:\n",
    "            img_size_0 += 1\n",
    "        if (self.size - img_size_1) % 2 == 1:\n",
    "            img_size_1 += 1\n",
    "        img = functional.resize(img, (img_size_0, img_size_1))\n",
    "        idx_tuple = np.where(img == 0)\n",
    "        if idx_tuple:\n",
    "            for i in range(len(idx_tuple[0])):\n",
    "                img[idx_tuple[0][i]][idx_tuple[1][i]][idx_tuple[2][i]] = 1\n",
    "        print(\"shape of img:{} \".format(img.shape))\n",
    "        return img\n",
    "\n",
    "\n",
    "class Convert2RGB(object):\n",
    "\n",
    "    def __init__(self, num_channel):\n",
    "        self.num_channel = num_channel\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # If the channel of img is not equal to desired size,\n",
    "        # then expand the channel of img to desired size.\n",
    "        img_channel = img.shape[0]\n",
    "        img = paddle.concat([img] * (self.num_channel - img_channel + 1), 0)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Pad(object):\n",
    "\n",
    "    def __init__(self, size, fill=0, padding_mode='constant'):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # If the H and W of img is not equal to desired size,\n",
    "        # then pad the channel of img to desired size.'\n",
    "        img = img.reshape((img.shape[1], img.shape[0], img.shape[2]))\n",
    "        img_size_h = img.shape[1]\n",
    "        img_size_w = img.shape[2]\n",
    "        left_pad = (self.size - img_size_w) // 2\n",
    "        right_pad = self.size - left_pad - img_size_w\n",
    "        up_pad = (self.size - img_size_h) // 2\n",
    "        low_pad = self.size - up_pad - img_size_h\n",
    "        img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "        pad_rs = F.pad(img, [left_pad, right_pad, up_pad, low_pad], value=1, mode='constant', data_format=\"NCHW\")\n",
    "        pad_rs = pad_rs.reshape((pad_rs.shape[-3], pad_rs.shape[-2], pad_rs.shape[-1]))\n",
    "        return pad_rs\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    transform = transforms.Compose([  # ResizeImg(32),\n",
    "        transforms.ToTensor(),\n",
    "        Pad(32),\n",
    "        Convert2RGB(3),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 准备 资料集\n",
    "- MNIST   : 一张图片资料大小:  $28*28*1$, 灰阶 , 10 个种类\n",
    "- SVHN    : 一张图片资料大小:  $32*32*3$, RGB , 10 个种类\n",
    "- USPS    : 一张图片资料大小:  $16*16*1$, 灰阶 , 10 个种类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from paddle.dataset import common\n",
    "from paddle.vision import datasets\n",
    "\n",
    "\n",
    "class Data():\n",
    "\n",
    "    def __init__(self):\n",
    "        transform = get_transform()\n",
    "\n",
    "        print('download training data and load training data of mnist dataset ! ')\n",
    "        self.MNIST_train_dataset = datasets.MNIST(mode='train',\n",
    "                                                  transform=transform)\n",
    "        # self.MNIST_test_dataset = datasets.MNIST(mode='test',\n",
    "        #                                          transform=transform)\n",
    "        print('load mnist dataset finished ! ')\n",
    "\n",
    "        # 如果需要三个数据集，可以使用Flowers数据集，但是需要修改网络中的类别数目\n",
    "        #print('download training data and load training data of flowers dataset ! ')\n",
    "        #self.Flowers_train_dataset = datasets.Flowers(mode='train',\n",
    "        #                                             transform=transform)\n",
    "        # self.Flowers_test_dataset = datasets.MNIST(mode='test',\n",
    "        #                                            transform=transform)\n",
    "        #print('load flowers dataset finished ! ')\n",
    "\n",
    "        print('download training data and load training data of cifar10 dataset ! ')\n",
    "        self.Cifar10_train_dataset = datasets.Cifar10(mode='train',\n",
    "                                                      transform=transform)\n",
    "        # self.Cifar10_test_dataset = datasets.Cifar10(mode='test',\n",
    "        #                                              transform=transform)\n",
    "        print('load cifar10 dataset finished ! ')\n",
    "\n",
    "        self.path = common.DATA_HOME\n",
    "\n",
    "    def get_datasets(self):\n",
    "        #a = [(self.MNIST_train_dataset, \"MNIST\"), (self.Cifar10_train_dataset, \"Cifar10\"),\n",
    "        #    (self.Cifar10_train_dataset, \"Cifar10\")]\n",
    "        a = [(self.MNIST_train_dataset, \"MNIST\"), (self.Cifar10_train_dataset, \"Cifar10\")]\n",
    "        return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 建立 Dataloader\n",
    "- *.train_loader: 拿取训练集并训练 \\\\\n",
    "- *.val_loader: 拿取验证集并验测结果 \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "import paddle.fluid.dataloader.sampler as sampler\n",
    "\n",
    "\n",
    "class Dataloader():\n",
    "\n",
    "    def __init__(self, dataset, batch_size, split_ratio=0.1):\n",
    "        self.dataset = dataset[0]\n",
    "        self.name = dataset[1]\n",
    "        train_sampler, val_sampler = self.split_dataset(split_ratio)\n",
    "\n",
    "        self.train_dataset_size = len(train_sampler)\n",
    "        self.val_dataset_size = len(val_sampler)\n",
    "\n",
    "        bs_train = BatchSampler(sampler=train_sampler,\n",
    "                                shuffle=False,\n",
    "                                batch_size=batch_size,\n",
    "                                drop_last=True)\n",
    "        bs_val = BatchSampler(sampler=val_sampler,\n",
    "                              shuffle=False,\n",
    "                              batch_size=batch_size,\n",
    "                              drop_last=True)\n",
    "        self.train_loader = DataLoader(self.dataset, batch_sampler=bs_train)\n",
    "        self.val_loader = DataLoader(self.dataset, batch_sampler=bs_val)\n",
    "        # print(\"number of labels: {}\".format(len(set(self.dataset.labels))))\n",
    "        self.train_iter = self.infinite_iter()\n",
    "\n",
    "    def split_dataset(self, split_ratio):\n",
    "        data_size = len(self.dataset)\n",
    "        split = int(data_size * split_ratio)\n",
    "        indices = list(range(data_size))\n",
    "        np.random.shuffle(indices)\n",
    "        train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "        train_sampler = sampler.RandomSampler(train_idx)\n",
    "        val_sampler = sampler.RandomSampler(valid_idx)\n",
    "        return train_sampler, val_sampler\n",
    "\n",
    "    def infinite_iter(self):\n",
    "        it = iter(self.train_loader)\n",
    "        while True:\n",
    "            try:\n",
    "                ret = next(it)\n",
    "                yield ret\n",
    "            except StopIteration:\n",
    "                it = iter(self.train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 小工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 储存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, store_model_path):\n",
    "    # save model and optimizer\n",
    "    paddle.save(model.state_dict(), f'{store_model_path}.pdparams')\n",
    "    paddle.save(optimizer.state_dict(), f'{store_model_path}.pdopt')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##载入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model(model, optimizer, load_model_path):\n",
    "    # load model and optimizer\n",
    "    print(f'Load model from {load_model_path}')\n",
    "    model_state_dict = paddle.load(f'{load_model_path}.pdparams')\n",
    "    opt_state_dict = paddle.load(f'{load_model_path}.pdopt')\n",
    "    model.set_state_dict(model_state_dict)\n",
    "    optimizer.set_state_dict(opt_state_dict)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 建立模型 & 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(data_path, batch_size, learning_rate):\n",
    "  # create model\n",
    "    model = Model()\n",
    "    optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=learning_rate)\n",
    "    data = Data()\n",
    "    datasets = data.get_datasets()\n",
    "    tasks = []\n",
    "    for dataset in datasets:\n",
    "        tasks.append(Dataloader(dataset, batch_size))\n",
    "\n",
    "    return model, optimizer, tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 正常训练 ( baseline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normal_train(model, optimizer, task, total_epochs, summary_epochs):\n",
    "    model.train()\n",
    "    model.clear_gradients()\n",
    "    ceriation = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    loss = 0.0\n",
    "    for epoch in range(summary_epochs):\n",
    "        imgs, labels = next(task.train_iter)\n",
    "        outputs = model(imgs)\n",
    "        ce_loss = ceriation(outputs, labels)\n",
    "\n",
    "        optimizer.clear_gradients()\n",
    "        ce_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += ce_loss.numpy()[0]\n",
    "        print(\"loss:{} of epoch:{} \".format(ce_loss.numpy()[0], epoch))\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            loss = loss / 50\n",
    "            print(\"\\r\", \"train task {} [{}] loss: {:.3f}      \\n\".format(task.name, (total_epochs + epoch + 1), loss),\n",
    "                  end=\" \")\n",
    "            losses.append(loss)\n",
    "            loss = 0.0\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EWC 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ewc_train(model, optimizer, task, total_epochs, summary_epochs, ewc, lambda_ewc):\n",
    "    model.train()\n",
    "    model.clear_gradients()\n",
    "    ceriation = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    loss = 0.0\n",
    "    for epoch in range(summary_epochs):\n",
    "        imgs, labels = next(task.train_iter)\n",
    "        outputs = model(imgs)\n",
    "        ce_loss = ceriation(outputs, labels)\n",
    "        total_loss = ce_loss\n",
    "        ewc_loss = ewc.penalty(model)\n",
    "        total_loss += lambda_ewc * ewc_loss\n",
    "\n",
    "        optimizer.clear_gradients()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += total_loss.numpy()[0]\n",
    "        print(\"loss:{} of epoch:{} \".format(ce_loss.numpy()[0], epoch))\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            loss = loss / 50\n",
    "            print(\"\\r\", \"train task {} [{}] loss: {:.3f}      \\n\".format(task.name, (total_epochs + epoch + 1), loss),\n",
    "                  end=\" \")\n",
    "            losses.append(loss)\n",
    "            loss = 0.0\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MAS 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mas_train(model, optimizer, task, total_epochs, summary_epochs, mas_tasks, lambda_mas, alpha=0.8):\n",
    "    model.train()\n",
    "    model.clear_gradients()\n",
    "    ceriation = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    loss = 0.0\n",
    "    for epoch in range(summary_epochs):\n",
    "        imgs, labels = next(task.train_iter)\n",
    "        # imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        ce_loss = ceriation(outputs, labels)\n",
    "        total_loss = ce_loss\n",
    "        mas_tasks.reverse()\n",
    "        if len(mas_tasks) > 1:\n",
    "            preprevious = 1 - alpha\n",
    "            scalars = [alpha, preprevious]\n",
    "            for mas, scalar in zip(mas_tasks[:2], scalars):\n",
    "                mas_loss = mas.penalty(model)\n",
    "                total_loss += lambda_mas * mas_loss * scalar\n",
    "        elif len(mas_tasks) == 1:\n",
    "            mas_loss = mas_tasks[0].penalty(model)\n",
    "            total_loss += lambda_mas * mas_loss\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        optimizer.clear_gradients()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += total_loss.numpy()[0]\n",
    "        print(\"loss:{} of epoch:{} \".format(ce_loss.numpy()[0], epoch))\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            loss = loss / 50\n",
    "            print(\"\\r\", \"train task {} [{}] loss: {:.3f}      \\n\".format(task.name, (total_epochs + epoch + 1), loss),\n",
    "                  end=\" \")\n",
    "            losses.append(loss)\n",
    "            loss = 0.0\n",
    "\n",
    "    return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def val(model, task):\n",
    "    model.eval()\n",
    "    correct_cnt = 0\n",
    "    for imgs, labels in task.val_loader:\n",
    "        outputs = model(imgs)\n",
    "        pred_label = np.argmax(outputs.numpy(), axis=-1)\n",
    "\n",
    "        correct_cnt += (pred_label == labels.numpy()).sum()\n",
    "\n",
    "    return correct_cnt / task.val_dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 主训练程序\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_process(model, optimizer, tasks, config):\n",
    "    task_loss, acc = {}, {}\n",
    "    for task_id, task in enumerate(tasks):\n",
    "        print('\\n')\n",
    "        total_epochs = 0\n",
    "        task_loss[task.name] = []\n",
    "        acc[task.name] = []\n",
    "        if config.mode == 'basic' or task_id == 0:\n",
    "            while (total_epochs < config.num_epochs):\n",
    "                model, optimizer, losses = normal_train(model, optimizer, task, total_epochs,\n",
    "                                                                    config.summary_epochs)\n",
    "                task_loss[task.name] += losses\n",
    "\n",
    "                for subtask in range(task_id + 1):\n",
    "                    acc[tasks[subtask].name].append(val(model, tasks[subtask]))\n",
    "\n",
    "                total_epochs += config.summary_epochs\n",
    "                if total_epochs % config.store_epochs == 0 or total_epochs >= config.num_epochs:\n",
    "                    save_model(model, optimizer, config.store_model_path)\n",
    "\n",
    "        if config.mode == 'ewc' and task_id > 0:\n",
    "            old_dataloaders = []\n",
    "            for old_task in range(task_id):\n",
    "                old_dataloaders += [tasks[old_task].val_loader]\n",
    "            ewc = EWC(model, old_dataloaders, device)\n",
    "            while (total_epochs < config.num_epochs):\n",
    "                model, optimizer, losses = ewc_train(model, optimizer, task, total_epochs,\n",
    "                                                                 config.summary_epochs, ewc,\n",
    "                                                                 config.lifelong_coeff)\n",
    "                task_loss[task.name] += losses\n",
    "\n",
    "                for subtask in range(task_id + 1):\n",
    "                    acc[tasks[subtask].name].append(val(model, tasks[subtask]))\n",
    "\n",
    "                total_epochs += config.summary_epochs\n",
    "                if total_epochs % config.store_epochs == 0 or total_epochs >= config.num_epochs:\n",
    "                    save_model(model, optimizer, config.store_model_path)\n",
    "\n",
    "        if config.mode == 'mas' and task_id > 0:\n",
    "            old_dataloaders = []\n",
    "            mas_tasks = []\n",
    "            for old_task in range(task_id):\n",
    "                old_dataloaders += [tasks[old_task].val_loader]\n",
    "                mas = MAS(model, old_dataloaders, device)\n",
    "                mas_tasks += [mas]\n",
    "            while (total_epochs < config.num_epochs):\n",
    "                model, optimizer, losses = mas_train(model, optimizer, task, total_epochs,\n",
    "                                                                 config.summary_epochs,\n",
    "                                                                 mas_tasks, config.lifelong_coeff)\n",
    "                task_loss[task.name] += losses\n",
    "\n",
    "                for subtask in range(task_id + 1):\n",
    "                    acc[tasks[subtask].name].append(val(model, tasks[subtask]))\n",
    "\n",
    "                total_epochs += config.summary_epochs\n",
    "                if total_epochs % config.store_epochs == 0 or total_epochs >= config.num_epochs:\n",
    "                    save_model(model, optimizer, config.store_model_path)\n",
    "\n",
    "        if config.mode == 'scp' and task_id > 0:\n",
    "            pass\n",
    "            ########################################\n",
    "            ##       TODO 区块 （ PART 2 ）         ##\n",
    "            ########################################\n",
    "            ##    PART 2  implementation 的部份    ##\n",
    "            ##   你也可以写别的 regularization 方法  ##\n",
    "            ##    助教这里有提供的是  scp    的 作法   ##\n",
    "            ##     Slicer Cramer Preservation     ##\n",
    "            ########################################\n",
    "            ########################################\n",
    "            ##       TODO 区块 （ PART 2 ）         ##\n",
    "            ########################################\n",
    "    return task_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class configurations(object):\n",
    "  def __init__(self):\n",
    "    self.batch_size = 256\n",
    "    self.num_epochs = 10000\n",
    "    self.store_epochs = 250\n",
    "    self.summary_epochs = 250\n",
    "    self.learning_rate = 0.0005\n",
    "    self.load_model = False\n",
    "    self.store_model_path = \"./model\"\n",
    "    self.load_model_path = \"./model\"\n",
    "    self.data_path = \"./data\"\n",
    "    self.mode = None\n",
    "    self.lifelong_coeff = 0.5\n",
    "\n",
    "###### 你也可以自己设定参数   ########\n",
    "###### 但上面的参数 是这次作业的预设直 #########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#主程式区块\n",
    "- 给 EWC, MAS 超参数 $\\lambda$ \n",
    "- 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ewc training\n",
      "download training data and load training data of mnist dataset ! \n",
      "load mnist dataset finished ! \n",
      "download training data and load training data of cifar10 dataset ! \n",
      "load cifar10 dataset finished ! \n",
      "Finish build model\n",
      "\n",
      "\n",
      "loss:46.33511734008789 of epoch:0 \n",
      "loss:33.42926025390625 of epoch:1 \n",
      "loss:32.75161361694336 of epoch:2 \n",
      "loss:27.345964431762695 of epoch:3 \n",
      "loss:18.874662399291992 of epoch:4 \n",
      "loss:12.846699714660645 of epoch:0 \n",
      "loss:12.845219612121582 of epoch:1 \n",
      "loss:10.548949241638184 of epoch:2 \n",
      "loss:8.525252342224121 of epoch:3 \n",
      "loss:6.8116655349731445 of epoch:4 \n",
      "\n",
      "\n",
      "Computing EWC\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[3072, 1024], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[1024], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., ..., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[1024, 512], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[512], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[512, 256], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[256], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[256, 128], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[128], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[128, 128], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[128], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[128, 10], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]])\n",
      "<class 'paddle.VarBase'>\n",
      "Tensor(shape=[10], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "loss:2.3116037845611572 of epoch:0 \n",
      "loss:2.304475784301758 of epoch:1 \n",
      "loss:2.297457456588745 of epoch:2 \n",
      "loss:2.2906739711761475 of epoch:3 \n",
      "loss:2.2876973152160645 of epoch:4 \n",
      "loss:2.2779436111450195 of epoch:0 \n",
      "loss:2.2705955505371094 of epoch:1 \n",
      "loss:2.249182939529419 of epoch:2 \n",
      "loss:2.2467591762542725 of epoch:3 \n",
      "loss:2.233130931854248 of epoch:4 \n",
      "mas training\n",
      "download training data and load training data of mnist dataset ! \n",
      "load mnist dataset finished ! \n",
      "download training data and load training data of cifar10 dataset ! \n",
      "load cifar10 dataset finished ! \n",
      "Finish build model\n",
      "\n",
      "\n",
      "loss:43.74868392944336 of epoch:0 \n",
      "loss:46.93212890625 of epoch:1 \n",
      "loss:46.7099723815918 of epoch:2 \n",
      "loss:39.844512939453125 of epoch:3 \n",
      "loss:27.4708194732666 of epoch:4 \n",
      "loss:19.834367752075195 of epoch:0 \n",
      "loss:18.584468841552734 of epoch:1 \n",
      "loss:14.387933731079102 of epoch:2 \n",
      "loss:10.485311508178711 of epoch:3 \n",
      "loss:7.7073073387146 of epoch:4 \n",
      "\n",
      "\n",
      "Computing MAS\n",
      "loss:2.318295478820801 of epoch:0 \n",
      "loss:2.3155226707458496 of epoch:1 \n",
      "loss:2.3129923343658447 of epoch:2 \n",
      "loss:2.3052303791046143 of epoch:3 \n",
      "loss:2.2961201667785645 of epoch:4 \n",
      "loss:2.2931642532348633 of epoch:0 \n",
      "loss:2.273019552230835 of epoch:1 \n",
      "loss:2.2643775939941406 of epoch:2 \n",
      "loss:2.270339250564575 of epoch:3 \n",
      "loss:2.24211049079895 of epoch:4 \n",
      "basic training\n",
      "download training data and load training data of mnist dataset ! \n",
      "load mnist dataset finished ! \n",
      "download training data and load training data of cifar10 dataset ! \n",
      "load cifar10 dataset finished ! \n",
      "Finish build model\n",
      "\n",
      "\n",
      "loss:50.06311798095703 of epoch:0 \n",
      "loss:39.723419189453125 of epoch:1 \n",
      "loss:35.58266067504883 of epoch:2 \n",
      "loss:24.19887351989746 of epoch:3 \n",
      "loss:23.171886444091797 of epoch:4 \n",
      "loss:10.090304374694824 of epoch:0 \n",
      "loss:11.420814514160156 of epoch:1 \n",
      "loss:8.748123168945312 of epoch:2 \n",
      "loss:9.404326438903809 of epoch:3 \n",
      "loss:5.019579887390137 of epoch:4 \n",
      "\n",
      "\n",
      "loss:2.309945821762085 of epoch:0 \n",
      "loss:2.304807662963867 of epoch:1 \n",
      "loss:2.311410665512085 of epoch:2 \n",
      "loss:2.29559588432312 of epoch:3 \n",
      "loss:2.293915033340454 of epoch:4 \n",
      "loss:2.2782130241394043 of epoch:0 \n",
      "loss:2.280367612838745 of epoch:1 \n",
      "loss:2.26499342918396 of epoch:2 \n",
      "loss:2.264601230621338 of epoch:3 \n",
      "loss:2.2559287548065186 of epoch:4 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the order is mnist -> cifar10\n",
    "==============================================\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    #mode_list = ['mas', 'ewc', 'basic']\n",
    "    mode_list = ['ewc', 'mas', 'basic']\n",
    "\n",
    "    ## hint: 谨慎的去选择 lambda 超参数 / ewc: 80~400, mas: 0.1 - 10\n",
    "    ############################################################################\n",
    "    #####                           TODO 区块 （ PART 1 ）                  #####\n",
    "    ############################################################################\n",
    "    coeff_list = [0, 0, 0]  ## 你需要在这 微调 lambda 参数, mas, ewc, baseline=0 ##\n",
    "    ############################################################################\n",
    "    #####                           TODO 区块 （ PART 1 ）                  #####\n",
    "    ############################################################################\n",
    "\n",
    "    config = configurations()\n",
    "    count = 0\n",
    "    for mode in mode_list:\n",
    "        config.mode = mode\n",
    "        config.lifelong_coeff = coeff_list[count]\n",
    "        print(\"{} training\".format(config.mode))\n",
    "        model, optimizer, tasks = build_model(config.data_path, config.batch_size, config.learning_rate)\n",
    "        print(\"Finish build model\")\n",
    "        if config.load_model:\n",
    "            model, optimizer = load_model(model, optimizer, config.load_model_path)\n",
    "        task_loss, acc = train_process(model, optimizer, tasks, config)\n",
    "        with open(f'./{config.mode}_acc.txt', 'w') as f:\n",
    "            json.dump(acc, f)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 画出 Result 图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_result(mode_list, task1, task2):\n",
    "    # draw the lines\n",
    "    count = 0\n",
    "    for reg_name in mode_list:\n",
    "        label = reg_name\n",
    "        with open(f'./{reg_name}_acc.txt', 'r') as f:\n",
    "            acc = json.load(f)\n",
    "        if count == 0:\n",
    "            color = 'red'\n",
    "        elif count == 1:\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'purple'\n",
    "        ax1 = plt.subplot(2, 1, 1)\n",
    "        plt.plot(range(len(acc[task1])), acc[task1], color, label=label)\n",
    "        ax1.set_ylabel(task1)\n",
    "        ax2 = plt.subplot(2, 1, 2, sharex=ax1, sharey=ax1)\n",
    "        plt.plot(range(len(acc[task2])), acc[task2], color, label=label)\n",
    "        ax2.set_ylabel(task2)\n",
    "        count += 1\n",
    "    plt.ylim((0.02, 1.02))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "mode_list = ['ewc', 'mas', 'basic']\n",
    "plot_result(mode_list, 'MNIST', 'Cifar10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在今年 ICLR 2020 的 paper，有以这两种方法做 baseline，并对这两种方法各自做了一个 geometry view，也提出新的方法，有兴趣的人可以参考\n",
    "\n",
    "paper link 如下 [SLICED CRAMER´ SYNAPTIC CONSOLIDATION FOR\n",
    "PRESERVING DEEPLY LEARNED REPRESENTATIONS](https://openreview.net/pdf?id=BJge3TNKwH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 进阶 \n",
    "请实做其他的 regularization 的方法，助教有提供的是 SCP 的作法，\n",
    "\n",
    "你也可以考虑实做出 SI, Rimennian Walk, IMM, 或是上面的方法, \n",
    "\n",
    "你可以参考助教上方的写法，写出雷同的 class 跟 training 来 train，\n",
    "\n",
    "记得画出与上方雷同的 evaluation 图表 (show result) example 需要比对的话 可以参考助教给的 slide。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_spherical(npoints, ndim=3):\n",
    "    vec = np.random.randn(ndim, npoints)\n",
    "    vec /= np.linalg.norm(vec, axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SCP(object):\n",
    "    \"\"\"\n",
    "    OPEN REVIEW VERSION:\n",
    "    https://openreview.net/forum?id=BJge3TNKwH\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Layer, dataloaders: list, L: int, device):\n",
    "        self.model = model \n",
    "        self.dataloaders = dataloaders\n",
    "        self.params = {n: p for n, p in self.model.named_parameters() if not p.stop_gradient}\n",
    "        self._means = {}\n",
    "        self.L= L\n",
    "        self.device = device\n",
    "        self._precision_matrices = self.calculate_importance()\n",
    "    \n",
    "        for n, p in self.params.items():\n",
    "            self._means[n] = p.clone().detach()\n",
    "    \n",
    "    def calculate_importance(self):\n",
    "        print('Computing SCP')\n",
    "\n",
    "        precision_matrices = {}\n",
    "        for n, p in self.params.items():  # 初始化 Fisher (F) 的矩阵（都补零）\n",
    "            t_val = p.clone().detach()\n",
    "            t_val.set_value(np.zeros(shape=t_val.numpy().shape, dtype=np.float32))\n",
    "            precision_matrices[n] = t_val.numpy()\n",
    "\n",
    "        self.model.eval()\n",
    "        dataloader_num = len(self.dataloaders)\n",
    "        num_data = sum([len(loader) for loader in self.dataloaders])\n",
    "        for dataloader in self.dataloaders:\n",
    "            for data in dataloader:\n",
    "                self.model.clear_gradients()\n",
    "                output = self.model(data[0])\n",
    "\n",
    "                ####################################################################################\n",
    "                #####                            TODO 区块 （ PART 2 ）                           #####\n",
    "                ####################################################################################\n",
    "                ##### 产生 SCP 的 Gamma(Γ) 矩阵（ 如同 MAS 的 Omega(Ω) 矩阵, EWC 的 Fisher(F) 矩阵 ）#####\n",
    "                ####################################################################################\n",
    "                #####        1.对所有资料的 Output vector 取 平均 得到 平均 vector φ(:,θ_A* )       #####\n",
    "                ####################################################################################\n",
    "\n",
    "                ####################################################################################\n",
    "                #####   2. 随机 从 单位球壳 取样 L 个 vector ξ #（ Hint: sample_spherical() ）      #####\n",
    "                ####################################################################################\n",
    "\n",
    "                ####################################################################################\n",
    "                #####   3.    每一个 vector ξ 和 vector φ( :,θ_A* )内积得到 scalar ρ               #####\n",
    "                #####           对 scalar ρ 取 backward ， 每个参数得到各自的 gradient ∇ρ           #####\n",
    "                #####       每个参数的 gradient ∇ρ 取平方 取 L 平均 得到 各个参数的 Γ scalar          #####  \n",
    "                #####              所有参数的  Γ scalar 组合而成其实就是 Γ 矩阵                      #####\n",
    "                ####(hint:记得每次backward之后要clear_gradients 去清gradient, 不然 gradient会累加)   ######   \n",
    "                ####################################################################################\n",
    "      \n",
    "                ####################################################################################      \n",
    "                #####                            TODO 区块 （ PART 2 ）                          #####\n",
    "                ####################################################################################\n",
    "\n",
    "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "        return precision_matrices\n",
    "\n",
    "    def penalty(self, model: nn.Layer):\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            _loss = self._precision_matrices[n] * (p - self._means[n]).numpy() ** 2\n",
    "            loss += _loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scp_train(model, optimizer, task, total_epochs, summary_epochs, scp_tasks, lambda_scp,alpha=0.65):\n",
    "  losses = []\n",
    "  loss = 0.0\n",
    "  ###############################\n",
    "  #####  TODO 区块 （PART 2） #####\n",
    "  ###############################\n",
    "  ##  参考 MAS. EWC train 的写法 ##                 \n",
    "  ###############################\n",
    "  #####  TODO 区块 （PART 2） #####\n",
    "  ###############################\n",
    "  return model, optimizer, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\": \n",
    "#   pass \n",
    "###############################\n",
    "#####  TODO 区块 （PART 2） #####\n",
    "###############################\n",
    "##     参考 main 区块一样       ##                 \n",
    "##     的 code 結合新方法       ##\n",
    "###############################\n",
    "#####  TODO 区块 （PART 2） #####\n",
    "###############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
