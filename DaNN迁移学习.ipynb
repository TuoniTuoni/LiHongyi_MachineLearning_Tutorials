{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目5-迁移学习\n",
    "\n",
    "## 友情提示\n",
    "同学们可以前往课程作业区先行动手尝试！！！\n",
    "\n",
    "## 项目描述\n",
    "本作业的任务是迁移学习中的领域对抗性训练(Domain Adversarial Training)。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/0aa95ffac5414622a0588283f09087b2fbacebfb5a8f4e449d4ed62d740ca868)\n",
    "\n",
    "> 也就是左下角的那一块。\n",
    "\n",
    "Domain Adaptation是让模型可以在训练时只需要 A dataset label，不需要 B dataset label 的情况下提高 B dataset 的准确率。 （A dataset & task 接近 B dataset & task）也就是给定真实图片 & 标签以及大量的手绘图片，请设计一种方法使得模型可以预测出手绘图片的标签是什么。\n",
    "\n",
    "## 数据集介绍\n",
    "这次的任务是源数据: 真实照片，目标数据: 手画涂鸦。\n",
    "\n",
    "我们必须让model看过真实照片以及标签，尝试去预测手画涂鸦的标签为何。\n",
    "\n",
    "资料位于'data/data58171/real_or_drawing.zip'\n",
    "\n",
    "* Training : 5000 张真实图片 + label, 32 x 32 RGB\n",
    "* Testing : 100000 张手绘图片，28 x 28 Gray Scale\n",
    "* Label: 总共需要预测 10 个 class。\n",
    "* 资料下载下来是以 0 ~ 9 作为label\n",
    "\n",
    "特别注意一点: **这次的源数据和目标数据的图片都是平衡的，你们可以使用这个资料做其他事情。 **\n",
    "\n",
    "## 项目要求\n",
    "\n",
    "* 禁止手动标记label或在网上寻找label\n",
    "* 禁止使用pre-trained model\n",
    "\n",
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!unzip -d work data/data58171/real_or_drawing.zip # 解压缩real_or_drawing数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境配置/安装\n",
    "\n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 场景和为什么要进行领域对抗性训练\n",
    "你现在有源数据 + 标签，其中源数据和目标数据可能有点关系，所以你想要在源数据上训练一个模型，并在目标数据上进行预测。\n",
    "\n",
    "但这样有什么样的问题? 相信大家学过异常检测就会知道，如果有数据是在源数据没有出现过的(或称Abnormal的)，那么模型大部分都会因为不熟悉这个数据而可能乱作一团。\n",
    "\n",
    "以下我们将模型拆成特征提取器(上半部)和分类器(下半部)来作例子:\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/30eab11bf0b749eaba7b2ca11eabea5e5fc5026d15c54fb99aa9d2db546ad11b)\n",
    "\n",
    "整个模型在学习源数据的时候，特征提取器因为看过很多次源数据，所以所抽取出来的特征可能就颇具意义，例如像图上的蓝色分布，已经将图片分成各个簇，所以这个时候分类器就可以依照这个簇去预测结果。\n",
    "\n",
    "但是在做目标数据的时候，特征提取器会没看过这样的数据，导致输出的目标特征可能不属于在源数据分布上，这样的特征给分类器预测结果显然就不会做得好。\n",
    "\n",
    "## 神经网络的领域对抗训练(DaNN)\n",
    "基于如此，是不是只要让源数据和目标数据经过特征提取都在同个分布上，就会做得好了呢? 这就是DaNN的主要核心。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/0e35af74164547c491955f30ebe90b418bfe2e917bb04156a7931d18aefebab5)\n",
    "\n",
    "我们追加一个领域分类器，在学习的过程中，让领域分类器去判断经过特征提取后的特征是源自于哪个领域，让特征提取器学习如何产生特征以**骗过**领域分类器。持久下来，通常特征提取器都会打赢领域分类器。 (因为领域分类器的输入来自于特征提取器，而且对特征提取器来说领域&分类的任务并没有冲突。)\n",
    "\n",
    "如此一来，我们就可以确信不管是哪一个领域，特征提取器都会把它产生在同一个特征分布上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据可视化\n",
    "\n",
    "以下的code分别为下载和观看这次的资料大概长什么样子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def no_axis_show(img, title='', cmap=None):\n",
    "  # imshow, 缩放模式为nearest。\n",
    "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
    "  # 不要显示axis。\n",
    "  fig.axes.get_xaxis().set_visible(False)\n",
    "  fig.axes.get_yaxis().set_visible(False)\n",
    "  plt.title(title)\n",
    "\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'work/real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'work/real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 特殊领域知识\n",
    "\n",
    "因为大家涂鸦的时候通常只会画轮廓，我们可以根据这点将source data做点边缘侦测处理，让source data更像target data一点。\n",
    "\n",
    "## Canny 边缘检测\n",
    "算法这边不赘述，只教大家怎么用。若有兴趣欢迎参考wiki或[这里](https://medium.com/@pomelyu5199/canny-edge-detector-%E5%AF%A6%E4%BD%9C-opencv-f7d1a0a57d19)。\n",
    "\n",
    "cv2.Canny使用非常方便，只需要两个参数: low_threshold, high_threshold。\n",
    "\n",
    "```cv2.Canny(image, low_threshold, high_threshold)```\n",
    "\n",
    "简单来说就是当边缘值超过high_threshold，我们就确定它是edge。如果只有超过low_threshold，那就先判断一下再决定是不是edge。\n",
    "\n",
    "以下我们直接拿源数据做做看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "original_img = plt.imread(f'work/real_or_drawing/train_data/0/0.bmp')\n",
    "plt.subplot(1, 5, 1)\n",
    "no_axis_show(original_img, title='original')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
    "\n",
    "canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
    "plt.subplot(1, 5, 3)\n",
    "no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
    "\n",
    "canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
    "plt.subplot(1, 5, 4)\n",
    "no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
    "\n",
    "canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
    "plt.subplot(1, 5, 5)\n",
    "no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据处理\n",
    "\n",
    "在这里我故意将数据做成可以使用paddle.io.datasets的形式，所以只要使用该函式便可以做出一个datasets。\n",
    "\n",
    "transform的部分请参考以下注解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "import paddle.optimizer as optim\n",
    "from paddle.io import DataLoader\n",
    "from paddle.vision.datasets import DatasetFolder\n",
    "from paddle.nn import Sequential, Conv2D, BatchNorm1D, BatchNorm2D, ReLU, MaxPool2D, Linear\n",
    "from paddle.vision.transforms import Compose, Grayscale, Transpose, RandomHorizontalFlip, RandomRotation, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Canny(paddle.vision.transforms.transforms.BaseTransform):\n",
    "    def __init__(self, low, high, keys=None):\n",
    "        super(Canny, self).__init__(keys)\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def _apply_image(self, img):\n",
    "        Canny = lambda img: cv2.Canny(np.array(img), self.low, self.high)\n",
    "        return Canny(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_transform = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(15),\n",
    "    Grayscale(),\n",
    "    Canny(low=170, high=300),\n",
    "    # Transpose(),\n",
    "    ToTensor()\n",
    "    ])\n",
    "target_transform = Compose([\n",
    "    Grayscale(),\n",
    "    Resize((32, 32)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(15, fill=(0,)),\n",
    "    ToTensor()\n",
    "    ])\n",
    "\n",
    "source_dataset = DatasetFolder('work/real_or_drawing/train_data', transform=source_transform)\n",
    "target_dataset = DatasetFolder('work/real_or_drawing/test_data', transform=target_transform)\n",
    "\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=32, shuffle=True)\n",
    "target_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(target_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型\n",
    "\n",
    "特征提取器: 典型的VGG-like叠法。\n",
    "\n",
    "标签预测 / 领域分类: MLP到尾。\n",
    "\n",
    "相信作业写到这边大家对以下的层都很熟悉，因此不再赘述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv = Sequential(\n",
    "            Conv2D(1, 64, 3, 1, 1),\n",
    "            BatchNorm2D(64),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2),\n",
    "\n",
    "            Conv2D(64, 128, 3, 1, 1),\n",
    "            BatchNorm2D(128),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2),\n",
    "\n",
    "            Conv2D(128, 256, 3, 1, 1),\n",
    "            BatchNorm2D(256),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2),\n",
    "\n",
    "            Conv2D(256, 256, 3, 1, 1),\n",
    "            BatchNorm2D(256),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2),\n",
    "\n",
    "            Conv2D(256, 512, 3, 1, 1),\n",
    "            BatchNorm2D(512),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x).squeeze()\n",
    "        return x\n",
    "\n",
    "class LabelPredictor(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabelPredictor, self).__init__()\n",
    "\n",
    "        self.layer = Sequential(\n",
    "            Linear(512, 512),\n",
    "            ReLU(),\n",
    "\n",
    "            Linear(512, 512),\n",
    "            ReLU(),\n",
    "\n",
    "            Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c\n",
    "\n",
    "class DomainClassifier(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "\n",
    "        self.layer = Sequential(\n",
    "            Linear(512, 512),\n",
    "            BatchNorm1D(512),\n",
    "            ReLU(),\n",
    "\n",
    "            Linear(512, 512),\n",
    "            BatchNorm1D(512),\n",
    "            ReLU(),\n",
    "\n",
    "            Linear(512, 512),\n",
    "            BatchNorm1D(512),\n",
    "            ReLU(),\n",
    "\n",
    "            Linear(512, 512),\n",
    "            BatchNorm1D(512),\n",
    "            ReLU(),\n",
    "\n",
    "            Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        y = self.layer(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 预处理\n",
    "\n",
    "这里我们选用Adam来当优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor()\n",
    "label_predictor = LabelPredictor()\n",
    "domain_classifier = DomainClassifier()\n",
    "\n",
    "class_criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "domain_criterion = paddle.nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(parameters=feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(parameters=label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(parameters=domain_classifier.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 开始训练\n",
    "\n",
    "## 如何实作DaNN?\n",
    "\n",
    "理论上，在原始论文中是加上梯度反转层，并将特征提取器 / 标签预测器 / 领域分类器 一起进行训练，但其实我们也可以交换的训练领域分类器 & 特征提取器(就像在训练对抗生成网络的生成器 & 辨别器一样)，这也是可行的。\n",
    "\n",
    "在code实现中，我们采取后者的方式，毕竟大家上个作业就是GAN，应该会比较熟悉:)。\n",
    "\n",
    "## 小提醒\n",
    "* 原文中的lambda(控制域名对抗性损失的系数)是有适应性的版本，如果有兴趣可以参考[原文](https://arxiv.org/pdf/1505.07818.pdf)。以下为了方便固定设置0.1。\n",
    "* 因为我们完全没有目标的标签，所以结果如何，只好去kaggle看看啰:)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:637: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "epoch   0: train D loss: 0.2486, train F loss: 2.0155, acc 0.2744\n",
      "epoch   1: train D loss: 0.5980, train F loss: 1.5812, acc 0.4206\n",
      "epoch   2: train D loss: 0.4825, train F loss: 1.4725, acc 0.4686\n",
      "epoch   3: train D loss: 0.5002, train F loss: 1.3849, acc 0.4962\n",
      "epoch   4: train D loss: 0.5209, train F loss: 1.3099, acc 0.5218\n",
      "epoch   5: train D loss: 0.5455, train F loss: 1.2892, acc 0.5250\n",
      "epoch   6: train D loss: 0.5452, train F loss: 1.2329, acc 0.5598\n",
      "epoch   7: train D loss: 0.5228, train F loss: 1.1991, acc 0.5624\n",
      "epoch   8: train D loss: 0.5520, train F loss: 1.1609, acc 0.5766\n",
      "epoch   9: train D loss: 0.5303, train F loss: 1.1286, acc 0.5898\n",
      "epoch  10: train D loss: 0.5484, train F loss: 1.1247, acc 0.5904\n",
      "epoch  11: train D loss: 0.5465, train F loss: 1.1060, acc 0.5942\n",
      "epoch  12: train D loss: 0.5174, train F loss: 1.0516, acc 0.6154\n",
      "epoch  13: train D loss: 0.5401, train F loss: 1.0447, acc 0.6216\n",
      "epoch  14: train D loss: 0.5224, train F loss: 1.0034, acc 0.6364\n",
      "epoch  15: train D loss: 0.5168, train F loss: 0.9810, acc 0.6416\n",
      "epoch  16: train D loss: 0.5229, train F loss: 0.9530, acc 0.6520\n",
      "epoch  17: train D loss: 0.4988, train F loss: 0.9192, acc 0.6704\n",
      "epoch  18: train D loss: 0.5285, train F loss: 0.9203, acc 0.6658\n",
      "epoch  19: train D loss: 0.5172, train F loss: 0.9122, acc 0.6680\n",
      "epoch  20: train D loss: 0.5125, train F loss: 0.8499, acc 0.6904\n",
      "epoch  21: train D loss: 0.4809, train F loss: 0.8384, acc 0.6948\n",
      "epoch  22: train D loss: 0.4858, train F loss: 0.8254, acc 0.7018\n",
      "epoch  23: train D loss: 0.5064, train F loss: 0.7983, acc 0.7150\n",
      "epoch  24: train D loss: 0.4922, train F loss: 0.7741, acc 0.7162\n",
      "epoch  25: train D loss: 0.4855, train F loss: 0.7540, acc 0.7208\n",
      "epoch  26: train D loss: 0.5027, train F loss: 0.7352, acc 0.7276\n",
      "epoch  27: train D loss: 0.4965, train F loss: 0.7241, acc 0.7322\n",
      "epoch  28: train D loss: 0.4930, train F loss: 0.7054, acc 0.7402\n",
      "epoch  29: train D loss: 0.4858, train F loss: 0.6660, acc 0.7522\n",
      "epoch  30: train D loss: 0.4727, train F loss: 0.6560, acc 0.7640\n",
      "epoch  31: train D loss: 0.4894, train F loss: 0.6284, acc 0.7670\n",
      "epoch  32: train D loss: 0.4708, train F loss: 0.6271, acc 0.7682\n",
      "epoch  33: train D loss: 0.4742, train F loss: 0.5811, acc 0.7826\n",
      "epoch  34: train D loss: 0.4620, train F loss: 0.6055, acc 0.7768\n",
      "epoch  35: train D loss: 0.4538, train F loss: 0.5525, acc 0.7972\n",
      "epoch  36: train D loss: 0.4672, train F loss: 0.5424, acc 0.7970\n",
      "epoch  37: train D loss: 0.4603, train F loss: 0.5420, acc 0.7982\n",
      "epoch  38: train D loss: 0.4493, train F loss: 0.5132, acc 0.8204\n",
      "epoch  39: train D loss: 0.4583, train F loss: 0.4703, acc 0.8254\n",
      "epoch  40: train D loss: 0.4446, train F loss: 0.4503, acc 0.8330\n",
      "epoch  41: train D loss: 0.4435, train F loss: 0.4550, acc 0.8298\n",
      "epoch  42: train D loss: 0.4456, train F loss: 0.4202, acc 0.8388\n",
      "epoch  43: train D loss: 0.4562, train F loss: 0.4485, acc 0.8334\n",
      "epoch  44: train D loss: 0.4558, train F loss: 0.4010, acc 0.8510\n",
      "epoch  45: train D loss: 0.4330, train F loss: 0.4188, acc 0.8398\n",
      "epoch  46: train D loss: 0.4355, train F loss: 0.4105, acc 0.8546\n",
      "epoch  47: train D loss: 0.4203, train F loss: 0.3844, acc 0.8556\n",
      "epoch  48: train D loss: 0.4287, train F loss: 0.3712, acc 0.8628\n",
      "epoch  49: train D loss: 0.4352, train F loss: 0.3652, acc 0.8630\n",
      "epoch  50: train D loss: 0.4307, train F loss: 0.3482, acc 0.8678\n",
      "epoch  51: train D loss: 0.4212, train F loss: 0.3329, acc 0.8738\n",
      "epoch  52: train D loss: 0.4204, train F loss: 0.3148, acc 0.8766\n",
      "epoch  53: train D loss: 0.4313, train F loss: 0.3166, acc 0.8842\n",
      "epoch  54: train D loss: 0.4165, train F loss: 0.2984, acc 0.8844\n",
      "epoch  55: train D loss: 0.4274, train F loss: 0.3140, acc 0.8762\n",
      "epoch  56: train D loss: 0.4129, train F loss: 0.2994, acc 0.8880\n",
      "epoch  57: train D loss: 0.4352, train F loss: 0.2861, acc 0.8922\n",
      "epoch  58: train D loss: 0.4206, train F loss: 0.2803, acc 0.8898\n",
      "epoch  59: train D loss: 0.4197, train F loss: 0.2695, acc 0.8966\n",
      "epoch  60: train D loss: 0.4219, train F loss: 0.2818, acc 0.8906\n",
      "epoch  61: train D loss: 0.4278, train F loss: 0.2632, acc 0.8914\n",
      "epoch  62: train D loss: 0.4158, train F loss: 0.2923, acc 0.8892\n",
      "epoch  63: train D loss: 0.4151, train F loss: 0.2578, acc 0.9048\n",
      "epoch  64: train D loss: 0.4274, train F loss: 0.2367, acc 0.9046\n",
      "epoch  65: train D loss: 0.4175, train F loss: 0.2426, acc 0.9044\n",
      "epoch  66: train D loss: 0.4126, train F loss: 0.2307, acc 0.9118\n",
      "epoch  67: train D loss: 0.4214, train F loss: 0.2264, acc 0.9090\n",
      "epoch  68: train D loss: 0.4031, train F loss: 0.2160, acc 0.9130\n",
      "epoch  69: train D loss: 0.4119, train F loss: 0.2104, acc 0.9158\n",
      "epoch  70: train D loss: 0.4185, train F loss: 0.2290, acc 0.9118\n",
      "epoch  71: train D loss: 0.4095, train F loss: 0.2009, acc 0.9238\n",
      "epoch  72: train D loss: 0.4059, train F loss: 0.1977, acc 0.9202\n",
      "epoch  73: train D loss: 0.4295, train F loss: 0.2296, acc 0.9100\n",
      "epoch  74: train D loss: 0.4022, train F loss: 0.1897, acc 0.9238\n",
      "epoch  75: train D loss: 0.4135, train F loss: 0.2014, acc 0.9230\n",
      "epoch  76: train D loss: 0.3955, train F loss: 0.1926, acc 0.9246\n",
      "epoch  77: train D loss: 0.4125, train F loss: 0.1788, acc 0.9282\n",
      "epoch  78: train D loss: 0.4113, train F loss: 0.2081, acc 0.9172\n",
      "epoch  79: train D loss: 0.4081, train F loss: 0.1741, acc 0.9316\n",
      "epoch  80: train D loss: 0.4166, train F loss: 0.1889, acc 0.9228\n",
      "epoch  81: train D loss: 0.4166, train F loss: 0.1616, acc 0.9372\n",
      "epoch  82: train D loss: 0.4050, train F loss: 0.1806, acc 0.9298\n",
      "epoch  83: train D loss: 0.4125, train F loss: 0.1470, acc 0.9402\n",
      "epoch  84: train D loss: 0.4097, train F loss: 0.1547, acc 0.9380\n",
      "epoch  85: train D loss: 0.4104, train F loss: 0.1766, acc 0.9280\n",
      "epoch  86: train D loss: 0.3957, train F loss: 0.1641, acc 0.9290\n",
      "epoch  87: train D loss: 0.3978, train F loss: 0.1503, acc 0.9398\n",
      "epoch  88: train D loss: 0.4022, train F loss: 0.1462, acc 0.9356\n",
      "epoch  89: train D loss: 0.3874, train F loss: 0.1659, acc 0.9338\n",
      "epoch  90: train D loss: 0.3909, train F loss: 0.1419, acc 0.9416\n",
      "epoch  91: train D loss: 0.4089, train F loss: 0.1539, acc 0.9350\n",
      "epoch  92: train D loss: 0.4083, train F loss: 0.1733, acc 0.9282\n",
      "epoch  93: train D loss: 0.4007, train F loss: 0.1316, acc 0.9448\n",
      "epoch  94: train D loss: 0.3977, train F loss: 0.1337, acc 0.9432\n",
      "epoch  95: train D loss: 0.3880, train F loss: 0.1492, acc 0.9378\n",
      "epoch  96: train D loss: 0.3781, train F loss: 0.1367, acc 0.9422\n",
      "epoch  97: train D loss: 0.4083, train F loss: 0.1531, acc 0.9364\n",
      "epoch  98: train D loss: 0.3897, train F loss: 0.1506, acc 0.9364\n",
      "epoch  99: train D loss: 0.3810, train F loss: 0.1376, acc 0.9406\n",
      "epoch 100: train D loss: 0.3891, train F loss: 0.1483, acc 0.9448\n",
      "epoch 101: train D loss: 0.3812, train F loss: 0.1426, acc 0.9408\n",
      "epoch 102: train D loss: 0.3771, train F loss: 0.1348, acc 0.9482\n",
      "epoch 103: train D loss: 0.3887, train F loss: 0.1128, acc 0.9494\n",
      "epoch 104: train D loss: 0.4025, train F loss: 0.1370, acc 0.9436\n",
      "epoch 105: train D loss: 0.3715, train F loss: 0.1360, acc 0.9474\n",
      "epoch 106: train D loss: 0.3999, train F loss: 0.1347, acc 0.9434\n",
      "epoch 107: train D loss: 0.3897, train F loss: 0.1304, acc 0.9436\n",
      "epoch 108: train D loss: 0.3954, train F loss: 0.1401, acc 0.9408\n",
      "epoch 109: train D loss: 0.3987, train F loss: 0.1202, acc 0.9490\n",
      "epoch 110: train D loss: 0.3835, train F loss: 0.1308, acc 0.9458\n",
      "epoch 111: train D loss: 0.3886, train F loss: 0.1186, acc 0.9486\n",
      "epoch 112: train D loss: 0.4038, train F loss: 0.0960, acc 0.9558\n",
      "epoch 113: train D loss: 0.3959, train F loss: 0.1143, acc 0.9508\n",
      "epoch 114: train D loss: 0.3928, train F loss: 0.1074, acc 0.9512\n",
      "epoch 115: train D loss: 0.3977, train F loss: 0.1075, acc 0.9504\n",
      "epoch 116: train D loss: 0.3910, train F loss: 0.1105, acc 0.9534\n",
      "epoch 117: train D loss: 0.4026, train F loss: 0.0895, acc 0.9554\n",
      "epoch 118: train D loss: 0.4005, train F loss: 0.1079, acc 0.9528\n",
      "epoch 119: train D loss: 0.3940, train F loss: 0.0942, acc 0.9546\n",
      "epoch 120: train D loss: 0.4058, train F loss: 0.1108, acc 0.9488\n",
      "epoch 121: train D loss: 0.3878, train F loss: 0.1211, acc 0.9484\n",
      "epoch 122: train D loss: 0.3937, train F loss: 0.0911, acc 0.9574\n",
      "epoch 123: train D loss: 0.3930, train F loss: 0.1076, acc 0.9532\n",
      "epoch 124: train D loss: 0.4032, train F loss: 0.0976, acc 0.9542\n",
      "epoch 125: train D loss: 0.4005, train F loss: 0.1107, acc 0.9522\n",
      "epoch 126: train D loss: 0.4121, train F loss: 0.1057, acc 0.9528\n",
      "epoch 127: train D loss: 0.3994, train F loss: 0.1071, acc 0.9514\n",
      "epoch 128: train D loss: 0.3793, train F loss: 0.1036, acc 0.9528\n",
      "epoch 129: train D loss: 0.3949, train F loss: 0.1029, acc 0.9492\n",
      "epoch 130: train D loss: 0.3937, train F loss: 0.1021, acc 0.9534\n",
      "epoch 131: train D loss: 0.3934, train F loss: 0.1122, acc 0.9528\n",
      "epoch 132: train D loss: 0.3892, train F loss: 0.0934, acc 0.9578\n",
      "epoch 133: train D loss: 0.3818, train F loss: 0.0815, acc 0.9620\n",
      "epoch 134: train D loss: 0.3880, train F loss: 0.0971, acc 0.9566\n",
      "epoch 135: train D loss: 0.3951, train F loss: 0.0938, acc 0.9586\n",
      "epoch 136: train D loss: 0.3995, train F loss: 0.0889, acc 0.9576\n",
      "epoch 137: train D loss: 0.3977, train F loss: 0.0879, acc 0.9576\n",
      "epoch 138: train D loss: 0.4001, train F loss: 0.0805, acc 0.9588\n",
      "epoch 139: train D loss: 0.4057, train F loss: 0.0937, acc 0.9578\n",
      "epoch 140: train D loss: 0.3955, train F loss: 0.0722, acc 0.9658\n",
      "epoch 141: train D loss: 0.3895, train F loss: 0.0801, acc 0.9602\n",
      "epoch 142: train D loss: 0.3922, train F loss: 0.0797, acc 0.9610\n",
      "epoch 143: train D loss: 0.3766, train F loss: 0.0918, acc 0.9616\n",
      "epoch 144: train D loss: 0.3926, train F loss: 0.0920, acc 0.9574\n",
      "epoch 145: train D loss: 0.3988, train F loss: 0.0787, acc 0.9602\n",
      "epoch 146: train D loss: 0.3918, train F loss: 0.0825, acc 0.9636\n",
      "epoch 147: train D loss: 0.3953, train F loss: 0.0752, acc 0.9660\n",
      "epoch 148: train D loss: 0.3856, train F loss: 0.0832, acc 0.9604\n",
      "epoch 149: train D loss: 0.3919, train F loss: 0.0743, acc 0.9598\n",
      "epoch 150: train D loss: 0.3909, train F loss: 0.0885, acc 0.9600\n",
      "epoch 151: train D loss: 0.4158, train F loss: 0.0870, acc 0.9548\n",
      "epoch 152: train D loss: 0.4065, train F loss: 0.0813, acc 0.9584\n",
      "epoch 153: train D loss: 0.3905, train F loss: 0.0807, acc 0.9606\n",
      "epoch 154: train D loss: 0.3897, train F loss: 0.0806, acc 0.9612\n",
      "epoch 155: train D loss: 0.3985, train F loss: 0.0680, acc 0.9664\n",
      "epoch 156: train D loss: 0.3953, train F loss: 0.0754, acc 0.9644\n",
      "epoch 157: train D loss: 0.3959, train F loss: 0.0714, acc 0.9626\n",
      "epoch 158: train D loss: 0.3973, train F loss: 0.0728, acc 0.9624\n",
      "epoch 159: train D loss: 0.3840, train F loss: 0.0826, acc 0.9618\n",
      "epoch 160: train D loss: 0.3862, train F loss: 0.0774, acc 0.9602\n",
      "epoch 161: train D loss: 0.3834, train F loss: 0.0691, acc 0.9634\n",
      "epoch 162: train D loss: 0.3886, train F loss: 0.0558, acc 0.9678\n",
      "epoch 163: train D loss: 0.4020, train F loss: 0.0631, acc 0.9690\n",
      "epoch 164: train D loss: 0.3952, train F loss: 0.0803, acc 0.9618\n",
      "epoch 165: train D loss: 0.3887, train F loss: 0.0843, acc 0.9598\n",
      "epoch 166: train D loss: 0.4011, train F loss: 0.0687, acc 0.9632\n",
      "epoch 167: train D loss: 0.4070, train F loss: 0.0627, acc 0.9654\n",
      "epoch 168: train D loss: 0.3913, train F loss: 0.0681, acc 0.9636\n",
      "epoch 169: train D loss: 0.3916, train F loss: 0.0732, acc 0.9656\n",
      "epoch 170: train D loss: 0.3927, train F loss: 0.0738, acc 0.9620\n",
      "epoch 171: train D loss: 0.3931, train F loss: 0.0617, acc 0.9690\n",
      "epoch 172: train D loss: 0.4015, train F loss: 0.0871, acc 0.9608\n",
      "epoch 173: train D loss: 0.3818, train F loss: 0.0668, acc 0.9672\n",
      "epoch 174: train D loss: 0.3949, train F loss: 0.0699, acc 0.9650\n",
      "epoch 175: train D loss: 0.4046, train F loss: 0.0659, acc 0.9640\n",
      "epoch 176: train D loss: 0.3875, train F loss: 0.0701, acc 0.9676\n",
      "epoch 177: train D loss: 0.3891, train F loss: 0.0665, acc 0.9678\n",
      "epoch 178: train D loss: 0.3881, train F loss: 0.0532, acc 0.9714\n",
      "epoch 179: train D loss: 0.3892, train F loss: 0.0600, acc 0.9706\n",
      "epoch 180: train D loss: 0.3877, train F loss: 0.0685, acc 0.9674\n",
      "epoch 181: train D loss: 0.3918, train F loss: 0.0680, acc 0.9646\n",
      "epoch 182: train D loss: 0.3973, train F loss: 0.0620, acc 0.9670\n",
      "epoch 183: train D loss: 0.3986, train F loss: 0.0655, acc 0.9654\n",
      "epoch 184: train D loss: 0.4037, train F loss: 0.0680, acc 0.9640\n",
      "epoch 185: train D loss: 0.3919, train F loss: 0.0715, acc 0.9658\n",
      "epoch 186: train D loss: 0.3875, train F loss: 0.0559, acc 0.9690\n",
      "epoch 187: train D loss: 0.3932, train F loss: 0.0624, acc 0.9688\n",
      "epoch 188: train D loss: 0.3872, train F loss: 0.0636, acc 0.9672\n",
      "epoch 189: train D loss: 0.3914, train F loss: 0.0732, acc 0.9656\n",
      "epoch 190: train D loss: 0.3900, train F loss: 0.0546, acc 0.9704\n",
      "epoch 191: train D loss: 0.3888, train F loss: 0.0595, acc 0.9698\n",
      "epoch 192: train D loss: 0.3864, train F loss: 0.0576, acc 0.9694\n",
      "epoch 193: train D loss: 0.4025, train F loss: 0.0570, acc 0.9702\n",
      "epoch 194: train D loss: 0.3816, train F loss: 0.0578, acc 0.9694\n",
      "epoch 195: train D loss: 0.4013, train F loss: 0.0486, acc 0.9694\n",
      "epoch 196: train D loss: 0.3980, train F loss: 0.0422, acc 0.9738\n",
      "epoch 197: train D loss: 0.3840, train F loss: 0.0460, acc 0.9732\n",
      "epoch 198: train D loss: 0.4148, train F loss: 0.0671, acc 0.9646\n",
      "epoch 199: train D loss: 0.3966, train F loss: 0.0506, acc 0.9710\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
    "    '''\n",
    "      Args:\n",
    "        source_dataloader: source data的dataloader\n",
    "        target_dataloader: target data的dataloader\n",
    "        lamb: 调控adversarial的loss系数。\n",
    "    '''\n",
    "\n",
    "    # D loss: Domain Classifier的loss\n",
    "    # F loss: Feature Extrator & Label Predictor的loss\n",
    "    # total_hit: 计算目前对了几笔 total_num: 目前经过了几笔\n",
    "    running_D_loss, running_F_loss = 0.0, 0.0\n",
    "    total_hit, total_num = 0.0, 0.0\n",
    "\n",
    "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "\n",
    "        # source_data = source_data.cuda()\n",
    "        # source_label = source_label.cuda()\n",
    "        # target_data = target_data.cuda()\n",
    "        \n",
    "        # 我们把source data和target data混在一起，否则batch_norm可能会算错 (两边的data的mean/var不太一样)\n",
    "        mixed_data = paddle.concat([source_data, target_data], axis=0)\n",
    "        domain_label = paddle.zeros([source_data.shape[0] + target_data.shape[0], 1])\n",
    "        # 设定source data的label为1\n",
    "        domain_label[:source_data.shape[0]] = 1\n",
    "\n",
    "        # Step 1 : 训练Domain Classifier\n",
    "        feature = feature_extractor(mixed_data)\n",
    "        # 因为我们在Step 1不需要训练Feature Extractor，所以把feature detach避免loss backprop上去。\n",
    "        domain_logits = domain_classifier(feature.detach())\n",
    "        # print('domain_logits.shape:', domain_logits.shape, 'domain_label.shape:', domain_label.shape)\n",
    "        loss = domain_criterion(domain_logits, domain_label)\n",
    "        running_D_loss+= loss.numpy()[0]\n",
    "        # print('loss:', loss)\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Step 2 : 训练Feature Extractor和Domain Classifier\n",
    "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
    "        domain_logits = domain_classifier(feature)\n",
    "        # loss为原本的class CE - lamb * domain BCE，相减的原因同GAN中的Discriminator中的G loss。\n",
    "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
    "        running_F_loss+= loss.numpy()[0]\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        optimizer_D.clear_grad()\n",
    "        optimizer_F.clear_grad()\n",
    "        optimizer_C.clear_grad()\n",
    "        # print('class_logits.shape:', class_logits.shape, 'source_label.shape:', source_label.shape)\n",
    "        # print('class_logits[0]:', class_logits[0], 'source_label[0]:', source_label[0])\n",
    "        total_hit += np.sum((paddle.argmax(class_logits, axis=1) == source_label).numpy())\n",
    "        total_num += source_data.shape[0]\n",
    "        print(i, end='\\r')\n",
    "\n",
    "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n",
    "\n",
    "# 训练200 epochs\n",
    "for epoch in range(200):\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=0.1)\n",
    "\n",
    "    paddle.save(feature_extractor.state_dict(), f'extractor_model.pdparams')\n",
    "    paddle.save(label_predictor.state_dict(), f'predictor_model.pdparams')\n",
    "\n",
    "    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 推断\n",
    "\n",
    "就跟前几次作业一样。这里我使用pandas来生产csv。\n",
    "\n",
    "此外，200 epochs的准确度可能会不太稳定，可以多丢几次或训练久一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "label_predictor.eval()\n",
    "feature_extractor.eval()\n",
    "for i, (test_data, _) in enumerate(test_dataloader):\n",
    "    test_data = test_data\n",
    "\n",
    "    class_logits = label_predictor(feature_extractor(test_data))\n",
    "\n",
    "    x = paddle.argmax(class_logits, axis=1).detach().numpy()\n",
    "    result.append(x)\n",
    "\n",
    "import pandas as pd\n",
    "result = np.concatenate(result)\n",
    "\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('work/DaNN_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
