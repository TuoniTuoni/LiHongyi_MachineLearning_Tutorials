{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军记者傅亚雨沈阳报道 来到沈阳，国奥队依然没有摆脱雨水的困扰。7月31日下午6点，国奥队的日常训练再度受到大雨的干扰，无奈之下队员们只慢跑了25分钟就草草收场。31日上午10点，国奥队在奥体中心外场训练的时候，天就是阴沉沉的，气象预报显示当天下午沈阳就有大雨，但幸好队伍上午的训练并没有受到任何干扰。下午6点，当球队抵达训练场时，大雨已经下了几个小时，而且丝毫没有停下来的意思。抱着试一试的态度，球队开始了当天下午的例行训练，25分钟过去了，天气没有任何转好的迹象，为了保护球员们，国奥队决定中止当天的训练，全队立即返回酒店。在雨中训练对足球队来说并不是什么稀罕事，但在奥运会即将开始之前，全队变得“娇贵”了。在沈阳最后一周的训练，国奥队首先要保证现有的球员不再出现意外的伤病情况以免影响正式比赛，因此这一阶段控制训练受伤、控制感冒等疾病的出现被队伍放在了相当重要的位置。而抵达沈阳之后，中后卫冯萧霆就一直没有训练，冯萧霆是7月27日在长春患上了感冒，因此也没有参加29日跟塞尔维亚的热身赛。队伍介绍说，冯萧霆并没有出现发烧症状，但为了安全起见，这两天还是让他静养休息，等感冒彻底好了之后再恢复训练。由于有了冯萧霆这个例子，因此国奥队对雨中训练就显得特别谨慎，主要是担心球员们受凉而引发感冒，造成非战斗减员。而女足队员马晓旭在热身赛中受伤导致无缘奥运的前科，也让在沈阳的国奥队现在格外警惕，“训练中不断嘱咐队员们要注意动作，我们可不能再出这样的事情了。”一位工作人员表示。从长春到沈阳，雨水一路伴随着国奥队，“也邪了，我们走到哪儿雨就下到哪儿，在长春几次训练都被大雨给搅和了，没想到来沈阳又碰到这种事情。”一位国奥球员也对雨水的“青睐”有些不解。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.884 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "马晓旭/意外/受伤/让/国奥/警惕/ /无奈/大雨/格外/青睐/殷家/军/记者/傅亚雨/沈阳/报道/ /来到/沈阳/，/国奥队/依然/没有/摆脱/雨水/的/困扰/。/7/月/31/日/下午/6/点/，/国奥队/的/日常/训练/再度/受到/大雨/的/干扰/，/无奈/之下/队员/们/只/慢跑/了/25/分钟/就/草草收场/。/31/日/上午/10/点/，/国奥队/在/奥体中心/外场/训练/的/时候/，/天/就是/阴沉沉/的/，/气象预报/显示/当天/下午/沈阳/就/有/大雨/，/但/幸好/队伍/上午/的/训练/并/没有/受到/任何/干扰/。/下午/6/点/，/当/球队/抵达/训练场/时/，/大雨/已经/下/了/几个/小时/，/而且/丝毫/没有/停下来/的/意思/。/抱/着/试一试/的/态度/，/球队/开始/了/当天/下午/的/例行/训练/，/25/分钟/过去/了/，/天气/没有/任何/转好/的/迹象/，/为了/保护/球员/们/，/国奥队/决定/中止/当天/的/训练/，/全队/立即/返回/酒店/。/在/雨/中/训练/对/足球队/来说/并/不是/什么/稀罕/事/，/但/在/奥运会/即将/开始/之前/，/全队/变得/“/娇贵/”/了/。/在/沈阳/最后/一周/的/训练/，/国奥队/首先/要/保证/现有/的/球员/不再/出现意外/的/伤病/情况/以免/影响/正式/比赛/，/因此/这一/阶段/控制/训练/受伤/、/控制/感冒/等/疾病/的/出现/被/队伍/放在/了/相当/重要/的/位置/。/而/抵达/沈阳/之后/，/中/后卫/冯萧霆/就/一直/没有/训练/，/冯萧霆/是/7/月/27/日/在/长春/患上/了/感冒/，/因此/也/没有/参加/29/日/跟/塞尔维亚/的/热身赛/。/队伍/介绍/说/，/冯萧霆/并/没有/出现/发烧/症状/，/但/为了/安全/起/见/，/这/两天/还是/让/他/静养/休息/，/等/感冒/彻底/好/了/之后/再/恢复/训练/。/由于/有/了/冯萧霆/这个/例子/，/因此/国奥队/对雨中/训练/就/显得/特别/谨慎/，/主要/是/担心/球员/们/受凉/而/引发/感冒/，/造成/非战斗/减员/。/而/女足/队员/马晓旭/在/热身赛/中/受伤/导致/无缘/奥运/的/前科/，/也/让/在/沈阳/的/国奥队/现在/格外/警惕/，/“/训练/中/不断/嘱咐/队员/们/要/注意/动作/，/我们/可/不能/再出/这样/的/事情/了/。/”/一位/工作人员/表示/。/从/长春/到/沈阳/，/雨水/一路/伴随/着/国奥队/，/“/也/邪/了/，/我们/走/到/哪儿/雨/就/下/到/哪儿/，/在/长春/几次/训练/都/被/大雨/给/搅和/了/，/没想到/来/沈阳/又/碰到/这种/事情/。/”/一位/国奥/球员/也/对/雨水/的/“/青睐/”/有些/不解/。\n",
      "line number: 0\n",
      "line number: 1000\n",
      "line number: 2000\n",
      "line number: 3000\n",
      "line number: 4000\n",
      "line number: 5000\n",
      "line number: 6000\n",
      "line number: 7000\n",
      "line number: 8000\n",
      "line number: 9000\n",
      "line number: 10000\n",
      "line number: 11000\n",
      "line number: 12000\n",
      "line number: 13000\n",
      "line number: 14000\n",
      "line number: 15000\n",
      "line number: 16000\n",
      "line number: 17000\n",
      "line number: 18000\n",
      "line number: 19000\n",
      "line number: 20000\n",
      "line number: 21000\n",
      "line number: 22000\n",
      "line number: 23000\n",
      "line number: 24000\n",
      "line number: 25000\n",
      "line number: 26000\n",
      "line number: 27000\n",
      "line number: 28000\n",
      "line number: 29000\n",
      "line number: 30000\n",
      "line number: 31000\n",
      "line number: 32000\n",
      "line number: 33000\n",
      "line number: 34000\n",
      "line number: 35000\n",
      "line number: 36000\n",
      "line number: 37000\n",
      "line number: 38000\n",
      "line number: 39000\n",
      "line number: 40000\n",
      "line number: 41000\n",
      "line number: 42000\n",
      "line number: 43000\n",
      "line number: 44000\n",
      "line number: 45000\n",
      "line number: 46000\n",
      "line number: 47000\n",
      "line number: 48000\n",
      "line number: 49000\n",
      "line number: 50000\n",
      "first training data: label 体育 segment 马晓旭/意外/受伤/让/国奥/警惕/ /无奈/大雨/格外/青睐/殷家/军/记者/傅亚雨/沈阳/报道/ /来到/沈阳/，/国奥队/依然/没有/摆脱/雨水/的/困扰/。/7/月/31/日/下午/6/点/，/国奥队/的/日常/训练/再度/受到/大雨/的/干扰/，/无奈/之下/队员/们/只/慢跑/了/25/分钟/就/草草收场/。/31/日/上午/10/点/，/国奥队/在/奥体中心/外场/训练/的/时候/，/天/就是/阴沉沉/的/，/气象预报/显示/当天/下午/沈阳/就/有/大雨/，/但/幸好/队伍/上午/的/训练/并/没有/受到/任何/干扰/。/下午/6/点/，/当/球队/抵达/训练场/时/，/大雨/已经/下/了/几个/小时/，/而且/丝毫/没有/停下来/的/意思/。/抱/着/试一试/的/态度/，/球队/开始/了/当天/下午/的/例行/训练/，/25/分钟/过去/了/，/天气/没有/任何/转好/的/迹象/，/为了/保护/球员/们/，/国奥队/决定/中止/当天/的/训练/，/全队/立即/返回/酒店/。/在/雨/中/训练/对/足球队/来说/并/不是/什么/稀罕/事/，/但/在/奥运会/即将/开始/之前/，/全队/变得/“/娇贵/”/了/。/在/沈阳/最后/一周/的/训练/，/国奥队/首先/要/保证/现有/的/球员/不再/出现意外/的/伤病/情况/以免/影响/正式/比赛/，/因此/这一/阶段/控制/训练/受伤/、/控制/感冒/等/疾病/的/出现/被/队伍/放在/了/相当/重要/的/位置/。/而/抵达/沈阳/之后/，/中/后卫/冯萧霆/就/一直/没有/训练/，/冯萧霆/是/7/月/27/日/在/长春/患上/了/感冒/，/因此/也/没有/参加/29/日/跟/塞尔维亚/的/热身赛/。/队伍/介绍/说/，/冯萧霆/并/没有/出现/发烧/症状/，/但/为了/安全/起/见/，/这/两天/还是/让/他/静养/休息/，/等/感冒/彻底/好/了/之后/再/恢复/训练/。/由于/有/了/冯萧霆/这个/例子/，/因此/国奥队/对雨中/训练/就/显得/特别/谨慎/，/主要/是/担心/球员/们/受凉/而/引发/感冒/，/造成/非战斗/减员/。/而/女足/队员/马晓旭/在/热身赛/中/受伤/导致/无缘/奥运/的/前科/，/也/让/在/沈阳/的/国奥队/现在/格外/警惕/，/“/训练/中/不断/嘱咐/队员/们/要/注意/动作/，/我们/可/不能/再出/这样/的/事情/了/。/”/一位/工作人员/表示/。/从/长春/到/沈阳/，/雨水/一路/伴随/着/国奥队/，/“/也/邪/了/，/我们/走/到/哪儿/雨/就/下/到/哪儿/，/在/长春/几次/训练/都/被/大雨/给/搅和/了/，/没想到/来/沈阳/又/碰到/这种/事情/。/”/一位/国奥/球员/也/对/雨水/的/“/青睐/”/有些/不解/。\n",
      "line number: 0\n",
      "line number: 1000\n",
      "line number: 2000\n",
      "line number: 3000\n",
      "line number: 4000\n",
      "line number: 5000\n",
      "line number: 6000\n",
      "line number: 7000\n",
      "line number: 8000\n",
      "line number: 9000\n",
      "first testing data: label 体育 segment 鲍勃/库西/奖归/谁/属/？/ /NCAA/最强/控卫/是/坎巴/还是/弗神/新浪/体育讯/如今/，/本赛季/的/NCAA/进入/到/了/末段/，/各项/奖项/的/评选/结果/也/即将/出炉/，/其中/评选/最佳/控卫/的/鲍勃/-/库西/奖/就/将/在/下周/最终/四强/战时/公布/，/鲍勃/-/库西/奖是/由奈/史密斯/篮球/名人堂/提供/，/旨在/奖励/年度/最佳/大学/控卫/。/最终/获奖/的/球员/也/即将/在/以下/几名/热门/人选/中/产生/。/〈/〈/〈/ /NCAA/疯狂/三月/专题/主页/上线/，/点击/链接/查看/精彩内容/吉梅尔/-/弗雷/戴特/，/杨百翰/大学/“/弗神/”/吉梅尔/-/弗雷/戴特/一直/都/备受/关注/，/他/不仅仅/是/一名/射手/，/他会用/“/终结/对手/脚踝/”/一样/的/变向/过/掉/面前/的/防守/者/，/并且/他/可以/用/任意/一支/手/完成/得分/，/如果/他/被/犯规/了/，/可以/提前/把/这/两份/划入/他/的/帐/下/了/，/因为/他/是/一名/命中率/高达/90%/的/罚球/手/。/弗雷/戴特/具有/所有/伟大/控卫/都/具备/的/一点/特质/，/他/是/一位/赢家/也/是/一位/领导者/。/“/他/整个/赛季/至始/至终/的/稳定/领导/着/球队/前进/，/这是/无可比拟/的/。/”/杨百翰/大学/主教练/戴夫/-/罗斯/称赞/道/，/“/他/的/得分/能力/毋庸置疑/，/但是/我/认为/他/带领/球队/获胜/的/能力/才/是/他/最/重要/的/控卫/职责/。/我们/在/主场/之外/的/比赛/(/客场/或/中/立场/)/共/取胜/19/场/，/他/都/表现/的/很棒/。/”/弗雷/戴特/能否/在/NBA/取得成功/？/当然/，/但是/有/很多/专业人士/比/我们/更/有/资格/去/做出/这样/的/判断/。/“/我/喜爱/他/。/”/凯尔特人/主教练/多克/-/里/弗斯/说道/，/“/他/很棒/，/我/看过/ESPN/的/片段/剪辑/，/从/剪辑/来看/，/他/是/个/超级/巨星/，/我/认为/他/很/成为/一名/优秀/的/NBA/球员/。/”/诺兰/-/史密斯/，/杜克大学/当/赛季/初/，/球队/宣布/大/一天/才/控卫凯瑞/-/厄尔/文因/脚趾/的/伤病/缺席/赛季/大部分/比赛/后/，/诺兰/-/史密斯/便/开始/接管/球权/，/他/在/进攻/端上/足/发条/，/在/ACC/联盟/(/杜克大学/所在/分区/)/的/得分/榜上/名列前茅/，/但/同时/他/在/分区/助攻/榜上/也/占据/头名/，/这/在/众强/林立/的/ACC/联盟/前无古人/。/“/我/不/认为/全美/有/其他/的/球员/能/在/凯瑞/-/厄尔/文/受伤/后/，/如此/好/的/接管/球队/，/并且/之前/毫无准备/。/”/杜克/主教练/迈克/-/沙舍/夫斯基/赞扬/道/，/“/他会/将/比赛/带入/自己/的/节奏/，/得分/，/组织/，/领导/球队/，/无所不能/。/而且/他/现在/是/攻防/俱佳/，/对/持球/人/的/防守/很/有/提高/。/总之/他/拥有/了/辉煌/的/赛季/。/”/坎巴/-/沃克/，/康涅狄格/大学/坎巴/-/沃克/带领/康涅狄格/在/赛季/初/的/毛伊岛/邀请赛/一路/力克/密歇根州/大/和/肯塔基/等队/夺冠/，/他场/均/30/分/4/助攻/得到/最佳/球员/。/在/大东/赛区/锦标赛/和/全国/锦标赛/中/，/他场/均/27.1/分/，/6.1/个/篮板/，/5.1/次/助攻/，/依旧/如此/给力/。/他/以/疯狂/的/表现/开始/这个/赛季/，/也/将/以/疯狂/的/表现/结束/这个/赛季/。/“/我们/在/全国/锦标赛/中/前进/着/，/并且/之前/曾经/5/天/连赢/5/场/，/赢得/了/大东/赛区/锦标赛/的/冠军/，/这些/都/归功于/坎巴/-/沃克/。/”/康涅狄格/大学/主教练/吉姆/-/卡洪/称赞/道/，/“/他/是/一名/纯正/的/控卫/而且/能为/我们/得分/，/他/有/过/单场/42/分/，/有过/单场/17/助攻/，/也/有/过/单场/15/篮板/。/这些/都/是/一名/6/英尺/175/镑/的/球员/所/完成/的/啊/！/我们/有/很多/好/球员/，/但/他/才/是/最好/的/领导者/，/为/球队/所/做/的/贡献/也/是/最大/。/”/乔丹/-/泰勒/，/威斯康辛/大学/全美/没有/一个/持球者/能/像/乔丹/-/泰勒/一样/很少/失误/，/他/4.26/的/助攻/失误/在/全美/遥遥领先/，/在/大十/赛区/的/比赛/中/，/他/平均/35.8/分钟/才/会/有/一次/失误/。/他/还是/名/很/出色/的/得分手/，/全场/砍/下/39/分/击败/印第安纳/大学/的/比赛/就是/最好/的/证明/，/其中/下半场/他/曾经/连/拿/18/分/。/“/那个/夜晚/他/证明/自己/值得/首轮/顺位/。/”/当时/的/见证者/印第安纳/大学/主教练/汤姆/-/克/雷恩/说道/。/“/对/一名/控卫/的/所有/要求/不过/是/领导/球队/、/使/球队/变/的/更好/、/带领/球队/成功/，/乔丹/-/泰勒/全/做到/了/。/”/威斯康辛/教练/博/-/莱恩/说道/。/诺里斯/-/科尔/，/克利夫兰/州/大/诺里斯/-/科尔/的/草根/传奇/正在/上演/，/默默无闻/的/他/被/克利夫兰/州/大/招募/后/便/开始/刻苦/地/训练/，/去年/夏天/他/曾/加练/上/千次/跳投/，/来/提高/这个/可能/的/弱点/。/他/在/本赛季/与/杨斯顿/州/大/的/比赛/中/得到/40/分/20/篮板/和/9/次/助攻/，/在/他/之前/，/过去/15/年/只有/一位/球员/曾经/在/NCAA/一级/联盟/做到/过/40/+/20/，/他/的/名字/是/布雷克/-/格里芬/。/“/他/可以/很/轻松/地防下/对方/王牌/。/”/克利夫兰/州/大/主教练/加里/-/沃特斯/如此/称赞/自己/的/弟子/，/“/同时/他/还/能/得分/，/并/为/球队/助攻/，/他/几乎/能/做到/一个/成功/的/团队/所有/需要/的/事/。/”/这/其中/四名/球员/都/带领/自己/的/球队/进入/到/了/甜蜜/16/强/，/虽然/有/3/个/球员/和/他们/各自/的/球队/被/挡/在/8/强/的/大门/之外/，/但是/他们/已经/表现/的/足够/出色/，/不远/的/将来/他们/很/可能/出现/在/一所/你/熟悉/的/NBA/球馆/里/。/(/clay/)\n",
      "label vocab: {'体育': 0, '娱乐': 1, '家居': 2, '房产': 3, '教育': 4, '时尚': 5, '时政': 6, '游戏': 7, '科技': 8, '财经': 9}\n",
      "word list length: 359255\n",
      "word list length after filtering: 208966\n",
      "vocab size: 208967\n",
      "process 0 data\n",
      "process 1000 data\n",
      "process 2000 data\n",
      "process 3000 data\n",
      "process 4000 data\n",
      "process 5000 data\n",
      "process 6000 data\n",
      "process 7000 data\n",
      "process 8000 data\n",
      "process 9000 data\n",
      "process 10000 data\n",
      "process 11000 data\n",
      "process 12000 data\n",
      "process 13000 data\n",
      "process 14000 data\n",
      "process 15000 data\n",
      "process 16000 data\n",
      "process 17000 data\n",
      "process 18000 data\n",
      "process 19000 data\n",
      "process 20000 data\n",
      "process 21000 data\n",
      "process 22000 data\n",
      "process 23000 data\n",
      "process 24000 data\n",
      "process 25000 data\n",
      "process 26000 data\n",
      "process 27000 data\n",
      "process 28000 data\n",
      "process 29000 data\n",
      "process 30000 data\n",
      "process 31000 data\n",
      "process 32000 data\n",
      "process 33000 data\n",
      "process 34000 data\n",
      "process 35000 data\n",
      "process 36000 data\n",
      "process 37000 data\n",
      "process 38000 data\n",
      "process 39000 data\n",
      "process 40000 data\n",
      "process 41000 data\n",
      "process 42000 data\n",
      "process 43000 data\n",
      "process 44000 data\n",
      "process 45000 data\n",
      "process 46000 data\n",
      "process 47000 data\n",
      "process 48000 data\n",
      "process 49000 data\n",
      "process 0 data\n",
      "process 1000 data\n",
      "process 2000 data\n",
      "process 3000 data\n",
      "process 4000 data\n",
      "process 5000 data\n",
      "process 6000 data\n",
      "process 7000 data\n",
      "process 8000 data\n",
      "process 9000 data\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'libsvm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-53c494b3eab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlibsvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvmutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm_save_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msvm_load_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibsvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommonutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm_read_problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'libsvm'"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import jieba\n",
    "\n",
    "train_file='./data/cnews.train.txt' # training data file name\n",
    "test_file='./data/cnews.test.txt'  # test data file name\n",
    "vocab='./data/cnews_dict.txt' # dictionary\n",
    "\n",
    "with codecs.open(train_file, 'r', 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# print sample content\n",
    "label, content = lines[0].strip('\\r\\n').split('\\t')\n",
    "print(content)\n",
    "\n",
    "\n",
    "# print word segment results\n",
    "segment = jieba.cut(content)\n",
    "print('/'.join(segment))\n",
    "\n",
    "\n",
    "# cut data\n",
    "def process_line(idx, line):\n",
    "    data = tuple(line.strip('\\r\\n').split('\\t'))\n",
    "    if not len(data)==2:\n",
    "        return None\n",
    "    content_segged = list(jieba.cut(data[1]))\n",
    "    if idx % 1000 == 0:\n",
    "        print('line number: {}'.format(idx))\n",
    "    return (data[0], content_segged)\n",
    "\n",
    "# data loading method\n",
    "def load_data(file):\n",
    "    with codecs.open(file, 'r', 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    data_records = [process_line(idx, line) for idx, line in enumerate(lines)]\n",
    "    data_records = [data for data in data_records if data is not None]\n",
    "    return data_records\n",
    "\n",
    "\n",
    "# load and process training data\n",
    "train_data = load_data(train_file)\n",
    "print('first training data: label {} segment {}'.format(train_data[0][0], '/'.join(train_data[0][1])))\n",
    "# load and process testing data\n",
    "test_data = load_data(test_file)\n",
    "print('first testing data: label {} segment {}'.format(test_data[0][0], '/'.join(test_data[0][1])))\n",
    "\n",
    "\n",
    "\n",
    "def build_vocab(train_data, thresh):\n",
    "    vocab = {'<UNK>': 0}\n",
    "    word_count = {} # word frequency\n",
    "    for idx, data in enumerate(train_data):\n",
    "        content = data[1]\n",
    "        for word in content:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "    word_list = [(k, v) for k, v in word_count.items()]\n",
    "    print('word list length: {}'.format(len(word_list)))\n",
    "    word_list.sort(key = lambda x : x[1], reverse = True) # sorted by word frequency\n",
    "    word_list_filtered = [word for word in word_list if word[1] > thresh]\n",
    "    print('word list length after filtering: {}'.format(len(word_list_filtered)))\n",
    "    # construct vocab\n",
    "    for word in word_list_filtered:\n",
    "        vocab[word[0]] = len(vocab)\n",
    "    print('vocab size: {}'.format(len(vocab))) # vocab size is word list size +1 due to unk token\n",
    "    return vocab\n",
    "\n",
    "# vocab = build_vocab(train_data, 1)\n",
    "def build_label_vocab(cate_file):\n",
    "    label_vocab = {}\n",
    "    with codecs.open(cate_file, 'r', 'utf-8') as f:\n",
    "        for lines in f:\n",
    "            line = lines.strip().split('\\t')\n",
    "            label_vocab[line[0]] = int(line[1])\n",
    "    return label_vocab\n",
    "\n",
    "label_vocab = build_label_vocab('./data/cnews.category.txt')\n",
    "print('label vocab: {}'.format(label_vocab))\n",
    "\n",
    "\n",
    "def construct_trainable_matrix(corpus, vocab, label_vocab, out_file):\n",
    "    records = []\n",
    "    for idx, data in enumerate(corpus):\n",
    "        if idx % 1000 == 0:\n",
    "            print('process {} data'.format(idx))\n",
    "        label = str(label_vocab[data[0]])  # label id\n",
    "        token_dict = {}\n",
    "        for token in data[1]:\n",
    "            token_id = vocab.get(token, 0)\n",
    "            if token_id in token_dict:\n",
    "                token_dict[token_id] += 1\n",
    "            else:\n",
    "                token_dict[token_id] = 1\n",
    "        feature = [str(int(k) + 1) + ':' + str(v) for k, v in token_dict.items()]\n",
    "        feature_text = ' '.join(feature)\n",
    "        records.append(label + ' ' + feature_text)\n",
    "\n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write('\\n'.join(records))\n",
    "vocab = build_vocab(train_data, 1)\n",
    "# vocab = [word.strip() for word in open('./data/cnews.vocab.txt','r',encoding='utf-8').readlines()]\n",
    "construct_trainable_matrix(train_data, vocab, label_vocab, './data/train.svm.txt')\n",
    "construct_trainable_matrix(test_data, vocab, label_vocab, './data/test.svm.txt')\n",
    "\n",
    "\n",
    "from libsvm import svm\n",
    "from libsvm.svmutil import svm_train,svm_predict,svm_save_model,svm_load_model\n",
    "from libsvm.commonutil import svm_read_problem\n",
    "# train svm\n",
    "train_label, train_feature = svm_read_problem('./data/train.svm.txt')\n",
    "print(train_label[0], train_feature[0])\n",
    "model=svm_train(train_label,train_feature,'-s 0 -c 5 -t 0 -g 0.5 -e 0.1')\n",
    "\n",
    "# predict\n",
    "test_label, test_feature = svm_read_problem('./data/test.svm.txt')\n",
    "print(test_label[0], test_feature[0])\n",
    "p_labs, p_acc, p_vals = svm_predict(test_label, test_feature, model)\n",
    "\n",
    "print('accuracy: {}'.format(p_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
