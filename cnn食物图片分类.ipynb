{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目3-食物图片分类\n",
    "\n",
    "## 友情提示\n",
    "同学们可以前往课程作业区先行动手尝试！！！\n",
    "\n",
    "## 项目描述\n",
    "训练一个简单的卷积神经网络，实现食物图片的分类。\n",
    "\n",
    "## 数据集介绍\n",
    "本次使用的数据集为food-11数据集，共有11类\n",
    "\n",
    "Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit.\n",
    "（面包，乳制品，甜点，鸡蛋，油炸食品，肉类，面条/意大利面，米饭，海鲜，汤，蔬菜/水果）\n",
    "Training set: 9866张\n",
    "Validation set: 3430张\n",
    "Testing set: 3347张\n",
    "\n",
    "**数据格式**\n",
    "下载 zip 档后解压缩会有三个资料夹，分别为training、validation 以及 testing\n",
    "training 以及 validation 中的照片名称格式为 [类别]_[编号].jpg，例如 3_100.jpg 即为类别 3 的照片（编号不重要）\n",
    "\n",
    "## 项目要求\n",
    "* 请使用 CNN 搭建 model\n",
    "* 不能使用额外 dataset\n",
    "* 禁止使用 pre-trained model(只能自己手写CNN)\n",
    "* 请不要上网寻找 label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./data/data74297/food-11.zip\r\n",
      "  End-of-central-directory signature not found.  Either this file is not\r\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\r\n",
      "  latter case the central directory and zipfile comment will be found on\r\n",
      "  the last disk(s) of this archive.\r\n",
      "note:  ./data/data74297/food-11.zip may be a plain executable, not an archive\r\n",
      "unzip:  cannot find zipfile directory in one of ./data/data74297/food-11.zip or\r\n",
      "        ./data/data74297/food-11.zip.zip, and cannot find ./data/data74297/food-11.zip.ZIP, period.\r\n"
     ]
    }
   ],
   "source": [
    " !unzip -d work ./data/data74297/food-11.zip # 解压缩food-11数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境配置/安装\n",
    "\n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 定义数据集\n",
    "在 paddle 中，我们可以利用 paddle.io 的 Dataset 及 DataLoader 来\"包装\"数据，使后续的训练及预测更为方便。\n",
    "Dataset 需要 overload 两个函数：__len__ 及 __getitem__\n",
    "__len__ 必须要回传 dataset 的大小，而 __getitem__ 则定义了当函数利用 [idx] 取值时，数据集应该要怎么回传数据。\n",
    "实际上我们并不会直接使用到这两个函数，但是使用 DataLoader 在 enumerate Dataset 时会使用到，没有做的话会在运行阶段出现错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import需要的套件\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.io import Dataset, DataLoader\n",
    "from paddle.nn import Sequential, Conv2D, BatchNorm2D, ReLU, MaxPool2D, Linear, Flatten\n",
    "from paddle.vision.transforms import Compose, Transpose, RandomRotation, RandomHorizontalFlip, Normalize, Resize\n",
    "# 分配GPU设备\n",
    "place = paddle.CUDAPlace(0)\n",
    "paddle.disable_static(place)\n",
    "paddle.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, image_path, image_size=(128, 128), mode='train'):\n",
    "        self.image_path = image_path\n",
    "        self.image_file_list = sorted(os.listdir(image_path))\n",
    "        self.mode = mode\n",
    "        # training 时做 data augmentation\n",
    "        self.train_transforms = Compose([\n",
    "            Resize(size=image_size),\n",
    "            RandomHorizontalFlip(),\n",
    "            RandomRotation(15),\n",
    "            Transpose(),\n",
    "            Normalize(mean=127.5, std=127.5)\n",
    "        ])\n",
    "        # testing 时不需做 data augmentation\n",
    "        self.test_transforms = Compose([\n",
    "            Resize(size=image_size),\n",
    "            Transpose(),\n",
    "            Normalize(mean=127.5, std=127.5)\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(os.path.join(self.image_path, self.image_file_list[idx]))\n",
    "        if self.mode == 'train':\n",
    "            img = self.train_transforms(img)\n",
    "            label = int(self.image_file_list[idx].split(\"_\")[0])\n",
    "            return img, label\n",
    "        else:\n",
    "            img = self.test_transforms(img)\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "traindataset = FoodDataset('work/food-11/training')\n",
    "valdataset = FoodDataset('work/food-11/validation')\n",
    "\n",
    "train_loader = DataLoader(traindataset, places=paddle.CUDAPlace(0), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(valdataset, places=paddle.CUDAPlace(0), batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型结构\n",
    "\n",
    "卷积神经网络时常使用“Conv+BN+激活+池化”作为一个基础block，我们可以将多个block堆叠在一起，进行特征提取，最后连接一个Linear层，实现图片分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifier(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # input 维度 [3, 128, 128]\n",
    "        self.cnn = Sequential(\n",
    "            Conv2D(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
    "            BatchNorm2D(64),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2, 2, 0),      # [64, 64, 64]\n",
    "\n",
    "            Conv2D(64, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            BatchNorm2D(128),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2, 2, 0),      # [128, 32, 32]\n",
    "\n",
    "            Conv2D(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "            BatchNorm2D(256),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2, 2, 0),      # [256, 16, 16]\n",
    "\n",
    "            Conv2D(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "            BatchNorm2D(512),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2, 2, 0),       # [512, 8, 8]\n",
    "            \n",
    "            Conv2D(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "            BatchNorm2D(512),\n",
    "            ReLU(),\n",
    "            MaxPool2D(2, 2, 0),       # [512, 4, 4]\n",
    "        )\n",
    "\n",
    "        self.fc = Sequential(\n",
    "            Linear(512*4*4, 1024),\n",
    "            ReLU(),\n",
    "            Linear(1024, 512),\n",
    "            ReLU(),\n",
    "            Linear(512, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_axis=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1      [[1, 3, 128, 128]]   [1, 64, 128, 128]        1,792     \n",
      " BatchNorm2D-1  [[1, 64, 128, 128]]   [1, 64, 128, 128]         256      \n",
      "    ReLU-1      [[1, 64, 128, 128]]   [1, 64, 128, 128]          0       \n",
      "  MaxPool2D-1   [[1, 64, 128, 128]]    [1, 64, 64, 64]           0       \n",
      "   Conv2D-2      [[1, 64, 64, 64]]     [1, 128, 64, 64]       73,856     \n",
      " BatchNorm2D-2   [[1, 128, 64, 64]]    [1, 128, 64, 64]         512      \n",
      "    ReLU-2       [[1, 128, 64, 64]]    [1, 128, 64, 64]          0       \n",
      "  MaxPool2D-2    [[1, 128, 64, 64]]    [1, 128, 32, 32]          0       \n",
      "   Conv2D-3      [[1, 128, 32, 32]]    [1, 256, 32, 32]       295,168    \n",
      " BatchNorm2D-3   [[1, 256, 32, 32]]    [1, 256, 32, 32]        1,024     \n",
      "    ReLU-3       [[1, 256, 32, 32]]    [1, 256, 32, 32]          0       \n",
      "  MaxPool2D-3    [[1, 256, 32, 32]]    [1, 256, 16, 16]          0       \n",
      "   Conv2D-4      [[1, 256, 16, 16]]    [1, 512, 16, 16]      1,180,160   \n",
      " BatchNorm2D-4   [[1, 512, 16, 16]]    [1, 512, 16, 16]        2,048     \n",
      "    ReLU-4       [[1, 512, 16, 16]]    [1, 512, 16, 16]          0       \n",
      "  MaxPool2D-4    [[1, 512, 16, 16]]     [1, 512, 8, 8]           0       \n",
      "   Conv2D-5       [[1, 512, 8, 8]]      [1, 512, 8, 8]       2,359,808   \n",
      " BatchNorm2D-5    [[1, 512, 8, 8]]      [1, 512, 8, 8]         2,048     \n",
      "    ReLU-5        [[1, 512, 8, 8]]      [1, 512, 8, 8]           0       \n",
      "  MaxPool2D-5     [[1, 512, 8, 8]]      [1, 512, 4, 4]           0       \n",
      "   Linear-1         [[1, 8192]]           [1, 1024]          8,389,632   \n",
      "    ReLU-6          [[1, 1024]]           [1, 1024]              0       \n",
      "   Linear-2         [[1, 1024]]            [1, 512]           524,800    \n",
      "    ReLU-7           [[1, 512]]            [1, 512]              0       \n",
      "   Linear-3          [[1, 512]]            [1, 11]             5,643     \n",
      "===========================================================================\n",
      "Total params: 12,836,747\n",
      "Trainable params: 12,830,859\n",
      "Non-trainable params: 5,888\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 49.59\n",
      "Params size (MB): 48.97\n",
      "Estimated Total Size (MB): 98.74\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看模型结构\n",
    "my_model = paddle.Model(Classifier())\n",
    "my_model.summary((-1, 3, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型训练\n",
    "使用训练数据集训练，并使用验证数据集寻找好的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch_num = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Classifier()\n",
    "loss = paddle.nn.loss.CrossEntropyLoss() # 因为是分类任务，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters()) # optimizer 使用 Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    }
   ],
   "source": [
    "print('start training...')\n",
    "for epoch in range(epoch_num):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # 模型训练\n",
    "    model.train()\n",
    "    for img, label in train_loader():\n",
    "        optimizer.clear_grad()\n",
    "        pred = model(img)\n",
    "        step_loss = loss(pred, label)\n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(pred.numpy(), axis=1) == label.numpy())\n",
    "        train_loss += step_loss.numpy()[0]\n",
    "\n",
    "    # 模型验证\n",
    "    model.eval()\n",
    "    for img, label in val_loader():\n",
    "        pred = model(img)\n",
    "        step_loss = loss(pred, label)\n",
    "        \n",
    "        val_acc += np.sum(np.argmax(pred.numpy(), axis=1) == label.numpy())\n",
    "        val_loss += step_loss.numpy()[0]\n",
    "\n",
    "    # 将结果打印出来\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "                (epoch + 1, epoch_num, \\\n",
    "                 time.time()-epoch_start_time, \\\n",
    "                 train_acc/traindataset.__len__(), \\\n",
    "                 train_loss/traindataset.__len__(), \\\n",
    "                 val_acc/valdataset.__len__(), \\\n",
    "                 val_loss/valdataset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "得到好的参数后，我们使用训练数据和验证数据共同训练（数据量变多，模型效果较好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir 'work/food-11/train_val'\n",
    "!cp work/food-11/training/* work/food-11/train_val/\n",
    "!cp work/food-11/validation/* work/food-11/train_val/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindataset = FoodDataset('work/food-11/train_val')\n",
    "train_loader = DataLoader(traindataset, places=paddle.CUDAPlace(0), batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch_num = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "model_best = Classifier()\n",
    "loss = paddle.nn.loss.CrossEntropyLoss() # 因为是分类任务，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model_best.parameters()) # optimizer 使用 Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('start training...')\n",
    "for epoch in range(epoch_num):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # 模型训练\n",
    "    model_best.train()\n",
    "    for img, label in train_loader():\n",
    "        optimizer.clear_grad()\n",
    "        pred = model_best(img)\n",
    "        step_loss = loss(pred, label)\n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(pred.numpy(), axis=1) == label.numpy())\n",
    "        train_loss += step_loss.numpy()[0]\n",
    "\n",
    "    # 将结果打印出来\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "                (epoch + 1, epoch_num, \\\n",
    "                 time.time()-epoch_start_time, \\\n",
    "                 train_acc/traindataset.__len__(), \\\n",
    "                 train_loss/traindataset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 测试\n",
    "利用刚刚训练好的模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "testdataset = FoodDataset('work/food-11/testing', mode='test')\n",
    "test_loader = DataLoader(testdataset, places=paddle.CUDAPlace(0), batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = list()\n",
    "model_best.eval()\n",
    "for img in test_loader():\n",
    "    pred = model_best(img[0])\n",
    "    test_label = np.argmax(pred.numpy(), axis=1)\n",
    "    for y in test_label:\n",
    "        prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将结果写入CSV文件\n",
    "with open('work/predict.csv', 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
