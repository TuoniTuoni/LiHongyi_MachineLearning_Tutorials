{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目7-元学习\n",
    "\n",
    "## 友情提示\n",
    "同学们可以前往课程作业区先行动手尝试！！！\n",
    "\n",
    "## 项目描述\n",
    "\n",
    "通过MAML算法实现regression。\n",
    "\n",
    "## 数据集介绍\n",
    "\n",
    "本部分内容主要是通过随机的一些数据学习，并实现拟合曲线的目的。\n",
    "\n",
    "使用的数据在代码里meta_task_data函数中在[-5, 5]范围内随机生成。\n",
    "\n",
    "## 项目要求\n",
    "\n",
    "* 构建适用MAML算法的数据集\n",
    "\n",
    "* 搭建前馈神经网络\n",
    "\n",
    "* 搭建元学习模型\n",
    "\n",
    "* 完成训练，探索元学习过程中的特点\n",
    "\n",
    "## 数据准备\n",
    "\n",
    "无\n",
    "\n",
    "## 环境配置安装\n",
    "\n",
    "无"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddle.io as data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "生成 $a*\\sin(x+b)$ 的资料点，其中 $a, b$ 的范围分别预设为 $[0.1, 5], [0, 2\\pi]$，每一个 $a*\\sin(x+b)$ 的函数有 10 个资料点当作训练资料。测试时则可用较密集的资料点直接由画图来看 generalize 的好坏。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "def meta_task_data(seed = 0, a_range=[0.1, 5], b_range = [0, 2*np.pi], task_num = 100,\n",
    "                   n_sample = 10, sample_range = [-5, 5], plot = False):\n",
    "    np.random.seed(seed)\n",
    "    a_s = np.random.uniform(low = a_range[0], high = a_range[1], size = task_num)\n",
    "    b_s = np.random.uniform(low = b_range[0], high = b_range[1], size = task_num)\n",
    "    total_x = []\n",
    "    total_y = []\n",
    "    label = []\n",
    "    for t in range(task_num):\n",
    "        x = np.random.uniform(low = sample_range[0], high = sample_range[1], size = n_sample)\n",
    "        total_x.append(x)\n",
    "        total_y.append( a_s[t]*np.sin(x+b_s[t]) )\n",
    "        label.append('{:.3}*sin(x+{:.3})'.format(a_s[t], b_s[t]))\n",
    "    if plot:\n",
    "        plot_x = [np.linspace(-5, 5, 1000)]\n",
    "        plot_y = []\n",
    "        for t in range(task_num):\n",
    "            plot_y.append( a_s[t]*np.sin(plot_x+b_s[t]) ) \n",
    "        return total_x, total_y, plot_x, plot_y, label\n",
    "    else:\n",
    "        return total_x, total_y, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以下我们将 $\\phi$ 称作 meta weight，$\\theta$ 则称为 sub weight。\n",
    "\n",
    "为了让 sub weight 的 gradient 能够传到 meta weight (因为 sub weight 的初始化是从 meta weight 来的，所以想当然我们用 sub weight 算出来的 loss 对 meta weight 也应该是可以算 gradient 才对)，这边我们需要重新定义一些 paddle 内的 layer 的运算。\n",
    "\n",
    "实际上 *MetaLinear* 这个 class 做的事情跟 paddle.nn.Linear 完全是一样的，唯一的差别在于这边的每一个 tensor 都没有被变成 paddle.nn.Parameter。这么做的原因是因为等一下我们从 meta weight 那里复制 (init weight 输入 meta weight 后 weight 与 bias 使用 .clone) 的时候，tensor 的 clone 的操作是可以传递 gradient 的，以方便我们用 gradient 更新 meta weight。这个写法的代价是我们就没办法使用 paddle.optimizer 更新 sub weight 了，因为参数都只用 tensor 纪录。也因此我们接下来需要自己写 gradient update 的函数 (只用 SGD 的话是简单的)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MetaLinear(nn.Layer):\n",
    "    def __init__(self, init_layer = None):\n",
    "        super(MetaLinear, self).__init__()\n",
    "        if type(init_layer) != type(None):\n",
    "            self.weight = init_layer.weight.clone()\n",
    "            self.bias = init_layer.bias.clone()\n",
    "    def clear_grad(self):\n",
    "        self.weight.grad  = paddle.zeros_like(self.weight)\n",
    "        self.bias.grad  = paddle.zeros_like(self.bias)\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "这里的 forward 和一般的 model 是一样的，唯一的差别在于我们需要多写一下 \\_\\_init\\_\\_ 函数让他比起一般的 paddle model 多一个可以从 meta weight 复制的功能 (这边因为我把 model 的架构写死了所以可能看起来会有点多余，读者可以自己把 net() 改成可以自己调整架构的样子，然后思考一下如何生成一个跟 meta weight 一样形状的 sub weight)\n",
    "\n",
    "update 函数就如同前一段提到的，我们需要自己先手动用 SGD 更新一次 sub weight，接着再使用下一步的 gradient (第二步) 来更新 meta weight。clear_grad 函数在此处没有用到，因为实际上我们计算第二步的 gradient 时会需要第一步的 grad，这也是为什么我们第一次 backward 的时候需要 create_graph=True (建立计算图以计算二阶的 gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class net(nn.Layer):\n",
    "    def __init__(self, init_weight=None):\n",
    "        super(net, self).__init__()\n",
    "        if type(init_weight) != type(None):\n",
    "            for name, module in init_weight.named_sublayers():\n",
    "                if name != '':\n",
    "                    setattr(self, name, MetaLinear(module))\n",
    "        else:\n",
    "            self.hidden1 = nn.Linear(1, 40)\n",
    "            self.hidden2 = nn.Linear(40, 40)\n",
    "            self.out = nn.Linear(40, 1)\n",
    "    \n",
    "    def clear_grad(self):\n",
    "        layers = self.__dict__['_buffers']\n",
    "        for layer in layers.keys():\n",
    "            layers[layer].clear_grad()\n",
    "    def update(self, parent, lr = 1):\n",
    "        layers = self.__dict__['_buffers']\n",
    "        parent_layers = parent.__dict__['_buffers']\n",
    "        for param in layers.keys():\n",
    "            layers[param].weight = layers[param].weight - lr*parent_layers[param].weight.grad\n",
    "            layers[param].bias = layers[param].bias - lr*parent_layers[param].bias.grad\n",
    "        # gradient will flow back due to clone backward\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "前面的 class 中我们已经都将复制 meta weight 到 sub weight，以及 sub weight 的更新， gradient 的传递都搞定了，meta weight 自己本身的参数就可以按照一般 paddle model 的模式，使用 paddle.optimizer 来更新了。\n",
    "\n",
    "gen_model 函数做的事情其实就是产生 N 个 sub weight，并且使用前面我们写好的复制 meta weight 的功能。\n",
    "\n",
    "注意到复制 weight 其实是整个 code 的关键，因为我们需要将 sub weight 计算的第二步 gradient 正确的传回 meta weight。读者从 meta weight 与 sub weight 更新参数作法的差别 (手动更新 / 用 paddle.nn.Parameter 与 paddle.optimizer) 可以再思考一下两者的差别。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Meta_learning_model():\n",
    "    def __init__(self, init_weight = None):\n",
    "        super(Meta_learning_model, self).__init__()\n",
    "        self.model = net()\n",
    "        if type(init_weight) != type(None):\n",
    "            self.model.load_state_dict(init_weight)\n",
    "        self.grad_buffer = 0\n",
    "    def gen_models(self, num, check = True):\n",
    "        models = [net(init_weight=self.model) for i in range(num)]\n",
    "        return models\n",
    "    def clear_buffer(self):\n",
    "        print(\"Before grad\", self.grad_buffer)\n",
    "        self.grad_buffer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "接下来就是生成训练 / 测试资料，建立 meta weightmeta weight 的模型以及用来比较的 model pretraining 的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TensorDataset(data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.x[index]\n",
    "        label = self.y[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsz = 10\n",
    "train_x, train_y, train_label = meta_task_data(task_num=50000*10) \n",
    "train_x = paddle.to_tensor(train_x).unsqueeze(-1) # add one dim\n",
    "train_y = paddle.to_tensor(train_y).unsqueeze(-1)\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=bsz, shuffle=False)\n",
    "\n",
    "test_x, test_y, plot_x, plot_y, test_label = meta_task_data(task_num=1, n_sample = 10, plot=True)  \n",
    "test_x = paddle.to_tensor(test_x).unsqueeze(-1) # add one dim\n",
    "test_y = paddle.to_tensor(test_y).unsqueeze(-1) # add one dim\n",
    "plot_x = paddle.to_tensor(plot_x).unsqueeze(-1) # add one dim\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=bsz, shuffle=False)  \n",
    "\n",
    "meta_model = Meta_learning_model()\n",
    "\n",
    "meta_optimizer = paddle.optimizer.Adam(parameters=meta_model.model.parameters(), learning_rate=1e-3)\n",
    "\n",
    "\n",
    "pretrain = net()\n",
    "pretrain.train()\n",
    "pretrain_optim = paddle.optimizer.Adam(parameters=pretrain.parameters(), learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "进行训练，注意一开始我们要先生成一群 sub weight (code 里面的 sub models)，然后将一个 batch 的不同的 sin 函数的 10 笔资料点拿来训练 sub weight。注意这边 sub weight 计算第一步 gradient 与第二步 gradient 时使用各五笔不重复的资料点 (因此使用 [:5] 与 [5:] 来取)。但在训练 model pretraining 的对照组时则没有这个问题 (所以 pretraining 的 model 是可以确实的走两步 gradient 的)\n",
    "\n",
    "每一个 sub weight 计算完 loss 后相加 (内层的 for 回圈) 后就可以使用 optimizer 来更新 meta weight，再次提醒一下 sub weight 计算第一次 loss 的时候 backward 是需要 create_graph=True 的，这样计算第二步 gradient 的时候才会真的计算到二阶的项。读者可以在这个地方思考一下如何将这段程式码改成 MAML 的一阶做法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "温馨提示: 这个训练耗时极久请耐心等待."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 93/50000 [00:06<54:11, 15.35it/s]  "
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "for e in range(epoch):\n",
    "    meta_model.model.train()\n",
    "    for x, y in tqdm(train_loader):\n",
    "        sub_models = meta_model.gen_models(bsz)\n",
    "        meta_l = 0\n",
    "        for model_num in range(len(sub_models)):\n",
    "            sample = list(range(10))\n",
    "            np.random.shuffle(sample)\n",
    "            \n",
    "            #pretraining\n",
    "            pretrain_optim.clear_grad()\n",
    "            index_x = paddle.to_tensor(sample[:5])\n",
    "            x1 = paddle.index_select(x=x[model_num], index=index_x)\n",
    "            y_tilde = pretrain(x1)\n",
    "\n",
    "            index_y = paddle.to_tensor(sample[:5])\n",
    "            y1 = paddle.index_select(x=y[model_num], index=index_y)\n",
    "\n",
    "            little_l = paddle.nn.MSELoss()\n",
    "            loss = little_l(y_tilde, y1)\n",
    "            \n",
    "            loss.backward()\n",
    "            pretrain_optim.step()\n",
    "            pretrain_optim.clear_grad()\n",
    "\n",
    "            index_x2 = paddle.to_tensor(sample[5:])\n",
    "            x2 = paddle.index_select(x=x[model_num], index=index_x2)\n",
    "\n",
    "            y_tilde = pretrain(x2)\n",
    "\n",
    "            index_y2 = paddle.to_tensor(sample[5:])\n",
    "            y2 = paddle.index_select(x=y[model_num], index=index_y2)\n",
    "\n",
    "            loss_2 = little_l(y_tilde, y2)\n",
    "            loss_2.backward()\n",
    "            pretrain_optim.step()\n",
    "        \n",
    "            # meta learning\n",
    "            \n",
    "            y_tilde = sub_models[model_num](x1)\n",
    "            little_l = F.mse_loss(y_tilde, y1)\n",
    "            #计算第一次 gradient 并保留计算图以接着计算更高阶的 gradient\n",
    "            little_l.backward(retain_graph=True)\n",
    "            # little_l.backward(create_graph = True)\n",
    "            sub_models[model_num].update(lr = 1e-2, parent = meta_model.model)\n",
    "            #先清空 optimizer 中计算的 gradient 值 (避免累加)\n",
    "            meta_optimizer.clear_grad()\n",
    "            \n",
    "            #计算第二次 (二阶) 的 gradient，二阶的原因来自第一次 update 时有计算过一次 gradient 了\n",
    "            y_tilde = sub_models[model_num](x2)\n",
    "            meta_l =  meta_l + F.mse_loss(y_tilde, y2)\n",
    "\n",
    "        meta_l = meta_l / bsz\n",
    "        meta_l.backward()\n",
    "        meta_optimizer.step()\n",
    "        meta_optimizer.clear_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "测试我们训练好的 meta weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_model = copy.deepcopy(meta_model.model)\n",
    "test_model = meta_model.model\n",
    "test_model.train()\n",
    "test_optim = paddle.optimizer.SGD(parameters=test_model.parameters(), learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "先画出待测试的 sin 函数，以及用圆点点出测试时给 meta weight 训练的十笔资料点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = [9.6,7.2])\n",
    "ax = plt.subplot(111)\n",
    "plot_x1 = plot_x.squeeze().numpy()\n",
    "ax.scatter(test_x.numpy().squeeze(), test_y.numpy().squeeze())\n",
    "ax.plot(plot_x1, plot_y[0].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "分别利用十笔资料点更新 meta weight 以及 pretrained model 一个 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model.train()\n",
    "pretrain.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "    for x, y in test_loader:\n",
    "        y_tilde = test_model(x[0])\n",
    "        little_l = paddle.nn.MSELoss()\n",
    "        loss = little_l(y_tilde, y[0])\n",
    "        test_optim.clear_grad()\n",
    "        loss.backward()\n",
    "        test_optim.step()\n",
    "        print(\"(meta)))Loss: \", loss)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for x, y in test_loader:\n",
    "        y_tilde = pretrain(x[0])\n",
    "        little_l = paddle.nn.MSELoss()\n",
    "        loss = little_l(y_tilde, y[0])\n",
    "        pretrain_optim.clear_grad()\n",
    "        loss.backward()\n",
    "        pretrain_optim.step()\n",
    "        print(\"(pretrain)Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "將更新后的模型所代表的函数绘制出來，与真实的 sin 函数比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model.eval()\n",
    "pretrain.eval()\n",
    "\n",
    "plot_y_tilde = test_model(plot_x[0]).squeeze().detach().numpy()\n",
    "plot_x2 = plot_x.squeeze().numpy()\n",
    "ax.plot(plot_x2, plot_y_tilde, label = 'tune(disjoint)')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_y_tilde = pretrain(plot_x[0]).squeeze().detach().numpy()\n",
    "plot_x2 = plot_x.squeeze().numpy()\n",
    "ax.plot(plot_x2, plot_y_tilde, label = 'pretrain')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "执行底下的 cell 以显示图形，并重复执行更新 meta weight 与 pretrained model 的 cell 来比较多更新几步后是否真的能看出 meta learning 比 model pretraining 有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
