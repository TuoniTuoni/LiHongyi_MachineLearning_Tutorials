{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目3.2-神经网络压缩（知识萃取）\n",
    "\n",
    "## 友情提示\n",
    "同学们可以前往课程作业区先行动手尝试 ！！！\n",
    "\n",
    "## 项目描述\n",
    "\n",
    "知识萃取：用一个小model模拟出大model的行为/正确率。        \n",
    "\n",
    "## 数据集介绍\n",
    "\n",
    "本次使用的数据集为food-11数据集，共有11类\n",
    "\n",
    "Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit.\n",
    "（面包，乳制品，甜点，鸡蛋，油炸食品，肉类，面条/意大利面，米饭，海鲜，汤，蔬菜/水果）\n",
    "Training set: 9866张\n",
    "Validation set: 3430张\n",
    "Testing set: 3347张\n",
    "\n",
    "**数据格式**\n",
    "\n",
    "下载 zip 档后解压缩会有三个资料夹，分别为training、validation 以及 testing\n",
    "training 以及 validation 中的照片名称格式为 [类别]_[编号].jpg，例如 3_100.jpg 即为类别 3 的照片（编号不重要）\n",
    "\n",
    "## 项目要求\n",
    "\n",
    "知识萃取: 让小model在学习任务的时候，藉由观察大model的行为来让自己学得更好。(直译:让小model萃取大model的知识)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: work/__MACOSX/food-11/validation/._5_210.jpg  \r"
     ]
    }
   ],
   "source": [
    "!unzip -d work data/data58106/food-11.zip # 解压缩food-11数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境配置/安装\n",
    "\n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 简介\n",
    "\n",
    "\n",
    "本项目任务是模型压缩 - Neural Network Compression。\n",
    "\n",
    "Compression有很多种门派，在这裡我们会介绍上课出现过的其中四种，分别是:\n",
    "\n",
    "* 知识蒸馏 Knowledge Distillation\n",
    "* 网路剪枝 Network Pruning\n",
    "* 用少量参数来做CNN Architecture Design\n",
    "* 参数量化 Weight Quantization\n",
    "\n",
    "在这个notebook中我们会介绍Knowledge Distillation，\n",
    "而我们有提供已经学习好的大model方便大家做Knowledge Distillation。\n",
    "而我们使用的小model是\"Architecture Design\"过的model。\n",
    "\n",
    "* Architecute Design在同目录中的hw7_Architecture_Design.ipynb。\n",
    "* 大网络的模型我们使用paddle提供的ResNet18训练的模型，路径为/home/aistudio/work/tea_net.pdparams\n",
    "  * 请使用paddle提供的ResNet18，把num_classes改成11后load进去即可。(后面有范例。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import os\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as optim\n",
    "import paddle.nn.functional as F\n",
    "import paddle.vision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "知识萃取\n",
    "===\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c660e2f2598a48988c78ac90775020cdeac8654339fb4617a3023b284aac2558)\n",
    "\n",
    "简单上来说就是让已经做得很好的大model们去告诉小model\"如何\"学习。\n",
    "而我们如何做到这件事情呢? 就是利用大model预测的logits给小model当作标准就可以了。\n",
    "\n",
    "## 为什么这会work?\n",
    "* 例如当data不是很乾淨的时候，对一般的model来说他是个noise，只会干扰学习。透过去学习其他大model预测的logits会比较好。\n",
    "* label和label之间可能有关连，这可以引导小model去学习。例如数字8可能就和6,9,0有关係。\n",
    "* 弱化已经学习不错的target(?)，避免让其gradient干扰其他还没学好的task。\n",
    "\n",
    "\n",
    "## 要怎么操作?\n",
    "* $Loss = \\alpha T^2 \\times KL(\\frac{\\text{Teacher's Logits}}{T} || \\frac{\\text{Student's Logits}}{T}) + (1-\\alpha)(\\text{Original Loss})$\n",
    "\n",
    "\n",
    "* 以下code为甚麽要对student使用log_softmax: https://github.com/peterliht/knowledge-distillation-pytorch/issues/2\n",
    "* reference: [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
    "    # 一般的Cross Entropy\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    # 让logits的log_softmax对目标几率(teacher的logits/T后softmax)做KL Divergence。\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs / T, axis=-1),\n",
    "                                                    F.softmax(teacher_outputs / T, axis=-1)) * (alpha * T * T)\n",
    "    return hard_loss + soft_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据处理\n",
    "\n",
    "我们的Dataset使用的是跟Hw3 - CNN同样的Dataset，因此这个区块的Augmentation / Read Image大家参考或直接抄就好。\n",
    "\n",
    "如果有不会的话可以回去看Hw3的colab。\n",
    "\n",
    "需要注意的是如果要自己写的话，Augment的方法最好使用我们的方法，避免输入有差异导致Teacher Net预测不好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import paddle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import paddle.vision.transforms as transforms\n",
    "\n",
    "\n",
    "class MyDataset(paddle.io.Dataset):\n",
    "\n",
    "    def __init__(self, folderName, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "        for img_path in sorted(glob(folderName + '/*.jpg')):\n",
    "            try:\n",
    "                # Get classIdx by parsing image path\n",
    "                # class_idx = int(re.findall(re.compile(r'\\d+'), img_path)[1])\n",
    "                class_idx = int(os.path.split(img_path)[-1].split(\"_\")[0])\n",
    "                #print(class_idx)\n",
    "            except:\n",
    "                # if inference mode (there's no answer), class_idx default 0\n",
    "                class_idx = 0\n",
    "\n",
    "            image = Image.open(img_path)\n",
    "            # Get File Descriptor\n",
    "            image_fp = image.fp\n",
    "            image.load()\n",
    "            # Close File Descriptor (or it'll reach OPEN_MAX)\n",
    "            image_fp.close()\n",
    "\n",
    "            self.data.append(image)\n",
    "            self.label.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, paddle.Tensor):\n",
    "            idx = idx.tolist()\n",
    "        image = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.label[idx]\n",
    "\n",
    "\n",
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "def get_dataloader(mode='training', batch_size=32):\n",
    "\n",
    "    assert mode in ['training', 'testing', 'validation']\n",
    "\n",
    "    dataset = MyDataset(\n",
    "        os.path.join(\"/home/aistudio/work/food-11\", f'{mode}'),\n",
    "        transform=trainTransform if mode == 'training' else testTransform)\n",
    "\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(mode == 'training'))\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 预处理\n",
    "\n",
    "我们已经提供TeacherNet的state_dict，其架构是paddle.vision提供的ResNet18。\n",
    "\n",
    "至于StudentNet的架构则在hw7_Architecture_Design.ipynb中。\n",
    "\n",
    "这里我们使用的Optimizer为AdamW，没有为甚么，就纯粹我想用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get dataloader\n",
    "train_dataloader = get_dataloader('training', batch_size=32)\n",
    "valid_dataloader = get_dataloader('validation', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StudentNet(paddle.nn.Layer):\n",
    "    '''\n",
    "      在这个网络里，我们会使用Depthwise & Pointwise Convolution Layer来做model。\n",
    "      你会发现，将原本的Convolution Layer换成Dw & Pw后，Accuracy通常不会降很多。\n",
    "\n",
    "      另外，取名为StudentNet是因为这个Model等会要做Knowledge Distillation。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base=16, width_mult=1):\n",
    "        '''\n",
    "          参数:\n",
    "            base: 这个model一开始的通道数量，每过一层都会*2，直到base*16为止。\n",
    "            width_mult: 为了之后的Network Pruning使用，在base*8 chs的Layer上会 * width_mult代表剪枝后的通道数量。\n",
    "        '''\n",
    "        super(StudentNet, self).__init__()\n",
    "        multiplier = [1, 2, 4, 8, 16, 16, 16, 16]\n",
    "\n",
    "        bandwidth = [base * m for m in multiplier]\n",
    "\n",
    "        for i in range(3, 7):\n",
    "            bandwidth[i] = int(bandwidth[i] * width_mult)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "\n",
    "                nn.Conv2D(3, bandwidth[0], 3, 1, 1),\n",
    "                nn.BatchNorm2D(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.MaxPool2D(2, 2, 0),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                \n",
    "                nn.Conv2D(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n",
    "                nn.BatchNorm2D(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[0], bandwidth[1], 1),\n",
    "                nn.MaxPool2D(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n",
    "                nn.BatchNorm2D(bandwidth[1]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[1], bandwidth[2], 1),\n",
    "                nn.MaxPool2D(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n",
    "                nn.BatchNorm2D(bandwidth[2]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[2], bandwidth[3], 1),\n",
    "                nn.MaxPool2D(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n",
    "                nn.BatchNorm2D(bandwidth[3]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[3], bandwidth[4], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n",
    "                nn.BatchNorm2D(bandwidth[4]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[4], bandwidth[5], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n",
    "                nn.BatchNorm2D(bandwidth[5]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[5], bandwidth[6], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n",
    "                nn.BatchNorm2D(bandwidth[6]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[6], bandwidth[7], 1),\n",
    "            ),\n",
    "\n",
    "            nn.AdaptiveAvgPool2D((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(bandwidth[7], 11),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.reshape((out.shape[0], -1))\n",
    "        return self.fc(out)\n",
    "\n",
    "teacher_net = models.resnet18(pretrained=False, num_classes=11)\n",
    "stu_net = StudentNet(base=16)\n",
    "\n",
    "tea_layer_state_dict = paddle.load(\n",
    "        \"/home/aistudio/work/pre_tea_net.pdparams\")\n",
    "teacher_net.set_dict(tea_layer_state_dict)\n",
    "optimizer = optim.AdamW(learning_rate=1e-3, parameters=stu_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 开始训练\n",
    "\n",
    "* 剩下的步骤与你在做Hw3 - CNN的时候一样。\n",
    "\n",
    "## 小提醒\n",
    "\n",
    "* paddle.no_grad是指接下来的运算或该tensor不需要算gradient。\n",
    "* model.eval()与model.train()差在于Batchnorm要不要纪录，以及要不要做Dropout。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(dataloader, teacher_net, student_net, optim, update=True, alpha=0.5):\n",
    "    total_num, total_hit, total_loss = 0, 0, 0\n",
    "    for now_step, batch_data in enumerate(dataloader()):\n",
    "        # 清空 optimizer\n",
    "        optim.clear_grad()\n",
    "        # 处理 input\n",
    "        inputs, hard_labels = batch_data\n",
    "        # 因为Teacher没有要backprop，所以我们使用paddle.no_grad\n",
    "        # 告诉paddle不要暂存中间值(去做backprop)以浪费记忆体空间。\n",
    "        with paddle.no_grad():\n",
    "            soft_labels = teacher_net(inputs)\n",
    "\n",
    "        if update:\n",
    "            logits = student_net(inputs)\n",
    "            # 使用我们之前所写的融合soft label&hard label的loss。\n",
    "            # T=20是原始论文的参数设定。\n",
    "            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "            loss.backward()\n",
    "            optim.clear_grad()\n",
    "            optim.step()\n",
    "        else:\n",
    "            # 只是算validation acc的话，就开no_grad节省空间。\n",
    "            with paddle.no_grad():\n",
    "                logits = student_net(inputs)\n",
    "                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
    "\n",
    "        total_hit += (paddle.argmax(logits, axis=-1).numpy() == hard_labels.numpy).sum()\n",
    "        #print(\"pre logits: \", paddle.argmax(logits, axis=-1).numpy())\n",
    "        #print(\"hard_lables: \", hard_labels.numpy)\n",
    "        #print(\"soft_labels: \", paddle.argmax(soft_labels, axis=-1).numpy())\n",
    "        total_num += len(inputs)\n",
    "\n",
    "        total_loss += loss.numpy()[0] * len(inputs)\n",
    "    return total_loss / total_num, total_hit / total_num\n",
    "\n",
    "\n",
    "# TeacherNet永远都是Eval mode.\n",
    "teacher_net.eval()\n",
    "now_best_acc = 0\n",
    "for epoch in range(200):\n",
    "    stu_net.train()\n",
    "    train_loss, train_acc = run_epoch(train_dataloader, teacher_net, stu_net, optimizer, update=True)\n",
    "    stu_net.eval()\n",
    "    valid_loss, valid_acc = run_epoch(valid_dataloader, teacher_net, stu_net, optimizer, update=False)\n",
    "\n",
    "    # 存下最好的model。\n",
    "    if valid_acc > now_best_acc:\n",
    "        now_best_acc = valid_acc\n",
    "        paddle.save(stu_net.state_dict(), 'student_model.pdparams')\n",
    "    print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(\n",
    "        epoch, train_loss, train_acc, valid_loss, valid_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
