{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目6-异常检测\n",
    "\n",
    "## 友情提示\n",
    "\n",
    "同学们可以前往课程作业区先行动手尝试！！！\n",
    "\n",
    "## 项目描述\n",
    "\n",
    "这份作业要做的是半监督异常检测，也就是说训练集是干净的，测试的时候才会混进超限数据（异常）。 我们以某个简单的图像数据集（图像加上他们的分类）作为示范，训练集为原先训练中的某几类，而测试集则是原先测试集的所有数据，要侦测的异常为训练集中未出现的类别。 标签的部分，1 为超限数据（异常），而 0 为正常数据。   \n",
    "最终要实现：在只给定干净的（无异常）训练集的情况下，分辨测试集中哪些数据是来自训练集或是从未见过的类别。\n",
    "\n",
    "## 数据集介绍\n",
    "\n",
    "* Training: 某个图片数据集的训练集 (大小32*32*3) 中的属于某些标签的数据（40000 笔）  \n",
    "* Testing: 此图片数据集中的所有测试数据（10000 笔）  \n",
    "* Notice: 请勿使用额外数据进行模型训练，亦不可使用预训练模型。可用额外数据辅助验证过程。\n",
    "\n",
    "## 项目要求\n",
    "\n",
    "实现以下内容：\n",
    "1. 方法1-Autoencoder\n",
    "1. 方法2-K-means\n",
    "* 假设训练集的标签种类不多\n",
    "* 假设训练集有 n 群\n",
    "* 用 K-means 计算训练集中的 n 个 centroid，再用这 n 个 centroid 对训练集分群\n",
    "* Inlier data 与所分到群的 centroid 的距离应较 outlier 的此距离来得小\n",
    "1. 方法3-PCA\n",
    "* 计算训练集的 principal component\n",
    "* 将测试集投影在这些 component 上\n",
    "* 再将这些投影重建回原先 space 的向量\n",
    "* 对重建的图片和原图计算平方差，正常的数值应该较异常的数值为小\n",
    "\n",
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train = np.load('data/data58383/train.npy', allow_pickle=True)\n",
    "test = np.load('data/data58383/test.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 环境配置/安装\n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "K-means: 假设训练集的标签种类不多（e.g., < 20），然而因其为未知，可以猜测其为 n，亦即假设训练集有 n 群。先用 K-means 计算训练集中的 n 个 centroid，再用这 n 个 centroid 对训练集分群。应该可以观察到，正常数据与所分到群的 centroid 的距离应较异常数据的此距离来得小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import f1_score, pairwise_distances, roc_auc_score\n",
    "from scipy.cluster.vq import vq, kmeans\n",
    "\n",
    "if task == 'knn':\n",
    "    x = train.reshape(len(train), -1)\n",
    "    y = test.reshape(len(test), -1)\n",
    "    scores = list()\n",
    "    for n in range(1, 10):\n",
    "        kmeans_x = MiniBatchKMeans(n_clusters=n, batch_size=100).fit(x)\n",
    "        y_cluster = kmeans_x.predict(y)\n",
    "        print(kmeans_x.cluster_centers_[y_cluster].shape)\n",
    "        y_dist = np.sum(np.square(kmeans_x.cluster_centers_[y_cluster] - y), axis=1)\n",
    "        y_pred = y_dist\n",
    "\n",
    "    #     print('y.shape:', y.shape,'y_pred.shape:', y_pred.shape)\n",
    "    #     score = f1_score(y, y_pred, average='micro')\n",
    "    #     # score = roc_auc_score(y_label, y_pred, average='micro')\n",
    "    #     scores.append(score)\n",
    "    # print(np.max(scores), np.argmax(scores))\n",
    "    # print(scores)\n",
    "    # print('auc score: {}'.format(np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PCA: 首先计算训练集的 principle component，将测试投影在这些 component 上，再将这些投影重建回原先 space 的向量。对重建的图片和原图计算 MSE，正常数据的数值应该较异常数据的数值为小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if task == 'pca':\n",
    "\n",
    "    x = train.reshape(len(train), -1)\n",
    "    y = test.reshape(len(test), -1)\n",
    "    pca = PCA(n_components=2).fit(x)\n",
    "\n",
    "    y_projected = pca.transform(y)\n",
    "    y_reconstructed = pca.inverse_transform(y_projected)  \n",
    "    dist = np.sqrt(np.sum(np.square(y_reconstructed - y).reshape(len(y), -1), axis=1))\n",
    "    \n",
    "    y_pred = dist\n",
    "    # score = roc_auc_score(y_label, y_pred, average='micro')\n",
    "    # score = f1_score(y_label, y_pred, average='micro')\n",
    "    # print('auc score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 自编码器模型与损失\n",
    "\n",
    "* fcn_autoencoder and vae are from https://github.com/L1aoXingyu/pytorch-beginner   \n",
    "* conv_autoencoder is from https://github.com/jellycsc/PyTorch-CIFAR-10-autoencoder/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "同学们可以尝试各种不同的 VAE 架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Sequential, Linear, ReLU, Tanh, Conv2D, Conv2DTranspose\n",
    "\n",
    "class fcn_autoencoder(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(fcn_autoencoder, self).__init__()\n",
    "        self.encoder = Sequential(\n",
    "            Linear(32 * 32 * 3, 128),\n",
    "            ReLU(),\n",
    "            Linear(128, 64),\n",
    "            ReLU(), \n",
    "            Linear(64, 12), \n",
    "            ReLU(), \n",
    "            Linear(12, 3)\n",
    "            )\n",
    "        self.decoder = Sequential(\n",
    "            Linear(3, 12),\n",
    "            ReLU(),\n",
    "            Linear(12, 64),\n",
    "            ReLU(),\n",
    "            Linear(64, 128),\n",
    "            ReLU(), \n",
    "            Linear(128, 32 * 32 * 3), \n",
    "            Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class conv_autoencoder(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder, self).__init__()\n",
    "        self.encoder = Sequential(\n",
    "            Conv2D(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
    "            ReLU(),\n",
    "            Conv2D(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
    "            ReLU(),\n",
    "\t\t\tConv2D(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
    "            ReLU(),\n",
    "            Conv2D(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]\n",
    "            ReLU()\n",
    "        )\n",
    "        self.decoder = Sequential(\n",
    "            Conv2DTranspose(96, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]\n",
    "            ReLU(),\n",
    "\t\t\tConv2DTranspose(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            ReLU(),\n",
    "\t\t\tConv2DTranspose(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            ReLU(),\n",
    "            Conv2DTranspose(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
    "            Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VAE(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = Linear(32*32*3, 400)\n",
    "        self.fc21 = Linear(400, 20)\n",
    "        self.fc22 = Linear(400, 20)\n",
    "        self.fc3 = Linear(20, 400)\n",
    "        self.fc4 = Linear(400, 32*32*3)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = paddle.to_tensor(std.shape()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def loss_vae(recon_x, x, mu, logvar, criterion):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    mse = criterion(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = paddle.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return mse + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "class TensorDataset(paddle.io.Dataset):\n",
    "    def __init__(self, tensors, label):\n",
    "        self.tensors = tensors\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tensors[index], paddle.to_tensor([1.])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/300], loss:0.0608\n",
      "epoch [2/300], loss:0.0465\n",
      "epoch [3/300], loss:0.0400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-34884e77272b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cnn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0min_dygraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_next_var_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from paddle.io import DataLoader, RandomSampler\n",
    "from paddle.optimizer import Adam, AdamW\n",
    "task = 'ae'\n",
    "\n",
    "if task == 'ae':\n",
    "    num_epochs = 300 # 1000\n",
    "    batch_size = 128\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    #{'fcn', 'cnn', 'vae'} \n",
    "    model_type = 'cnn' \n",
    "\n",
    "    x = train\n",
    "    if model_type == 'fcn' or model_type == 'vae':\n",
    "        x = x.reshape(len(x), -1)\n",
    "        \n",
    "    data = paddle.to_tensor(x)\n",
    "    train_dataset = TensorDataset(data, None)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    model_classes = {'fcn':fcn_autoencoder(), 'cnn':conv_autoencoder(), 'vae':VAE()}\n",
    "    model = model_classes[model_type]\n",
    "    criterion = paddle.nn.loss.MSELoss()\n",
    "    optimizer = Adam(parameters=model.parameters(), learning_rate=learning_rate)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_dataloader:\n",
    "            if model_type == 'cnn':\n",
    "                img = paddle.transpose(data[0], perm=[0, 3, 1, 2])\n",
    "            else:\n",
    "                img = data[0]\n",
    "            # ===================forward=====================\n",
    "            img = paddle.to_tensor(img, dtype='float32')\n",
    "            output = model(img)\n",
    "            if model_type == 'vae':\n",
    "                loss = loss_vae(output[0], img, output[1], output[2], criterion)\n",
    "            else:\n",
    "                loss = criterion(output, img)\n",
    "            # ===================backward====================\n",
    "            optimizer.clear_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # ===================save====================\n",
    "            if loss.numpy()[0] < best_loss:\n",
    "                best_loss = loss.numpy()[0]\n",
    "                paddle.save(model.state_dict(), 'best_model_{}.pdparams'.format(model_type))\n",
    "        # ===================log========================\n",
    "        print('epoch [{}/{}], loss:{:.4f}'\n",
    "              .format(epoch + 1, num_epochs, loss.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "将 testing 的图片输入 model 后，可以得到其重建的图片，并对两者取平方差。可以发现 inlier 的平方差应该与 outlier 的平方差形成差距明显的两群数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: cnn\n"
     ]
    }
   ],
   "source": [
    "if task == 'ae':\n",
    "    print('model_type:', model_type)\n",
    "    if model_type == 'fcn' or model_type == 'vae':\n",
    "        y = test.reshape(len(test), -1)\n",
    "    else:\n",
    "        y = test\n",
    "        \n",
    "    data = paddle.to_tensor(y, dtype='float32')\n",
    "    test_dataset = TensorDataset(data, None)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    layer_state_dict = paddle.load('best_model_{}.pdparams'.format(model_type))\n",
    "    model.set_state_dict(layer_state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    reconstructed = list()\n",
    "    for i, data in enumerate(test_dataloader): \n",
    "        if model_type == 'cnn':\n",
    "            img = paddle.transpose(data[0], perm=[0, 3, 1, 2])\n",
    "        else:\n",
    "            img = data[0]\n",
    "        output = model(img)\n",
    "        if model_type == 'cnn':\n",
    "            output = paddle.transpose(output, perm=[0, 2, 3, 1])\n",
    "        elif model_type == 'vae':\n",
    "            output = output[0]\n",
    "        reconstructed.append(output.detach().numpy())\n",
    "    reconstructed = np.concatenate(reconstructed, axis=0)\n",
    "    anomality = np.sqrt(np.sum(np.square(reconstructed - y).reshape(len(y), -1), axis=1))\n",
    "    y_pred = anomality\n",
    "    with open('prediction.csv', 'w') as f:\n",
    "        f.write('id,anomaly\\n')\n",
    "        for i in range(len(y_pred)):\n",
    "            f.write('{},{}\\n'.format(i+1, y_pred[i]))\n",
    "    # score = roc_auc_score(y_label, y_pred, average='micro')\n",
    "    # score = f1_score(y_label, y_pred, average='micro')\n",
    "    # print('auc score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
