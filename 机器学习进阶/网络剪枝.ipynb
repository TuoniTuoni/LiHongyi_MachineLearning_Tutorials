{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目3.3-神经网络压缩（网络剪枝）\n",
    "\n",
    "## 友情提示\n",
    "同学们可以前往课程作业区先行动手尝试 ！！！\n",
    "\n",
    "## 项目描述\n",
    "\n",
    "网络剪枝：去掉已经学习好的大model的一些权重值接近于0的分枝。\n",
    "\n",
    "## 数据集介绍\n",
    "\n",
    "本次使用的数据集为food-11数据集，共有11类\n",
    "\n",
    "Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit.\n",
    "（面包，乳制品，甜点，鸡蛋，油炸食品，肉类，面条/意大利面，米饭，海鲜，汤，蔬菜/水果）\n",
    "Training set: 9866张\n",
    "Validation set: 3430张\n",
    "Testing set: 3347张\n",
    "\n",
    "**数据格式**\n",
    "\n",
    "下载 zip 档后解压缩会有三个资料夹，分别为training、validation 以及 testing\n",
    "training 以及 validation 中的照片名称格式为 [类别]_[编号].jpg，例如 3_100.jpg 即为类别 3 的照片（编号不重要）\n",
    "\n",
    "## 项目要求\n",
    "\n",
    "网络剪枝: 将已经学习好的大model做剪枝，让整体model变小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: work/__MACOSX/food-11/validation/._5_210.jpg  \r"
     ]
    }
   ],
   "source": [
    "!unzip -d work data/data58106/food-11.zip # 解压缩food-11数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 环境配置/安装\n",
    "\n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 简介\n",
    "\n",
    "本项目任务是模型压缩 - Neural Network Compression。\n",
    "\n",
    "Compression有很多种门派，在这裡我们会介绍上课出现过的其中四种，分别是:\n",
    "\n",
    "* 知识蒸馏 Knowledge Distillation\n",
    "* 网路剪枝 Network Pruning\n",
    "* 用少量参数来做CNN Architecture Design\n",
    "* 参数量化 Weight Quantization\n",
    "\n",
    "在这个notebook中我们会介绍Network Pruning，\n",
    "而我们有提供已经做完Knowledge Distillation的小model来做Pruning。\n",
    "\n",
    "* Model架构 / Architecute Design在同目录中的hw7_Architecture_Design.ipynb。\n",
    "* 已经train好的小model(1.5M， work/stu_net.pdparams)\n",
    "  * 参数为 base=16, width_mult=1 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import os\n",
    "import numpy as np\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as optim\n",
    "import paddle.nn.functional as F\n",
    "import paddle.vision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "网路剪枝\n",
    "===\n",
    "在这裡我们会教网路剪枝\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/754346f13206441aaedc32f097134cb8d479ac95e3e643338f5ab6a249f5b0e6)\n",
    "\n",
    "简单上来说就是让一个已经学完的model中的neuron进行删减，让整个网路变得更瘦。\n",
    "\n",
    "## Weight & Neuron 剪枝\n",
    "* weight和neuron pruning差别在于prune掉一个neuron就等于是把一个matrix的整个column全部砍掉。但如此一来速度就会比较快。因为neuron pruning后matrix整体变小，但weight pruning大小不变，只是有很多空洞。\n",
    "\n",
    "## 为什么要剪枝\n",
    "* 既然要Neuron Pruning，那就必须要先衡量Neuron的重要性。衡量完所有的Neuron后，就可以把比较不重要的Neuron删减掉。\n",
    "* 在这裡我们介绍一个很简单可以衡量Neuron重要性的方法 - 就是看batchnorm layer的$\\gamma$因子来决定neuron的重要性。 (by paper - Network Slimming)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6ac69ba3a4d74f1e86a02f1ed95206fceb037ff8b69d4f7b94e03fd04fd6851f)\n",
    "\n",
    "* 相信大家看这个paddle提供的batchnorm公式应该就可以意识到为甚麽$\\gamma$可以当作重要性来衡量了:)\n",
    "\n",
    "* Network Slimming其实步骤没有这麽简单，有兴趣的同学可以check以下连结。[Netowrk Slimming](https://arxiv.org/abs/1708.06519)\n",
    "\n",
    "\n",
    "## 为什么这会 work?\n",
    "* 树多必有枯枝，有些neuron只是在躺分，所以有他没他没有差。\n",
    "* 困难的说可以回想起老师说过的大乐透假说(The Lottery Ticket Hypothesis)就可以知道了。\n",
    "\n",
    "## 要怎麽操作?\n",
    "* 为了避免複杂的操作，我们会将StudentNet(width_mult=$\\alpha$)的neuron经过筛选后移植到StudentNet(width_mult=$\\beta$)。($\\alpha > \\beta$)\n",
    "* 筛选的方法也很简单，只需要抓出每一个block的batchnorm的$\\gamma$即可。\n",
    "\n",
    "## 一些实作细节\n",
    "* 假设model中间两层是这样的:\n",
    "\n",
    "|Layer|Output # of Channels|\n",
    "|-|-|\n",
    "|Input|in_chs|\n",
    "|Depthwise(in_chs)|in_chs|\n",
    "|BatchNorm(in_chs)|in_chs|\n",
    "|Pointwise(in_chs, **mid_chs**)|**mid_chs**|\n",
    "|**Depthwise(mid_chs)**|**mid_chs**|\n",
    "|**BatchNorm(mid_chs)**|**mid_chs**|\n",
    "|Pointwise(**mid_chs**, out_chs)|out_chs|\n",
    "\n",
    "则你会发现利用第二个BatchNorm来做筛选的时候，跟他的Neuron有直接关係的是该层的Depthwise&Pointwise以及上层的Pointwise。\n",
    "因此再做neuron筛选时记得要将这四个(包括自己, bn)也要同时prune掉。\n",
    "\n",
    "* 在Design Architecure内，model的一个block，名称所对应的Weight；\n",
    "\n",
    "|#|name|meaning|code|weight shape|\n",
    "|-|-|-|-|-|\n",
    "|0|cnn.{i}.0|Depthwise Convolution Layer|nn.Conv2d(x, x, 3, 1, 1, group=x)|(x, 1, 3, 3)|\n",
    "|1|cnn.{i}.1|Batch Normalization|nn.BatchNorm2d(x)|(x)|\n",
    "|2||ReLU6|nn.ReLU6||\n",
    "|3|cnn.{i}.3|Pointwise Convolution Layer|nn.Conv2d(x, y, 1),|(y, x, 1, 1)|\n",
    "|4||MaxPooling|nn.MaxPool2d(2, 2, 0)||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def network_slimming(old_model, new_model):\n",
    "    params = old_model.state_dict()\n",
    "    new_params = new_model.state_dict()\n",
    "\n",
    "    # selected_idx: 每一层所选择的neuron index\n",
    "    selected_idx = []\n",
    "    # 我们总共有7层CNN，因此逐一抓取选择的neuron index们。\n",
    "    for i in range(8):\n",
    "        # 根据上表，我们要抓的gamma系数在cnn.{i}.1.weight内。\n",
    "        importance = params[f'cnn.{i}.1.weight']\n",
    "        # 抓取总共要筛选几个neuron。\n",
    "        old_dim = len(importance)\n",
    "        new_dim = len(new_params[f'cnn.{i}.1.weight'])\n",
    "        # 以Ranking做Index排序，较大的会在前面(descending=True)。\n",
    "        ranking = paddle.argsort(importance, descending=True)\n",
    "        # 把筛选结果放入selected_idx中。\n",
    "        selected_idx.append(ranking[:new_dim])\n",
    "\n",
    "    now_processed = 1\n",
    "    for (name, p1), (name2, p2) in zip(params.items(), new_params.items()):\n",
    "        # 如果是cnn层，则移植参数。\n",
    "        # 如果是FC层，或是该参数只有一个数字(例如batchnorm的tracenum等等资讯)，那麽就直接複製。\n",
    "        if name.startswith('cnn') and p1.shape != np.array([]).shape and now_processed != len(selected_idx):\n",
    "            # 当处理到Pointwise的weight时，让now_processed+1，表示该层的移植已经完成。\n",
    "            if name.startswith(f'cnn.{now_processed}.3'):\n",
    "                now_processed += 1\n",
    "\n",
    "            # 如果是pointwise，weight会被上一层的pruning和下一层的pruning所影响，因此需要特判。\n",
    "            if name.endswith('3.weight'):\n",
    "                # 如果是最后一层cnn，则输出的neuron不需要prune掉。\n",
    "                if len(selected_idx) == now_processed:\n",
    "                    for idx, i in enumerate(selected_idx[now_processed - 1]):\n",
    "                        temp_dim = new_params[name][idx].shape\n",
    "                        new_params[name][idx].set_value(p1[int(i.numpy()[0])][:temp_dim[0]])\n",
    "                # 反之，就依照上层和下层所选择的index进行移植。\n",
    "                else:\n",
    "                    for idx, i in enumerate(selected_idx[now_processed]):\n",
    "                        temp_dim = new_params[name][idx].shape\n",
    "                        new_params[name][idx].set_value(p1[int(i.numpy()[0])][:temp_dim[0]])\n",
    "            else:\n",
    "                for idx, i in enumerate(selected_idx[now_processed]):\n",
    "                    new_params[name][idx].set_value(p1[int(i.numpy()[0])])\n",
    "        else:\n",
    "            new_params[name] = p1\n",
    "\n",
    "    # 让新model load进被我们筛选过的parameters，并回传new_model。\n",
    "    new_model.set_dict(new_params)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据处理\n",
    "\n",
    "我们的Dataset使用的是跟Hw3 - CNN同样的Dataset，因此这个区块的Augmentation / Read Image大家参考就好。\n",
    "\n",
    "如果有不会的话可以回去看Hw3的colab。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import paddle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import paddle.vision.transforms as transforms\n",
    "\n",
    "class MyDataset(paddle.io.Dataset):\n",
    "\n",
    "    def __init__(self, folderName, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "        for img_path in sorted(glob(folderName + '/*.jpg')):\n",
    "            try:\n",
    "                # Get classIdx by parsing image path\n",
    "                class_idx = int(os.path.split(img_path)[-1].split(\"_\")[0])\n",
    "            except:\n",
    "                # if inference mode (there's no answer), class_idx default 0\n",
    "                class_idx = 0\n",
    " \n",
    "            image = Image.open(img_path)\n",
    "            # Get File Descriptor\n",
    "            image_fp = image.fp\n",
    "            image.load()\n",
    "            # Close File Descriptor (or it'll reach OPEN_MAX)\n",
    "            image_fp.close()\n",
    "\n",
    "            self.data.append(image)\n",
    "            self.label.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, paddle.Tensor):\n",
    "            idx = idx.tolist()\n",
    "        image = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.label[idx]\n",
    "\n",
    "\n",
    "trainTransform = transforms.Compose([\n",
    "    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "testTransform = transforms.Compose([\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def get_dataloader(mode='training', batch_size=32):\n",
    "\n",
    "    assert mode in ['training', 'testing', 'validation']\n",
    "\n",
    "    dataset = MyDataset(\n",
    "        os.path.join(\"/home/aistudio/work/food-11\", f'{mode}'),\n",
    "        transform=trainTransform if mode == 'training' else testTransform)\n",
    "\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(mode == 'training'))\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 预处理\n",
    "\n",
    " 我们已经提供原始小model binary，架构是hw7_Architecture_Design.ipynb中的StudentNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get dataloader\n",
    "train_dataloader = get_dataloader('training', batch_size=32)\n",
    "valid_dataloader = get_dataloader('validation', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StudentNet(paddle.nn.Layer):\n",
    "    '''\n",
    "      在这个网络里，我们会使用Depthwise & Pointwise Convolution Layer来做model。\n",
    "      你会发现，将原本的Convolution Layer换成Dw & Pw后，Accuracy通常不会降很多。\n",
    "\n",
    "      另外，取名为StudentNet是因为这个Model等会要做Knowledge Distillation。\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base=16, width_mult=1):\n",
    "        '''\n",
    "          参数:\n",
    "            base: 这个model一开始的通道数量，每过一层都会*2，直到base*16为止。\n",
    "            width_mult: 为了之后的Network Pruning使用，在base*8 chs的Layer上会 * width_mult代表剪枝后的通道数量。\n",
    "        '''\n",
    "        super(StudentNet, self).__init__()\n",
    "        multiplier = [1, 2, 4, 8, 16, 16, 16, 16]\n",
    "\n",
    "        bandwidth = [base * m for m in multiplier]\n",
    "\n",
    "        for i in range(3, 7):\n",
    "            bandwidth[i] = int(bandwidth[i] * width_mult)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "\n",
    "                nn.Conv2D(3, bandwidth[0], 3, 1, 1),\n",
    "                nn.BatchNorm2D(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.MaxPool2D(2, 2, 0),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                \n",
    "                nn.Conv2D(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n",
    "                nn.BatchNorm2D(bandwidth[0]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[0], bandwidth[1], 1),\n",
    "                nn.MaxPool2D(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n",
    "                nn.BatchNorm2D(bandwidth[1]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[1], bandwidth[2], 1),\n",
    "                nn.MaxPool2D(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n",
    "                nn.BatchNorm2D(bandwidth[2]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[2], bandwidth[3], 1),\n",
    "                nn.MaxPool2D(2, 2, 0),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n",
    "                nn.BatchNorm2D(bandwidth[3]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[3], bandwidth[4], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n",
    "                nn.BatchNorm2D(bandwidth[4]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[4], bandwidth[5], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n",
    "                nn.BatchNorm2D(bandwidth[5]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[5], bandwidth[6], 1),\n",
    "            ),\n",
    "\n",
    "            nn.Sequential(\n",
    "                nn.Conv2D(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n",
    "                nn.BatchNorm2D(bandwidth[6]),\n",
    "                nn.ReLU6(),\n",
    "                nn.Conv2D(bandwidth[6], bandwidth[7], 1),\n",
    "            ),\n",
    "\n",
    "            nn.AdaptiveAvgPool2D((1, 1)),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(bandwidth[7], 11),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.reshape((out.shape[0], -1))\n",
    "        return self.fc(out)\n",
    "\n",
    "net = StudentNet()\n",
    "stu_layer_state_dict = paddle.load(\n",
    "    \"/home/aistudio/work/stu_net.pdparams\")\n",
    "net.set_dict(stu_layer_state_dict)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(learning_rate=1e-3, parameters=net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 开始训练\n",
    "\n",
    "* 每次Prune rate是0.95，Prune完后会重新fine-tune 3 epochs。\n",
    "* 其余的步骤与你在做Hw3 - CNN的时候一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(dataloader, update=True, alpha=0.5):\n",
    "    total_num, total_hit, total_loss = 0, 0, 0\n",
    "    for now_step, batch_data in enumerate(dataloader):\n",
    "        # 清空 optimizer\n",
    "        optimizer.clear_gradients()\n",
    "        # 处理 input\n",
    "        inputs, labels = batch_data\n",
    "  \n",
    "        logits = net(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        if update:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_hit += (paddle.argmax(logits, axis=-1).numpy() == labels.numpy).sum()\n",
    "        total_num += len(inputs)\n",
    "        total_loss += loss.numpy()[0] * len(inputs)\n",
    "\n",
    "    return total_loss / total_num, total_hit / total_num\n",
    "\n",
    "now_width_mult = 1\n",
    "for i in range(5):\n",
    "    now_width_mult *= 0.95\n",
    "    new_net = StudentNet(width_mult=now_width_mult)\n",
    "    params = net.state_dict()\n",
    "    net = network_slimming(net, new_net)\n",
    "    now_best_acc = 0\n",
    "    for epoch in range(5):\n",
    "        net.train()\n",
    "        train_loss, train_acc = run_epoch(train_dataloader, update=True)\n",
    "        net.eval()\n",
    "        valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n",
    "        # 在每个width_mult的情况下，存下最好的model。\n",
    "        if valid_acc > now_best_acc:\n",
    "            now_best_acc = valid_acc\n",
    "            paddle.save(net.state_dict(), f'custom_small_rate_{now_width_mult}.pdparams')\n",
    "        print('rate {:6.4f} epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(now_width_mult, \n",
    "            epoch, train_loss, train_acc, valid_loss, valid_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
