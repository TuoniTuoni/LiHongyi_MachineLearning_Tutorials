{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目4-语句分类\n",
    "\n",
    "## 友情提示\n",
    "同学们可以前往课程作业区先行动手尝试！！！\n",
    "\n",
    "## 项目描述\n",
    "* 本次作业是要让同学接触自然语言处理当中一个简单的任务 —— 语句分类（文本分类）\n",
    "* 给定一个语句，判断他有没有恶意（负面标 1，正面标 0）\n",
    "\n",
    "## 数据集介绍\n",
    "有三个文件，分别是 training_label.txt、training_nolabel.txt、testing_data.txt\n",
    "\n",
    "- training_label.txt：有标签的训练数据（句子配上 0 or 1，+++$+++ 只是分隔符号，不要理它）\n",
    "    - e.g., 1 +++$+++ are wtf ... awww thanks !\n",
    "\n",
    "- training_nolabel.txt：没有标签的训练数据（只有句子），用来做半监督学习\n",
    "    - ex: hates being this burnt !! ouch\n",
    "\n",
    "- testing_data.txt：你要判断测试数据里面的句子是 0 or 1\n",
    "\n",
    "    >id,text\n",
    "\n",
    "    >0,my dog ate our dinner . no , seriously ... he ate it .\n",
    "\n",
    "    >1,omg last day sooon n of primary noooooo x im gona be swimming out of school wif the amount of tears am gona cry\n",
    "\n",
    "    >2,stupid boys .. they ' re so .. stupid !\n",
    "\n",
    "## 项目要求\n",
    "* 用一些方法 pretrain 出 word embedding (e.g., skip-gram, CBOW. )\n",
    "* 请使用 RNN 实现文本分类\n",
    "* 不能使用额外 data (禁止使用其他 corpus 或 pretrained model)\n",
    "\n",
    "## 数据准备\n",
    "无\n",
    "\n",
    "## 环境配置/安装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "path_prefix = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 用来过滤警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/pandas/core/tools/datetimes.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "# utils.py\n",
    "# 这个块用来先定义一些等等常用到的函数\n",
    "import paddle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "paddle.disable_static()\n",
    "\n",
    "def load_training_data(path='training_label.txt'):\n",
    "    # 把训练时需要的数据读进来\n",
    "    # 如果是 'training_label.txt'，需要读取标签，如果是 'training_nolabel.txt'，不需要读取标签\n",
    "    if 'training_label' in path:\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
    "        x = [line[2:] for line in lines]\n",
    "        y = [line[0] for line in lines]\n",
    "        return x, y\n",
    "    else:\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n').split(' ') for line in lines]\n",
    "        return x\n",
    "\n",
    "def load_testing_data(path='testing_data'):\n",
    "    # 把测试时需要的数据读进来\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
    "        X = [sen.split(' ') for sen in X]\n",
    "    print(\"X\", X)\n",
    "    return X\n",
    "\n",
    "def evaluation(outputs, labels):\n",
    "    # outputs => probability (float)\n",
    "    # labels => labels\n",
    "\n",
    "    outputs = paddle.to_tensor([1.0 if element>=0.5 else 0.0 for element in outputs])\n",
    "    labels = labels.squeeze(1)\n",
    "    correct = paddle.sum(paddle.cast(paddle.equal(outputs, labels), dtype=\"int64\")).numpy()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "# w2v.py\n",
    "# 这个块是用来训练 word to vector 的词向量\n",
    "# 注意！这个块在训练 word to vector 时是用 cpu，可能要花到 10 分钟以上\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec(x):\n",
    "    # 训练word to vector的词向量\n",
    "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n",
    "    return model\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"loading training data ...\")\n",
    "#     train_x, y = load_training_data('rnn_data/training_label.txt')\n",
    "#     train_x_no_label = load_training_data('rnn_data/training_nolabel.txt')\n",
    "\n",
    "#     print(\"loading testing data ...\")\n",
    "#     test_x = load_testing_data('rnn_data/testing_data.txt')\n",
    "\n",
    "#     #model = train_word2vec(train_x + train_x_no_label + test_x)\n",
    "#     model = train_word2vec(train_x + test_x)\n",
    "    \n",
    "#     print(\"saving model ...\")\n",
    "#     # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n",
    "#     model.save(os.path.join(path_prefix, 'w2v_all.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "# 这个块用来做数据的预处理\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "    def get_w2v_model(self):\n",
    "        # 把之前训练好的 word to vec 模型读进来\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "    def add_embedding(self, word):\n",
    "        # 把词加进 embedding，并赋予他一个随机生成的表示向量\n",
    "        # 词只会是 \"<PAD>\" 或 \"<UNK>\"\n",
    "        vector = np.random.uniform(size=(1, self.embedding_dim))\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = np.concatenate([self.embedding_matrix, vector], 0)\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        # 取得训练好的 Word2vec词向量\n",
    "        if load:\n",
    "            print(\"loading word to vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # 制作一个 word2idx 的 字典\n",
    "        # 制作一个 idx2word 的 列表\n",
    "        # 制作一个 word2vector 的 列表\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            # print('get words #{}'.format(i+1), end='\\r')\n",
    "            #e.g. self.word2index['he'] = 1 \n",
    "            #e.g. self.index2word[1] = 'he'\n",
    "            #e.g. self.vectors[1] = 'he' vector\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        # print('')\n",
    "#        self.embedding_matrix = paddle.to_tensor(self.embedding_matrix)\n",
    "        self.embedding_matrix = np.array(self.embedding_matrix)\n",
    "        # 将 \"<PAD>\" 跟 \"<UNK>\" 加进 embedding 里面\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        # print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        self.embedding_matrix = self.embedding_matrix.astype(np.float32)\n",
    "        return self.embedding_matrix\n",
    "    def pad_sequence(self, sentence):\n",
    "        # 将每个句子变成一样的长度\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    def sentence_word2idx(self):\n",
    "        # 把句子里面的字转成相对应的索引\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            # print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            # 将每个句子变成一样的长度\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return paddle.to_tensor(sentence_list)\n",
    "    \n",
    "    def labels_to_tensor(self, y):\n",
    "        # 把标签转成张量\n",
    "        y = [float(label) for label in y]\n",
    "        return paddle.to_tensor(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data.py\n",
    "# 实现了dataset所需要的 '__init__', '__getitem__', '__len__'\n",
    "# 好让 dataloader 能使用\n",
    "import paddle\n",
    "from paddle.io import Dataset\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    Data can be a list of numpy array or a list of lists\n",
    "    input data shape : (data_num, seq_len, feature_dim)\n",
    "    __len__ will return the number of data\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: \n",
    "            return self.data[idx], paddle.to_tensor([1.])\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "# 这个块是要拿来训练的模型\n",
    "\n",
    "import paddle.nn as nn\n",
    "\n",
    "class LSTM_Net(nn.Layer):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # 制作 embedding layer\n",
    "        # self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        # self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "#         if fix_embedding:\n",
    "#             w_param_attrs = paddle.ParamAttr(trainable=False)\n",
    "#         else:\n",
    "#             w_param_attrs = paddle.ParamAttr(trainable=True)\n",
    "#         self.embedding = nn.Embedding((embedding.shape[0],embedding.shape[1]), param_attr= w_param_attrs)\n",
    "        self.embedding = nn.Embedding(embedding.shape[0],embedding.shape[1], sparse=True)\n",
    "        self.embedding.weight.set_value(embedding)\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout),\n",
    "                                         nn.Linear(hidden_dim, 1),\n",
    "                                         nn.Sigmoid() )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "#         print(\"embedding\",inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 维度 (batch, seq_len, hidden_size)\n",
    "        # 取用 LSTM 最后一层的隐藏状态\n",
    "        x = x[:, -1, :] \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "# 这个块是用来训练模型的\n",
    "\n",
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model):\n",
    "    model.train() # 将模型的模式设为 train，这样优化器就可以更新模型的参数\n",
    "    criterion = paddle.nn.loss.BCELoss() # 定义损失函数，这裡我们使用 二元交叉熵损失\n",
    "    t_batch = len(train) \n",
    "    v_batch = len(valid) \n",
    "    optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters()) # 将模型的参数给优化器，并给予适当的学习率\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        # 这段做训练\n",
    "        for i, (inputs, labels) in enumerate(train):\n",
    "            optimizer.clear_grad() # 由于 loss.backward() 的梯度会累加，所以每次喂完一个 batch 后需要归零\n",
    "            outputs = model(inputs) # 将输入喂给模型\n",
    "            loss = criterion(outputs, labels) # 计算此时模型的训练损失\n",
    "            loss.backward() # 算损失的梯度\n",
    "            optimizer.step() # 更新训练模型的参数\n",
    "            correct = evaluation(outputs, labels) # 计算此时模型的训练准确率\n",
    "\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.numpy()\n",
    "            \n",
    "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "            \tepoch+1, i+1, t_batch, loss.numpy()[0], correct[0]*100/batch_size), end='\\r')\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss[0]/t_batch, total_acc[0]/t_batch*100))\n",
    "\n",
    "        # 这段做验证\n",
    "        model.eval() # 将模型的模式设为eval，这样模型的参数就会固定住\n",
    "#         with torch.no_grad():\n",
    "        total_loss, total_acc = 0, 0\n",
    "        for i, (inputs, labels) in enumerate(valid):\n",
    "            outputs = model(inputs) # 将输入喂给模型\n",
    "\n",
    "            loss = criterion(outputs, labels) # 计算此时模型的验证损失\n",
    "            correct = evaluation(outputs, labels) # 计算此时模型的验证准确率\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.numpy()\n",
    "\n",
    "        print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss[0]/v_batch, total_acc[0]/v_batch*100))\n",
    "        if total_acc > best_acc:\n",
    "            # 如果验证的结果优于之前所有的结果，就把当下的模型存下来以备之后做预测时使用\n",
    "            best_acc = total_acc\n",
    "            #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
    "            paddle.save(model.state_dict(), \"lstm_crf.pdparams\")\n",
    "            paddle.save(optimizer.state_dict(),  \"lstm_crf.pdopt\")\n",
    "            print('saving model with acc {:.3f}'.format(total_acc[0]/v_batch*100))\n",
    "        print('-----------------------------------------------')\n",
    "        model.train() # 将模型的模式设为 train，这样优化器就可以更新模型的参数（因为刚刚转成 eval 模式）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testing(batch_size, test_loader, model):\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    with paddle.no_grad():\n",
    "        for i, (inputs,labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            outputs = [1 if element>=0.5 else 0 for element in outputs]\n",
    "            ret_output += outputs\n",
    "    return ret_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch1: 4/1407 ] loss:0.712 acc:53.125 \n",
      "[ Epoch1: 29/1407 ] loss:0.706 acc:53.125 \r"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from paddle.io import BatchSampler, DataLoader\n",
    "\n",
    "\n",
    "# 处理好各个数据的路径\n",
    "train_with_label = os.path.join(path_prefix, 'work/data/training_label.txt')\n",
    "train_no_label = os.path.join(path_prefix, 'work/data/training_nolabel.txt')\n",
    "testing_data = os.path.join(path_prefix, 'work/data/testing_data.txt')\n",
    "\n",
    "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 处理 word to vec model 的路径\n",
    "\n",
    "# 定义句子长度、要不要固定 embedding、批次大小、要训练几个 epoch、学习率的值、模型的资料夹路径\n",
    "sen_len = 20\n",
    "fix_embedding = True # 保持训练时的嵌入不变\n",
    "batch_size = 128\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "# model_dir = os.path.join(path_prefix, 'model/') # 检查点模型的模型目录\n",
    "model_dir = path_prefix #  检查点模型的模型目录\n",
    "\n",
    "print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 读进来\n",
    "train_x, y = load_training_data(train_with_label)\n",
    "train_x_no_label = load_training_data(train_no_label)\n",
    "\n",
    "# 对 输入 跟 标签 做预处理\n",
    "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "# 制作一个模型的对象\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
    "\n",
    "# 把 数据 分为 训练数据 跟 验证数据\n",
    "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n",
    "\n",
    "# 把 数据 做成 dataset 供 dataloader 取用\n",
    "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
    "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
    "\n",
    "# 把 数据 转成 batch of tensors\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            places = paddle.CPUPlace())\n",
    "\n",
    "val_loader = DataLoader(dataset = val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            places = paddle.CPUPlace())\n",
    "\n",
    "# 开始训练\n",
    "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Predict and Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开始测试模型并做预测\n",
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data(testing_data)\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "# print(\"test_x\", test_x[0])\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "# print(\"test_dataset\", test_dataset[0])\n",
    "test_loader = DataLoader(dataset = test_dataset,batch_size = batch_size,shuffle = False,\n",
    "                                            places = paddle.CPUPlace())\n",
    "\n",
    "print('\\nload model ...')\n",
    "\n",
    "param_state_dict = paddle.load(\"lstm_crf.pdparams\")\n",
    "opt_state_dict = paddle.load(\"lstm_crf.pdopt\")\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
    "model.set_state_dict(param_state_dict)\n",
    "model.set_state_dict(opt_state_dict)\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters()) # 将模型的参数给优化器，并给予适当的学习率\n",
    "optimizer.set_state_dict(opt_state_dict)\n",
    "\n",
    "outputs = testing(batch_size, test_loader, model)\n",
    "\n",
    "# 写到 csv 档案供上传 Kaggle\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
    "print(\"Finish Predicting\")\n",
    "\n",
    "# 以下是使用命令行上传到 Kaggle 的方式\n",
    "# 需要先 pip install kaggle、Create API Token，详细请看 https://github.com/Kaggle/kaggle-api 以及 https://www.kaggle.com/code1110/how-to-submit-from-google-colab\n",
    "# kaggle competitions submit [competition-name] -f [csv file path]] -m [message]\n",
    "# e.g., kaggle competitions submit ml-2020spring-hw4 -f output/predict.csv -m \"......\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 检查文件在哪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
