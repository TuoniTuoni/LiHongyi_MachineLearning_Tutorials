{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 项目4-序列到序列\n",
    "\n",
    "## 友情提示\n",
    "同学们可以前往课程作业区先行动手尝试！！！\n",
    "\n",
    "## 项目描述\n",
    "- 英文翻译中文\n",
    "  - 输入： 一句英文 （e.g.\t\ttom is a student .） \n",
    "  - 输出： 中文翻译 （e.g. \t\t汤姆 是 个 学生 。）\n",
    "\n",
    "## 数据集介绍\n",
    "- Data (出自manythings 的 cmn-eng):\n",
    "  - 训练资料：18000句\n",
    "  - 检验资料：  500句\n",
    "  - 测试资料： 2636句\n",
    "- Format:\n",
    "  - 不同语言的句子用 TAB ('\\t') 分开\n",
    "  - 字跟字之间用空白分开\n",
    "\n",
    "## 项目要求\n",
    "  - 实现seq2seq\n",
    "  - Teachering Forcing 的功用: 尝试不用 Teachering Forcing 做训练\n",
    "  - 实现 Attention Mechanism\n",
    "  - 实现 Beam Search\n",
    "  - 实现 Schedule Sampling\n",
    "\n",
    "## 数据准备\n",
    "已经下好\n",
    "\n",
    "## 环境配置/安装\n",
    "\n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 序列到序列介绍\n",
    "- 大多数常见的 **sequence-to-sequence (seq2seq) model** 为 **encoder-decoder model**，主要由两个部分组成，分别是 **Encoder** 和 **Decoder**，而这两个部分则大多使用 **recurrent neural network (RNN)** 来实作，主要是用来解决输入和输出的长度不一样的情况\n",
    "- **Encoder** 是将**一连串**的输入，如文字、影片、声音讯号等，编码为**单个向量**，这单个向量可以想像为是整个输入的抽象表示，包含了整个输入的资讯\n",
    "- **Decoder** 是将 Encoder 输出的单个向量逐步解码，**一次输出一个结果**，直到将最后目标输出被产生出来为止，每次输出会影响下一次的输出，一般会在开头加入 \"< BOS >\" 来表示开始解码，会在结尾输出 \"< EOS >\" 来表示输出结束\n",
    "\n",
    "\n",
    "![seq2seq](https://ai-studio-static-online.cdn.bcebos.com/2a6aa43ecef24003a564104362d3294bc25fa6cc7d314496aa80d405ed920d0c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 下载和引入需要的 libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.optimizer as optim\n",
    "from paddle.io import Dataset, DataLoader\n",
    "paddle.disable_static()\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 资料结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义资料的转换\n",
    "- 将不同长度的答案拓展到相同长度，以便训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LabelTransform(object):\n",
    "    def __init__(self, size, pad):\n",
    "        self.size = size\n",
    "        self.pad = pad\n",
    "\n",
    "    def __call__(self, label):\n",
    "        label = np.pad(label, (0, (self.size - label.shape[0])), mode='constant', constant_values=self.pad)\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义 Dataset\n",
    "- Data (出自manythings 的 cmn-eng):\n",
    "  - 训练资料：18000句\n",
    "  - 检验资料：  500句\n",
    "  - 测试资料： 2636句\n",
    "\n",
    "- 资料预处理:\n",
    "  - 英文：\n",
    "    - 用 subword-nmt 套件将word转为subword\n",
    "    - 建立字典：取出标签中出现频率高于定值的subword\n",
    "  - 中文：\n",
    "    - 用 jieba 将中文句子断词\n",
    "    - 建立字典：取出标签中出现频率高于定值的词\n",
    "  - 特殊字元： < PAD >, < BOS >, < EOS >, < UNK > \n",
    "    - < PAD >  ：无意义，将句子拓展到相同长度\n",
    "    - < BOS >  ：Begin of sentence, 开始字元\n",
    "    - < EOS >  ：End of sentence, 结尾字元\n",
    "    - < UNK > ：单字没有出现在字典裡的字\n",
    "  - 将字典里每个 subword (词) 用一个整数表示，分为英文和中文的字典，方便之后转为 one-hot vector   \n",
    "\n",
    "- 处理后的档案:\n",
    "  - 字典：\n",
    "    - int2word_*.json: 将整数转为文字\n",
    "    ![int2word_en.json](https://ai-studio-static-online.cdn.bcebos.com/ca259b973e0046bb88c50cd8e1e350b5af1ee2ed6833491c82ecce84f234467d)\n",
    "    - word2int_*.json: 将文字转为整数\n",
    "    ![word2int_en.json](https://ai-studio-static-online.cdn.bcebos.com/2f745b301c354acdab93c5425dbd16457f426dac676a4091b8e4252f2fb5fdd7)\n",
    "    - $*$ 分为英文（en）和中文（cn）\n",
    "  \n",
    "  - 训练资料:\n",
    "    - 不同语言的句子用 TAB ('\\t') 分开\n",
    "    - 字跟字之间用空白分开\n",
    "    ![data](https://ai-studio-static-online.cdn.bcebos.com/cfe51aeb3f51463eb60fed6d78983a439664d4baf9b94e529a5418e516c03c57)\n",
    "    \n",
    "\n",
    "\n",
    "- 在将答案传出去前，在答案开头加入 \"< BOS >\" 符号，并于答案结尾加入 \"< EOS >\" 符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "class EN2CNDataset(Dataset):\n",
    "    def __init__(self, root, max_output_len, set_name):\n",
    "        self.root = root\n",
    "\n",
    "        self.word2int_cn, self.int2word_cn = self.get_dictionary('cn')\n",
    "        self.word2int_en, self.int2word_en = self.get_dictionary('en')\n",
    "\n",
    "        # 载入资料\n",
    "        self.data = []\n",
    "        with open(os.path.join(self.root, f'{set_name}.txt'), \"r\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(line)\n",
    "        print (f'{set_name} dataset size: {len(self.data)}')\n",
    "\n",
    "        self.cn_vocab_size = len(self.word2int_cn)\n",
    "        self.en_vocab_size = len(self.word2int_en)\n",
    "        self.transform = LabelTransform(max_output_len, self.word2int_en['<PAD>'])\n",
    "\n",
    "    def get_dictionary(self, language):\n",
    "        # 载入字典\n",
    "        with open(os.path.join(self.root, f'word2int_{language}.json'), \"r\") as f:\n",
    "            word2int = json.load(f)\n",
    "        with open(os.path.join(self.root, f'int2word_{language}.json'), \"r\") as f:\n",
    "            int2word = json.load(f)\n",
    "        return word2int, int2word\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, Index):\n",
    "        # 先将中英文分开\n",
    "        sentences = self.data[Index]\n",
    "        sentences = re.split('[\\t\\n]', sentences)\n",
    "        sentences = list(filter(None, sentences))\n",
    "        #print (sentences)\n",
    "        assert len(sentences) == 2\n",
    "\n",
    "        # 预备特殊字符\n",
    "        BOS = self.word2int_en['<BOS>']\n",
    "        EOS = self.word2int_en['<EOS>']\n",
    "        UNK = self.word2int_en['<UNK>']\n",
    "\n",
    "        # 在开头添加 <BOS>，在结尾添加 <EOS> ，不在字典的 subword (词) 用 <UNK> 取代\n",
    "        en, cn = [BOS], [BOS]\n",
    "        # 将句子拆解为 subword 并转为整数\n",
    "        sentence = re.split(' ', sentences[0])\n",
    "        sentence = list(filter(None, sentence))\n",
    "        #print (f'en: {sentence}')\n",
    "        for word in sentence:\n",
    "            en.append(self.word2int_en.get(word, UNK))\n",
    "        en.append(EOS)\n",
    "\n",
    "        # 将句子拆解为单词并转为整数\n",
    "        # e.g. < BOS >, we, are, friends, < EOS > --> 1, 28, 29, 205, 2\n",
    "        sentence = re.split(' ', sentences[1])\n",
    "        sentence = list(filter(None, sentence))\n",
    "        #print (f'cn: {sentence}')\n",
    "        for word in sentence:\n",
    "            cn.append(self.word2int_cn.get(word, UNK))\n",
    "        cn.append(EOS)\n",
    "\n",
    "        en, cn = np.asarray(en), np.asarray(cn)\n",
    "\n",
    "        # 用 <PAD> 将句子补到相同长度\n",
    "        en, cn = self.transform(en), self.transform(cn)\n",
    "        en, cn = paddle.to_tensor(en), paddle.to_tensor(cn)\n",
    "\n",
    "        return en, cn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Encoder\n",
    "- seq2seq模型的编码器为RNN。 对于每个输入，，**Encoder** 会输出**一个向量**和**一个隐藏状态(hidden state)**，并将隐藏状态用于下一个输入，换句话说，**Encoder** 会逐步读取输入序列，并输出单个矢量（最终隐藏状态）\n",
    "- 参数:\n",
    "  - en_vocab_size 是英文字典的大小，也就是英文的 subword 的个数\n",
    "  - emb_dim 是 embedding 的维度，主要将 one-hot vector 的单词向量压缩到指定的维度，主要是为了降维和浓缩资讯的功用，可以使用预先训练好的 word embedding，如 Glove 和 word2vector\n",
    "  - hid_dim 是 RNN 输出和隐藏状态的维度\n",
    "  - n_layers 是 RNN 要叠多少层\n",
    "  - dropout 是决定有多少的机率会将某个节点变为 0，主要是为了防止 overfitting ，一般来说是在训练时使用，测试时则不使用\n",
    "- Encoder 的输入和输出:\n",
    "  - 输入: \n",
    "    - 英文的整数序列 e.g. 1, 28, 29, 205, 2\n",
    "  - 输出: \n",
    "    - outputs: 最上层 RNN 全部的输出，可以用 Attention 再进行处理\n",
    "    - hidden: 每层最后的隐藏状态，将传递到 Decoder 进行解码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Layer):\n",
    "    def __init__(self, en_vocab_size, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(en_vocab_size, emb_dim, sparse=True)\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, direction=\"bidirectional\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input = [batch size, sequence len, vocab size]\n",
    "        embedding = self.embedding(input)\n",
    "        outputs, hidden = self.rnn(self.dropout(embedding))\n",
    "        # outputs = [batch size, sequence len, hid dim * directions]\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
    "        # outputs 是最上层RNN的輸出\n",
    "\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decoder\n",
    "- **Decoder** 是另一个 RNN，在最简单的 seq2seq decoder 中，仅使用 **Encoder** 每一层最后的隐藏状态来进行解码，而这最后的隐藏状态有时被称为 “content vector”，因为可以想像它对整个前文序列进行编码， 此 “content vector” 用作 **Decoder** 的**初始**隐藏状态， 而 **Encoder** 的输出通常用于 Attention Mechanism\n",
    "- 参数\n",
    "  - en_vocab_size 是英文字典的大小，也就是英文的 subword 的个数\n",
    "  - emb_dim 是 embedding 的维度，是用来将 one-hot vector 的单词向量压缩到指定的维度，主要是为了降维和浓缩资讯的功用，可以使用预先训练好的 word embedding，如 Glove 和 word2vector\n",
    "  - hid_dim 是 RNN 输出和隐藏状态的维度\n",
    "  - output_dim 是最终输出的维度，一般来说是将 hid_dim 转到 one-hot vector 的单词向量\n",
    "  - n_layers 是 RNN 要叠多少层\n",
    "  - dropout 是决定有多少的机率会将某个节点变为0，主要是为了防止 overfitting ，一般来说是在训练时使用，测试时则不用\n",
    "  - isatt 是来决定是否使用 Attention Mechanism\n",
    "\n",
    "- Decoder 的输入和输出:\n",
    "  - 输入:\n",
    "    - 前一次解码出来的单词的整数表示\n",
    "  - 输出:\n",
    "    - hidden: 根据输入和前一次的隐藏状态，现在的隐藏状态更新的结果\n",
    "    - output: 每个字有多少机率是这次解码的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Layer):\n",
    "    def __init__(self, cn_vocab_size, emb_dim, hid_dim, n_layers, dropout, isatt):\n",
    "        super().__init__()\n",
    "        self.cn_vocab_size = cn_vocab_size\n",
    "        self.hid_dim = hid_dim * 2\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(cn_vocab_size, config.emb_dim)\n",
    "        self.isatt = isatt\n",
    "        self.attention = Attention(hid_dim)\n",
    "        # 如果使用 Attention Mechanism 会使得输入维度变化，请在这里修改\n",
    "        # e.g. Attention 接在输入后面会使得维度变化，所以输入维度改为\n",
    "        # self.input_dim = emb_dim + hid_dim * 2 if isatt else emb_dim\n",
    "        self.input_dim = emb_dim\n",
    "        self.rnn = nn.GRU(self.input_dim, self.hid_dim, self.n_layers, dropout = dropout)\n",
    "        self.embedding2vocab1 = nn.Linear(self.hid_dim, self.hid_dim * 2)\n",
    "        self.embedding2vocab2 = nn.Linear(self.hid_dim * 2, self.hid_dim * 4)\n",
    "        self.embedding2vocab3 = nn.Linear(self.hid_dim * 4, self.cn_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size, vocab size]\n",
    "        # hidden = [batch size, n layers * directions, hid dim]\n",
    "        # Decoder 只会是单向，所以 directions=1\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [batch size, 1, emb dim]\n",
    "        if self.isatt:\n",
    "            attn = self.attention(encoder_outputs, hidden)\n",
    "          # TODO: 在这里决定如何使用 Attention，e.g. 相加 或是 接在后面， 请注意维度变化\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output = [batch size, 1, hid dim]\n",
    "        # hidden = [num_layers, batch size, hid dim]\n",
    "\n",
    "        # 将 RNN 的输出转为每个词出现的机率\n",
    "        output = self.embedding2vocab1(output.squeeze(1))\n",
    "        output = self.embedding2vocab2(output)\n",
    "        prediction = self.embedding2vocab3(output)\n",
    "        # prediction = [batch size, vocab size]\n",
    "        return prediction, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Attention\n",
    "- 当输入过长，或是单独靠 “content vector” 无法取得整个输入的意思时，用 Attention Mechanism 来提供 **Decoder** 更多的信息\n",
    "- 主要是根据现在 **Decoder hidden state** ，去计算在 **Encoder outputs** 中，那些与其有较高的关系，根据关系的数值来决定该传给 **Decoder** 那些额外信息 \n",
    "- 常见 Attention 的实作是用 Neural Network / Dot Product 来算 **Decoder hidden state** 和 **Encoder outputs** 之间的关系，再对所有算出来的数值做 **softmax** ，最后根据过完 **softmax** 的值对 **Encoder outputs** 做 **weight sum**\n",
    "\n",
    "- TODO:\n",
    "实现 Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Layer):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        # encoder_outputs = [batch size, sequence len, hid dim * directions]\n",
    "        # decoder_hidden = [num_layers, batch size, hid dim]\n",
    "        # 一般来说是取 Encoder 最后一层的 hidden state 来做 attention\n",
    "        ########\n",
    "        # TODO #\n",
    "        ########\n",
    "        attention_energies = paddle.sum(decoder_hidden*encoder_outputs, dim=2)\n",
    "        attn_weights = nn.functional.softmax(attention_energies, dim=0)\n",
    "        print(attn_weights.size())\n",
    "        # attention=None\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Seq2Seq\n",
    "- 由 **Encoder** 和 **Decoder** 组成\n",
    "- 接收输入并传给 **Encoder** \n",
    "- 将 **Encoder** 的输出传给 **Decoder**\n",
    "- 不断地将 **Decoder** 的输出传回 **Decoder** ，进行解码  \n",
    "- 当解码完成后，将 **Decoder** 的输出传回 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Layer):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "                \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, input, target, teacher_forcing_ratio):\n",
    "        # input  = [batch size, input len, vocab size]\n",
    "        # target = [batch size, target len, vocab size]\n",
    "        # teacher_forcing_ratio 是有多少机率使用正确答案来训练\n",
    "        batch_size = target.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        vocab_size = self.decoder.cn_vocab_size\n",
    "\n",
    "        # 准备一个储存空间来储存输出\n",
    "        # outputs = paddle.zeros((batch_size, target_len, vocab_size))\n",
    "        # outputs = np.zeros((batch_size, target_len, vocab_size))\n",
    "        outputs = [paddle.zeros((batch_size, vocab_size))]\n",
    "\n",
    "        # 将输入放入 Encoder\n",
    "        encoder_outputs, hidden = self.encoder(input)\n",
    "        # Encoder 最后的隐藏层(hidden state) 用来初始化 Decoder\n",
    "        # encoder_outputs 主要是使用在 Attention\n",
    "        # encoder_outputs = [batch size, sequence len, hid dim * directions]\n",
    "        # 因为 Encoder 是双向的RNN，所以需要将同一层两个方向的 hidden state 接在一起\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\n",
    "        hidden = paddle.reshape(hidden, shape=[self.encoder.n_layers, 2, batch_size, -1])\n",
    "\n",
    "        hidden = paddle.concat((hidden[:, -2, :, :], hidden[:, -1, :, :]), axis=2)\n",
    "        # 取的 <BOS> token\n",
    "        input = target[:, 0]\n",
    "        preds = []\n",
    "        for t in range(1, target_len):\n",
    "            # input [16] hidden [3, 16, 1024] encoder_outputs [16, 50, 1024]\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            # output.size [16, 3805]  hidden size [3, 16, 1024]\n",
    "            # outputs[:, t, :] = output.numpy()\n",
    "            outputs.append(output)\n",
    "            # 决定是否用正确答案来做训练\n",
    "            teacher_force = random.random() <= teacher_forcing_ratio\n",
    "            # 取出机率最大的单词\n",
    "            top1 = output.argmax(1)\n",
    "            # 如果是 teacher force 则用正解训练，反之用自己预测的单词做预测\n",
    "            input = target[:, t] if teacher_force and t < target_len else top1\n",
    "            preds.append(top1.unsqueeze(1))\n",
    "        preds = paddle.concat(preds, 1)    \n",
    "        return outputs, preds\n",
    "\n",
    "    def inference(self, input, target):\n",
    "        ########\n",
    "        # TODO #\n",
    "        ########\n",
    "        # 在这里实施 Beam Search\n",
    "        # 此函式的 batch size = 1  \n",
    "        # input  = [batch size, input len, vocab size]\n",
    "        # target = [batch size, target len, vocab size]\n",
    "        batch_size = input.shape[0]\n",
    "        input_len = input.shape[1]        # 取得最大字数\n",
    "        vocab_size = self.decoder.cn_vocab_size\n",
    "\n",
    "        # 准备一个储存空间来储存输出\n",
    "        outputs = [paddle.zeros((batch_size, vocab_size))]\n",
    "        # 将输入放入 Encoder\n",
    "        encoder_outputs, hidden = self.encoder(input)\n",
    "        # Encoder 最后的隐藏层(hidden state) 用来初始化 Decoder\n",
    "        # encoder_outputs 主要是使用在 Attention\n",
    "        # 因为 Encoder 是双向的RNN，所以需要将同一层两个方向的 hidden state 接在一起\n",
    "        # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\n",
    "        # hidden = hidden.reshape(self.encoder.n_layers, 2, batch_size, -1)\n",
    "        hidden = paddle.reshape(hidden, [self.encoder.n_layers, 2, batch_size, -1])\n",
    "        hidden = paddle.concat((hidden[:, -2, :, :], hidden[:, -1, :, :]), axis=2)\n",
    "        # 取的 <BOS> token\n",
    "        input = target[:, 0]\n",
    "        preds = []\n",
    "        for t in range(1, input_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            # 将预测结果存起来\n",
    "            outputs.append(output)\n",
    "            # 取出机率最大的单词\n",
    "            top1 = output.argmax(1)\n",
    "            input = top1\n",
    "            preds.append(top1.unsqueeze(1))\n",
    "        preds = paddle.concat(preds, 1)\n",
    "        return outputs, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paddle.disable_static()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# utils\n",
    "- 基本操作:\n",
    "  - 储存模型\n",
    "  - 载入模型\n",
    "  - 建构模型\n",
    "  - 将一连串的数字还原回句子\n",
    "  - 计算 BLEU score\n",
    "  - 迭代 dataloader\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 储存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, store_model_path, step):\n",
    "    paddle.save(model.state_dict(), f'{store_model_path}/model_{step}.pdparams')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model(model, load_model_path):\n",
    "    print(f'Load model from {load_model_path}')\n",
    "    state_dict = paddle.load(f'{load_model_path}.pdparams')\n",
    "    model.set_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 建构模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(config, en_vocab_size, cn_vocab_size):\n",
    "    # 建构模型\n",
    "    encoder = Encoder(en_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout)\n",
    "    decoder = Decoder(cn_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout, config.attention)\n",
    "    model = Seq2Seq(encoder, decoder)\n",
    "    # 建构 optimizer\n",
    "    optimizer = optim.Adam(learning_rate=config.learning_rate, parameters=model.parameters())\n",
    "    # optimizer = optim.Adam(learning_rate=config.learning_rate, parameters=model.parameters())\n",
    "    if config.load_model:\n",
    "        model = load_model(model, config.load_model_path)\n",
    "\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数字转句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokens2sentence(outputs, int2word):\n",
    "    sentences = []\n",
    "    for tokens in outputs:\n",
    "        sentence = []\n",
    "        for token in tokens:\n",
    "            word = int2word[str(int(token))]\n",
    "            if word == '<EOS>':\n",
    "                break\n",
    "            sentence.append(word)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 计算 BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def computebleu(sentences, targets):\n",
    "    score = 0 \n",
    "    assert (len(sentences) == len(targets))\n",
    "\n",
    "    def cut_token(sentence):\n",
    "        tmp = []\n",
    "        for token in sentence:\n",
    "            if token == '<UNK>' or token.isdigit() or len(bytes(token[0], encoding='utf-8')) == 1:\n",
    "                tmp.append(token)\n",
    "            else:\n",
    "                tmp += [word for word in token]\n",
    "        return tmp \n",
    "\n",
    "    for sentence, target in zip(sentences, targets):\n",
    "        sentence = cut_token(sentence)\n",
    "        target = cut_token(target)\n",
    "        score += sentence_bleu([target], sentence, weights=(1, 0, 0, 0))                                                                                          \n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 迭代 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def infinite_iter(data_loader):\n",
    "    it = iter(data_loader)\n",
    "    while True:\n",
    "        try:\n",
    "            ret = next(it)\n",
    "            yield ret\n",
    "        except StopIteration:\n",
    "            it = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## schedule_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########\n",
    "# TODO #\n",
    "########\n",
    "\n",
    "# 请在这里直接 return 0 来取消 Teacher Forcing\n",
    "# 请在这里实现 schedule_sampling 的策略\n",
    "\n",
    "def schedule_sampling():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 训练步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练\n",
    "- 训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter, loss_function, total_steps, summary_steps, train_dataset):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for step in range(summary_steps):\n",
    "        loss = paddle.zeros([1])\n",
    "        # print(\"loss at first\", loss)\n",
    "\n",
    "        sources, targets = next(train_iter)\n",
    "        outputs, preds = model(sources, targets, schedule_sampling())\n",
    "        # targets 的第一个 token 是 <BOS> 所以忽略\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            if i>0:\n",
    "                step_loss = loss_function(output, targets[:, i].unsqueeze(1))\n",
    "                avg_step_loss = paddle.mean(step_loss)\n",
    "                loss += avg_step_loss\n",
    "\n",
    "        loss = loss / (targets.shape[1]-1)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if (step + 1) % 5 == 0:\n",
    "            print (\"\\r\", \"train [{}] loss: {:.3f}, Perplexity: {:.3f}      \".format(total_steps + step + 1, loss.numpy()[0], np.exp(loss.numpy()[0])), end=\" \")\n",
    "            losses.append(loss.numpy()[0])\n",
    "\n",
    "    return model, optimizer, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 检验/测试\n",
    "- 防止训练发生overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader, loss_function):\n",
    "    model.eval()\n",
    "#     loss_sum, bleu_score= 0.0, 0.0\n",
    "    bleu_score= 0.0\n",
    "    n = 0\n",
    "    result = []\n",
    "    for sources, targets in dataloader:\n",
    "        batch_size = sources.shape[0]\n",
    "\n",
    "        outputs, preds = model.inference(sources, targets)\n",
    "        # targets 的第一个 token 是 <BOS> 所以忽略\n",
    "        loss = paddle.zeros([1])\n",
    "        for i, output in enumerate(outputs):\n",
    "            if i>0:\n",
    "                step_loss = loss_function(output, targets[:, i].unsqueeze(1))\n",
    "                avg_step_loss = paddle.mean(step_loss)\n",
    "                loss += avg_step_loss\n",
    "        \n",
    "        loss = loss / (targets.shape[1]-1)\n",
    "\n",
    "        # 将预测结果转为文字\n",
    "        preds = tokens2sentence(preds, dataloader.dataset.int2word_cn)\n",
    "\n",
    "        sources = tokens2sentence(sources, dataloader.dataset.int2word_en)\n",
    "        targets = tokens2sentence(targets, dataloader.dataset.int2word_cn)\n",
    "\n",
    "        for source, pred, target in zip(sources, preds, targets):\n",
    "            result.append((source, pred, target))\n",
    "        # 计算 Bleu Score\n",
    "        bleu_score += computebleu(preds, targets)\n",
    "\n",
    "        n += batch_size\n",
    "\n",
    "    return loss.numpy()[0], bleu_score / n, result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练流程\n",
    "- 先训练，再检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_process(config):\n",
    "    # 准备训练资料\n",
    "    train_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'training')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, places=paddle.CPUPlace())\n",
    "    train_iter = infinite_iter(train_loader)\n",
    "    # 准备检验资料\n",
    "    val_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'validation')\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, places=paddle.CPUPlace())\n",
    "    # 建构模型\n",
    "    print(\"train_dataset.en_vocab_size\",train_dataset.en_vocab_size)\n",
    "    model, optimizer = build_model(config, train_dataset.en_vocab_size, train_dataset.cn_vocab_size)\n",
    "    loss_function = nn.loss.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "    \n",
    "    train_losses, val_losses, bleu_scores = [], [], []\n",
    "    total_steps = 0\n",
    "    while (total_steps < config.num_steps):\n",
    "        # 训练模型\n",
    "        model, optimizer, loss = train(model, optimizer, train_iter, loss_function, total_steps, config.summary_steps, train_dataset)\n",
    "        train_losses += loss\n",
    "        # 检验模型\n",
    "        val_loss, bleu_score, result = test(model, val_loader, loss_function)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        total_steps += config.summary_steps\n",
    "        print (\"\\r\", \"val [{}] loss: {:.3f}, Perplexity: {:.3f}, blue score: {:.3f}       \".format(total_steps, val_loss, np.exp(val_loss), bleu_score))\n",
    "\n",
    "        # 储存模型和结果\n",
    "        if total_steps % config.store_steps == 0 or total_steps >= config.num_steps:\n",
    "            save_model(model, optimizer, config.store_model_path, total_steps)\n",
    "            with open(f'{config.store_model_path}/output_{total_steps}.txt', 'w') as f:\n",
    "                for line in result:\n",
    "                    print (line, file=f)\n",
    "    \n",
    "    return train_losses, val_losses, bleu_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 测试流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_process(config):\n",
    "    # 准备测试资料\n",
    "    test_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'testing')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, places=paddle.CPUPlace())\n",
    "    # 建构模型\n",
    "    model, optimizer = build_model(config, test_dataset.en_vocab_size, test_dataset.cn_vocab_size)\n",
    "    print (\"Finish build model\")\n",
    "    loss_function = nn.loss.CrossEntropyLoss(ignore_index=0)\n",
    "    model.eval()\n",
    "    # 测试模型\n",
    "    test_loss, bleu_score, result = test(model, test_loader, loss_function)\n",
    "    # 储存结果\n",
    "    with open(f'{config.store_model_path}/test_output.txt', 'w') as f:\n",
    "        for line in result:\n",
    "            print (line, file=f)\n",
    "    return test_loss, bleu_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Config\n",
    "- 实验的参数设定表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class configurations(object):\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.emb_dim = 256\n",
    "        self.hid_dim = 512\n",
    "        self.n_layers = 3\n",
    "        self.dropout = 0.5\n",
    "        self.learning_rate = 0.0005\n",
    "        self.max_output_len = 50              # 最后输出句子的最大长度\n",
    "        self.num_steps = 12000                # 总训练次数\n",
    "        self.store_steps = 300                # 训练多少次后须储存模型\n",
    "        self.summary_steps = 300              # 训练多少次后须检验是否有overfitting\n",
    "        self.load_model = False               # 是否需载入模型\n",
    "        self.store_model_path = \"work/ckpt\"      # 储存模型的位置\n",
    "        self.load_model_path = \"work/ckpt/model_900\"           # 载入模型的位置 e.g. \"./ckpt/model_{step}\" \n",
    "        self.data_path = \"work/cmn-eng\"          # 资料存放的位置\n",
    "        self.attention = False                # 是否使用 Attention Mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Main Function\n",
    "- 读入参数\n",
    "- 进行训练或是推论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " {'batch_size': 128, 'emb_dim': 256, 'hid_dim': 512, 'n_layers': 3, 'dropout': 0.5, 'learning_rate': 0.0005, 'max_output_len': 50, 'num_steps': 12000, 'store_steps': 300, 'summary_steps': 300, 'load_model': False, 'store_model_path': 'work/ckpt', 'load_model_path': 'work/ckpt/model_900', 'data_path': 'work/cmn-eng', 'attention': False}\n",
      "training dataset size: 18000\n",
      "validation dataset size: 500\n",
      "train_dataset.en_vocab_size 3922\n",
      " train [300] loss: 1.117, Perplexity: 3.056       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val [300] loss: 1.696, Perplexity: 5.454, blue score: 0.202       \n",
      " val [600] loss: 1.519, Perplexity: 4.570, blue score: 0.255       \n",
      " val [900] loss: 1.316, Perplexity: 3.730, blue score: 0.256       \n",
      " val [1200] loss: 1.393, Perplexity: 4.028, blue score: 0.270       \n",
      " val [1500] loss: 1.247, Perplexity: 3.480, blue score: 0.300       \n",
      " val [1800] loss: 1.239, Perplexity: 3.453, blue score: 0.332       \n",
      " val [2100] loss: 1.344, Perplexity: 3.835, blue score: 0.352       \n",
      " train [2280] loss: 0.422, Perplexity: 1.524       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-89013f3d0791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'config:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-8661d6239595>\u001b[0m in \u001b[0;36mtrain_process\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# 训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 检验模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-4e380c038581>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_iter, loss_function, total_steps, summary_steps, train_dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedule_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# targets 的第一个 token 是 <BOS> 所以忽略\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-5dfeebc87dca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# 将输入放入 Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Encoder 最后的隐藏层(hidden state) 用来初始化 Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# encoder_outputs 主要是使用在 Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3b7decac7f87>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# input = [batch size, sequence len, vocab size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# outputs = [batch size, sequence len, hid dim * directions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# hidden =  [num_layers * directions, batch size  , hid dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, initial_states, sequence_length)\u001b[0m\n\u001b[1;32m   1136\u001b[0m                     \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m                     mode=\"upscale_in_train\")\n\u001b[0;32m-> 1138\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mfinal_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, initial_states, sequence_length, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m         outputs, final_states = paddle.fluid.layers.birnn(\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_bw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             self.time_major, **kwargs)\n\u001b[0m\u001b[1;32m    922\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/rnn.py\u001b[0m in \u001b[0;36mbirnn\u001b[0;34m(cell_fw, cell_bw, inputs, initial_states, sequence_length, time_major, **kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m                                 \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                                 \u001b[0mis_reverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                 **kwargs)\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     outputs = map_structure(lambda x, y: tensor.concat([x, y], -1), outputs_fw,\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/rnn.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(cell, inputs, initial_states, sequence_length, time_major, is_reverse, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0min_dygraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         return _rnn_dynamic_graph(cell, inputs, initial_states, sequence_length,\n\u001b[0;32m--> 497\u001b[0;31m                                   time_major, is_reverse, **kwargs)\n\u001b[0m\u001b[1;32m    498\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         return _rnn_static_graph(cell, inputs, initial_states, sequence_length,\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/rnn.py\u001b[0m in \u001b[0;36m_rnn_dynamic_graph\u001b[0;34m(cell, inputs, initial_states, sequence_length, time_major, is_reverse, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mstep_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mstep_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             new_states = map_structure(\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, states)\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0mh_gates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_gates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0mx_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_or_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0mh_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_gates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_or_sections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/manipulation.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(x, num_or_sections, axis, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \"\"\"\n\u001b[1;32m    495\u001b[0m     return paddle.fluid.layers.split(\n\u001b[0;32m--> 496\u001b[0;31m         input=x, num_or_sections=num_or_sections, dim=axis, name=name)\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/nn.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(input, num_or_sections, dim, name)\u001b[0m\n\u001b[1;32m   4888\u001b[0m                 \u001b[0;34m\"The type of 'num_or_sections' in split must be int, list or tuple in imperative mode, but \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4889\u001b[0m                 \"received %s.\" % (type(num_or_sections)))\n\u001b[0;32m-> 4890\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4892\u001b[0m     check_variable_and_dtype(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = configurations()\n",
    "    print ('config:\\n', vars(config))\n",
    "    train_losses, val_losses, bleu_scores = train_process(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " {'batch_size': 128, 'emb_dim': 256, 'hid_dim': 512, 'n_layers': 3, 'dropout': 0.5, 'learning_rate': 0.0005, 'max_output_len': 50, 'num_steps': 12000, 'store_steps': 300, 'summary_steps': 300, 'load_model': False, 'store_model_path': 'work/ckpt', 'load_model_path': 'work/ckpt/model_900', 'data_path': 'work/cmn-eng', 'attention': False}\n",
      "testing dataset size: 2636\n",
      "Finish build model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 1.850272297859192, bleu_score: 0.0024481339031657383\n"
     ]
    }
   ],
   "source": [
    "# 在执行 Test 之前，请先行至 config 设定所要载入的模型位置\n",
    "if __name__ == '__main__':\n",
    "    config = configurations()\n",
    "    print ('config:\\n', vars(config))\n",
    "    test_loss, bleu_score = test_process(config)\n",
    "    print (f'test loss: {test_loss}, bleu_score: {bleu_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 图形化训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 以图表呈现 训练 的 loss 变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1f44b8d1552f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'次数'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('次数')\n",
    "plt.ylabel('loss')\n",
    "plt.title('train loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 以图表呈现 检验 的 loss 变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('次数')\n",
    "plt.ylabel('loss')\n",
    "plt.title('validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(bleu_scores)\n",
    "plt.xlabel('次数')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.title('BLEU score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
