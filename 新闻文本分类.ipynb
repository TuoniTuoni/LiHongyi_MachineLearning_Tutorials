{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 试题说明\n",
    "\n",
    "基于THUCNews数据集的文本分类， THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档，参赛者需要根据新闻标题的内容用算法来判断该新闻属于哪一类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据说明\n",
    "THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。\n",
    "\n",
    "训练集，验证集按照“原文标题+\\t+标签”的格式抽取出来，可以直接根据新闻标题进行文本分类任务，希望答题者能够给出自己的解决方案。\n",
    "\n",
    "测试集仅提供“原文标题”，答题者需要对其预测相应的分类类别。\n",
    "\n",
    "训练集：data/train.txt\n",
    "\n",
    "验证集：data/dev.txt\n",
    "\n",
    "测试集：data/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## 提交答案\n",
    "考试提交，需要提交模型代码项目版本和结果文件。结果文件为TXT文件格式，命名为result.txt，文件内的字段需要按照指定格式写入。\n",
    "\n",
    "1.每个类别的行数和测试集原始数据行数应一一对应，不可乱序\n",
    "\n",
    "2.输出结果应检查是否为83599行数据，否则成绩无效\n",
    "\n",
    "3.输出结果文件命名为result.txt，一行一个类别，样例如下：\n",
    "\n",
    "···\n",
    "\n",
    "游戏\n",
    "\n",
    "财经\n",
    "\n",
    "时政\n",
    "\n",
    "股票\n",
    "\n",
    "家居\n",
    "\n",
    "科技\n",
    "···"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基线系统\n",
    "\n",
    "## 数据处理\n",
    "\n",
    "### 构建词汇表\n",
    "\n",
    "在搭建模型之前，我们需要对整体语料构造词表。通过切词统计词频，去除低频词，从而完成构造词表。我们使用jieba作为中文切词工具。\n",
    "\n",
    "停用词表，我们从网上直接获取：[https://github.com/goto456/stopwords/blob/master/baidu_stopwords.txt](https://github.com/goto456/stopwords/blob/master/baidu_stopwords.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp \n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.812 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "import jieba\n",
    "\n",
    "\n",
    "def sort_and_write_words(all_words, file_path):\n",
    "    words = list(chain(*all_words))\n",
    "    words_vocab = Counter(words).most_common()\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write('[UNK]\\n[PAD]\\n')\n",
    "        # filter the count of words below 5\n",
    "        # 过滤低频词，词频<5\n",
    "        for word, num in words_vocab:\n",
    "            if num < 5:\n",
    "                continue\n",
    "            f.write(word + \"\\n\")\n",
    "\n",
    "\n",
    "(root, directory, files), = list(os.walk(\"./work/data\"))\n",
    "all_words = []\n",
    "for file_name in files:\n",
    "    with open(os.path.join(root, file_name), \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if file_name in [\"train.txt\", \"dev.txt\"]:\n",
    "                text, label = line.strip().split(\"\\t\")\n",
    "            elif file_name == \"test.txt\":\n",
    "                text = line.strip()\n",
    "            else:\n",
    "                continue\n",
    "            words = jieba.lcut(text)\n",
    "            words = [word for word in words if word.strip() !='']\n",
    "            all_words.append(words)\n",
    "\n",
    "# 写入词表\n",
    "sort_and_write_words(all_words, \"work/data/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79441 work/data/vocab.txt\n",
      "1395 work/data/stop_words.txt\n"
     ]
    }
   ],
   "source": [
    "# 词汇表大小\n",
    "!wc -l work/data/vocab.txt\n",
    "# 停用词表大小\n",
    "!wc -l work/data/stop_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 加载自定义数据集\n",
    "\n",
    "\n",
    "构建词汇表完毕之后，我们可以加载自定义数据集。加载自定义数据集可以通过继承`paddle.io.Dataset`完成。\n",
    "\n",
    "更多自定义数据集方式参考：[自定义数据集](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data_prepare/dataset_self_defined.rst)\n",
    "\n",
    "同时，PaddleNLP提供了文本分类、序列标注、阅读理解等多种任务的常用数据集，一键即可加载，详细信息参考[数据集](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data_prepare/dataset_list.rst)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "\n",
    "class NewsData(paddle.io.Dataset):\n",
    "    def __init__(self, data_path, mode=\"train\"):\n",
    "        is_test = True if mode == \"test\" else False\n",
    "        self.label_map = { item:index for index, item in enumerate(self.label_list)}\n",
    "        self.examples = self._read_file(data_path, is_test)\n",
    "\n",
    "    def _read_file(self, data_path, is_test):\n",
    "        examples = []\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if is_test:\n",
    "                    text = line.strip()\n",
    "                    examples.append((text,))\n",
    "                else:\n",
    "                    text, label = line.strip('\\n').split('\\t')\n",
    "                    label = self.label_map[label]\n",
    "                    examples.append((text, label))\n",
    "        return examples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    @property\n",
    "    def label_list(self):\n",
    "        return ['财经', '彩票', '房产', '股票', '家居', '教育', '科技', '社会', '时尚', '时政', '体育', '星座', '游戏', '娱乐']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Text: 网易第三季度业绩低于分析师预期; Label ID 6\n",
      "Text: 巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘; Label ID 10\n",
      "Text: 美国称支持向朝鲜提供紧急人道主义援助; Label ID 9\n",
      "Text: 增资交银康联 交行夺参股险商首单; Label ID 3\n",
      "Text: 午盘：原材料板块领涨大盘; Label ID 3\n",
      "\n",
      "Test data:\n",
      "Text: 北京君太百货璀璨秋色 满100省353020元\n",
      "Text: 教育部：小学高年级将开始学习性知识\n",
      "Text: 专业级单反相机 佳能7D单机售价9280元\n",
      "Text: 星展银行起诉内地客户 银行强硬客户无奈\n",
      "Text: 脱离中国的实际 强压人民币大幅升值只能是梦想\n"
     ]
    }
   ],
   "source": [
    "# Loads dataset.\n",
    "train_ds = NewsData(\"work/data/train.txt\", mode=\"train\")\n",
    "dev_ds = NewsData(\"work/data/dev.txt\", mode=\"dev\")\n",
    "test_ds = NewsData(\"work/data/test.txt\", mode=\"test\")\n",
    "\n",
    "print(\"Train data:\")\n",
    "for text, label in train_ds[:5]:\n",
    "    print(f\"Text: {text}; Label ID {label}\")\n",
    "\n",
    "print()\n",
    "print(\"Test data:\")\n",
    "for text, in test_ds[:5]:\n",
    "    print(f\"Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 读入数据\n",
    "\n",
    "加载数据集之后，还需要将原始文本转化为word id，读入数据。\n",
    "\n",
    "PaddleNLP提供了许多关于NLP任务中构建有效的数据pipeline的常用API\n",
    "\n",
    "| API                             | 简介                                       |\n",
    "| ------------------------------- | :----------------------------------------- |\n",
    "| `paddlenlp.data.Stack`          | 堆叠N个具有相同shape的输入数据来构建一个batch |\n",
    "| `paddlenlp.data.Pad`            | 将长度不同的多个句子padding到统一长度，取N个输入数据中的最大长度 |\n",
    "| `paddlenlp.data.Tuple`          | 将多个batchify函数包装在一起 |\n",
    "\n",
    "更多数据处理操作详见： https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/data.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Data: \n",
      " [[1 2 3 4]\n",
      " [3 4 5 6]\n",
      " [5 6 7 8]]\n",
      "\n",
      "Padded Data: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 0]\n",
      " [8 9 0 0]]\n",
      "\n",
      "ids: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 0]\n",
      " [8 9 0 0]]\n",
      "\n",
      "labels: \n",
      " [[1]\n",
      " [0]\n",
      " [1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.data import Stack, Pad, Tuple\n",
    "a = [1, 2, 3, 4]\n",
    "b = [3, 4, 5, 6]\n",
    "c = [5, 6, 7, 8]\n",
    "result = Stack()([a, b, c])\n",
    "print(\"Stacked Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "a = [1, 2, 3, 4]\n",
    "b = [5, 6, 7]\n",
    "c = [8, 9]\n",
    "result = Pad(pad_val=0)([a, b, c])\n",
    "print(\"Padded Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "data = [\n",
    "        [[1, 2, 3, 4], [1]],\n",
    "        [[5, 6, 7], [0]],\n",
    "        [[8, 9], [1]],\n",
    "       ]\n",
    "batchify_fn = Tuple(Pad(pad_val=0), Stack())\n",
    "ids, labels = batchify_fn(data)\n",
    "print(\"ids: \\n\", ids)\n",
    "print()\n",
    "print(\"labels: \\n\", labels)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本基线将对数据作以下处理：\n",
    "\n",
    "* 将原始数据处理成模型可以读入的格式。首先使用jieba切词，之后将jieba切完后的单词映射词表中单词id。\n",
    "\n",
    "* 使用`paddle.io.DataLoader`接口多线程异步加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import MapDataset\n",
    "\n",
    "from utils import convert_example, read_vocab, write_results\n",
    "\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      use_gpu=False,\n",
    "                      batchify_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = MapDataset(dataset)\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    if mode == 'train' and use_gpu:\n",
    "        sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        shuffle = True if mode == 'train' else False\n",
    "        sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=sampler,\n",
    "        return_list=True,\n",
    "        collate_fn=batchify_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = read_vocab(\"work/data/vocab.txt\")\n",
    "stop_words = read_vocab(\"work/data/stop_words.txt\")\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "\n",
    "trans_fn = partial(convert_example, vocab=vocab, stop_words=stop_words, is_test=False)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=vocab.get('[PAD]', 0)),  # input_ids\n",
    "    Stack(dtype=\"int64\"),  # seq len\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]\n",
    "train_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    trans_fn=trans_fn,\n",
    "    batch_size=batch_size,\n",
    "    mode='train',\n",
    "    use_gpu=True,\n",
    "    batchify_fn=batchify_fn)\n",
    "dev_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    trans_fn=trans_fn,\n",
    "    batch_size=batch_size,\n",
    "    mode='validation',\n",
    "    use_gpu=True,\n",
    "    batchify_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 组网、配置、训练\n",
    "\n",
    "### 定义模型结构\n",
    "\n",
    "读入了数据之后，即可定义模型结构。此处，我们选择BiLSTM作为baseline。\n",
    "\n",
    "PaddleNLP提供了序列化建模模块paddlenlp.seq2vec模块，该模块可以将文本抽象成一个携带语义的文本向量。\n",
    "\n",
    "关于seq2vec模块更多信息参考：[paddlenlp.seq2vec是什么？快来看看如何用它完成情感分析任务](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    "\n",
    "\n",
    "本基线模型选用`LSTMencoder`搭建一个BiLSTM模型用于文本分类任务。\n",
    "\n",
    "- `paddle.nn.Embedding`组建word-embedding层\n",
    "- `paddlenlp.seq2vec.LSTMEncoder`组建句子建模层\n",
    "- `paddle.nn.Linear`构造二分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class LSTMModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 emb_dim=128,\n",
    "                 padding_idx=0,\n",
    "                 lstm_hidden_size=198,\n",
    "                 direction='forward',\n",
    "                 lstm_layers=1,\n",
    "                 dropout_rate=0.0,\n",
    "                 pooling_type=None,\n",
    "                 fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "\n",
    "        # 首先将输入word id 查表后映射成 word embedding\n",
    "        self.embedder = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=padding_idx)\n",
    "\n",
    "        # 将word embedding经过LSTMEncoder变换到文本语义表征空间中\n",
    "        self.lstm_encoder = paddlenlp.seq2vec.LSTMEncoder(\n",
    "            emb_dim,\n",
    "            lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            direction=direction,\n",
    "            dropout=dropout_rate,\n",
    "            pooling_type=pooling_type)\n",
    "\n",
    "        # LSTMEncoder.get_output_dim()方法可以获取经过encoder之后的文本表示hidden_size\n",
    "        self.fc = nn.Linear(self.lstm_encoder.get_output_dim(), fc_hidden_size)\n",
    "\n",
    "        # 最后的分类器\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, text, seq_len):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "\n",
    "        # Shape: (batch_size, num_tokens, num_directions*lstm_hidden_size)\n",
    "        # num_directions = 2 if direction is 'bidirectional' else 1\n",
    "        text_repr = self.lstm_encoder(embedded_text, sequence_length=seq_len)\n",
    "\n",
    "\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc_out = paddle.tanh(self.fc(text_repr))\n",
    "\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(fc_out)\n",
    "        return logits\n",
    "\n",
    "model= LSTMModel(\n",
    "        len(vocab),\n",
    "        len(train_ds.label_list),\n",
    "        direction='bidirectional',\n",
    "        padding_idx=vocab['[PAD]'])\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练\n",
    "\n",
    "数据读入模型构建完毕，定义优化器，选择学习率和评价指标，我们即可开始训练。\n",
    "\n",
    "根据比赛评价规则，此处选用准确率Accuracy作为评价指标。\n",
    "\n",
    "模型训练模型之后模型参数会自动保存在ckpt文件夹下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.Adam(\n",
    "    parameters=model.parameters(), learning_rate=5e-4)\n",
    "\n",
    "# Defines loss and metric.\n",
    "criterion = paddle.nn.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "model.prepare(optimizer, criterion, metric)\n",
    "\n",
    "# Starts training and evaluating.\n",
    "model.fit(train_loader, dev_loader, epochs=epochs, save_dir='./ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/03e33c2d18f04849bac71a5a39dd543033c43255fc454a23a7cd5189a85b9905)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测\n",
    "\n",
    "训练模型之后，我们可以利用当前训练的模型对测试集数据进行预测，并写入预测结果至result.txt文件中。\n",
    "\n",
    "之后将结果文件提交至[课程或比赛区](https://aistudio.baidu.com/aistudio/course/introduce/1978)即可看到成绩噢！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=vocab.get('[PAD]', 0)),  # input_ids\n",
    "    Stack(dtype=\"int64\"),  # seq len\n",
    "): [data for data in fn(samples)]\n",
    "test_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    trans_fn=partial(convert_example, vocab=vocab, stop_words=stop_words, is_test=True),\n",
    "    batch_size=batch_size,\n",
    "    mode='test',\n",
    "    use_gpu=True,\n",
    "    batchify_fn=test_batchify_fn)\n",
    "\n",
    "# Does predict.\n",
    "results = model.predict(test_loader)\n",
    "inverse_lable_map = {value:key for key, value in test_ds.label_map.items()}\n",
    "all_labels = []\n",
    "for batch_results in results[0]:\n",
    "    label_ids = np.argmax(batch_results, axis=1).tolist()\n",
    "    labels = [inverse_lable_map[label_id] for label_id in label_ids]\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "write_results(all_labels, \"./result.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 进阶优化\n",
    "\n",
    "1. 可以尝试`paddlenlp.seq2vec`中其它模型，观察模型效果。\n",
    "\n",
    "2. **预训练模型**：\n",
    "鉴于目前预训练模型ERNIE/BERT对语义有着更强大的表征意义，我们可以通过更换为预训练模型完成该分类任务。\n",
    "\n",
    "PaddleNLP提供了许多中文[预训练模型](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/transformers.md)如ERNIE、ERNIE-Tiny、BERT、RoBERTa、Electra等预训练模型，也内置了各种预训练模型用于文本分类Fine-tune常用网络。可参考PaddleNLP AI Studio项目学习试用预训练模型。\n",
    "\n",
    " 如：使用ERNIE Fine-tune文本分类任务：[参考项目](https://aistudio.baidu.com/aistudio/projectdetail/1294333)\n",
    "\n",
    "\n",
    "此外，还可调用`paddlenlp.embeddings`接口，使用预训练好的词向量初始化embedding，加快模型收敛速度：[参考项目](https://aistudio.baidu.com/aistudio/projectdetail/1535355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import ErnieForSequenceClassification, ErnieTokenizer\n",
    "\n",
    "model = ErnieForSequenceClassification.from_pretrained(\"ernie-1.0\", num_classes=len(train_ds.label_list))\n",
    "tokenizer = ErnieTokenizer.from_pretrained(\"ernie-1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入PaddleNLP的QQ技术交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
